{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "model_name = 'gpt-4o'\n",
    "\n",
    "# data = pd.read_excel('test_data/aspects_test_data.xlsx', sheet_name='verifiability')\n",
    "data = pd.read_excel('test_data/gold_human_annotations.xlsx', sheet_name='verifiability')\n",
    "\n",
    "## delete columns chatgpt_verifiability_definitions_incontext_learning_score','chatgpt_verifiability_definitions_incontext_learning_rationale\n",
    "\n",
    "if 'chatgpt_verifiability_definitions_incontext_learning_score' in data.columns:\n",
    "    data = data.drop(columns=['chatgpt_verifiability_definitions_incontext_learning_score','chatgpt_verifiability_definitions_incontext_learning_rationale'])\n",
    "# data = data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'actionability',\n",
       "       'actionability_label', 'actionability_label_type', 'batch',\n",
       "       'grounding_specificity', 'grounding_specificity_label',\n",
       "       'grounding_specificity_label_type', 'verifiability',\n",
       "       'verifiability_label', 'verifiability_label_type', 'helpfulness',\n",
       "       'helpfulness_label', 'helpfulness_label_type', 'professional_tone',\n",
       "       'professional_tone_label', 'professional_tone_label_type',\n",
       "       'valid_point', 'valid_point_label', 'valid_point_label_type', 'id'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXTRACT_CLAIM = '''\n",
    "Check if there is any claims in the text, or it just includes normal statements.\n",
    "\n",
    "**Opinion & Claims**\n",
    "- Subjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\n",
    "- Any suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\n",
    "- Any comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\n",
    "- Any deductions or inferred observations that go beyond just stating facts or results from the paper.\n",
    "- Generally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n",
    "    - Ex: “Important methods like X are not discussed”. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\n",
    "\n",
    "**Normal Statements**\n",
    "Definition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\n",
    "Clear and precise reasoning or explanation.\n",
    "- References to external works/data, when applicable, are specific and relevant.\n",
    "- Common-sense arguments are logically unassailable.\n",
    "- Indicating that something exists, or missing without indicating that it should be removed or included.\n",
    "- General statements about the paper, that don’t include an opinion.\n",
    "- Objective and factual statements that don’t need any kind of verification.\n",
    "- Asking for clarifications and general questions.\n",
    "- Logical statements, or things that can be inferred directly.\n",
    "- We treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n",
    "    - Example: This paper is well written, and the experimentation methods are well designed.\n",
    "\n",
    "Generate a rationale and use it to output the score. The score should be \"Yes\" if there are claims, and \"No\" if there are no claims.\n",
    "{examples}\n",
    "Review Point:\n",
    "{review_point}    \n",
    "\n",
    "'''\n",
    "\n",
    "CLAIM_VERIFICATION = '''\n",
    "This aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: “Will this review point help authors improve their draft?”. There is no correlation between the aspect score and the length of the review point.\n",
    "Evaluate the review point based on the aspect description provided next.\n",
    "\n",
    "\n",
    "Verifiability:\n",
    "This aspect measures how well the claim in the text is verified. Evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don’t need to be stated directly; they can also be inferred.\n",
    "**Verification**\n",
    "- The claim is verified by providing logical reasoning.\n",
    "- The claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\n",
    "- The claim is verified by providing external references.\n",
    "\n",
    "Verifiability is rated on a scale from 1-5. We will now provide a definition for each.\n",
    "1: Unverifiable\n",
    "Definition: The comment contains a claim without any supporting evidence or justification.\n",
    "Examples\n",
    "The results fall behind previous work, and the reasons for this should be investigated.\n",
    "For many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\n",
    "While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n",
    "2: Borderline Verifiable\n",
    "Definition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\n",
    "Examples\n",
    "This method shouldn’t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn’t work for them.\n",
    "It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\n",
    "In the experiments, the transfer tasks are too artificial. “At the pretraining stage, we train the models with examples from two classes (“bird\" vs. “frog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST”.\n",
    "3: Somewhat Verifiable\n",
    "Definition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\n",
    "Examples\n",
    "The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\n",
    "The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\n",
    "The approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n",
    "4: Mostly Verifiable\n",
    "Definition: The comment’s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\n",
    "Examples\n",
    "The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\n",
    "The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\n",
    "As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n",
    "5: Fully Verifiable\n",
    "Definition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\n",
    "Clear and precise reasoning or explanation.\n",
    "    - References to external works/data, when applicable, are specific and relevant.\n",
    "    - Common-sense arguments are logically unassailable.\n",
    "Examples\n",
    "The landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\n",
    "The first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\n",
    "The paper’s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n",
    "\n",
    "Generate a rationale and use it to output the score. \n",
    "{examples}\n",
    "\n",
    "Review Point:\n",
    "{review_point}\n",
    "'''\n",
    "\n",
    "EXTRACT_CLAIM = \"\"\"\n",
    "\n",
    "Claim Extraction\n",
    "\n",
    "Objective:\n",
    "Determine whether the given text contains claims or merely consists of factual statements.\n",
    "\n",
    "Opinion & Claims\n",
    "\n",
    "Subjective statements, including opinions or disagreements with experimental choices.\n",
    "\n",
    "Suggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n",
    "\n",
    "Judgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n",
    "\n",
    "Deductions or inferred observations beyond stating mere facts.\n",
    "\n",
    "Any statement where evidence or justification is required to support the claim.\n",
    "\n",
    "Normal Statements\n",
    "\n",
    "Definition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n",
    "\n",
    "Indicating existence or absence of something without suggesting changes.\n",
    "\n",
    "General statements about the paper that do not express an opinion.\n",
    "\n",
    "Objective, factual statements that do not require verification.\n",
    "\n",
    "Requests for clarification or general questions.\n",
    "\n",
    "Logical statements or directly inferable information.\n",
    "\n",
    "Positive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n",
    "\n",
    "Scoring Criteria:\n",
    "\n",
    "Yes: If the text contains claims, opinions, or suggestions.\n",
    "\n",
    "No: If the text consists solely of normal statements.\n",
    "\n",
    "{examples}\n",
    "Review Point:\n",
    "{review_point}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "CLAIM_VERIFICATION =\"\"\"\n",
    "Objective\n",
    "The primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: “Will this review point help authors improve their draft?” There is no correlation between the aspect score and the length of the review point.\n",
    "\n",
    "Evaluation Aspect: Verifiability\n",
    "\n",
    "Definition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n",
    "\n",
    "Verification Methods:\n",
    "\n",
    "Logical reasoning supports the claim.\n",
    "\n",
    "Common sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n",
    "\n",
    "External references substantiate the claim.\n",
    "\n",
    "Scoring Criteria (1-5 Scale)\n",
    "\n",
    "1 - Unverifiable\n",
    "\n",
    "Definition: The comment contains a claim without any supporting evidence or justification.\n",
    "Examples:\n",
    "\n",
    "\"The results fall behind previous work, and the reasons for this should be investigated.\"\n",
    "\n",
    "\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n",
    "\n",
    "\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n",
    "\n",
    "2 - Borderline Verifiable\n",
    "\n",
    "Definition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\n",
    "Examples:\n",
    "\n",
    "\"This method shouldn’t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn’t work for them.\"\n",
    "\n",
    "\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n",
    "\n",
    "\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n",
    "\n",
    "3 - Somewhat Verifiable\n",
    "\n",
    "Definition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\n",
    "Examples:\n",
    "\n",
    "\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n",
    "\n",
    "\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n",
    "\n",
    "\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n",
    "\n",
    "4 - Mostly Verifiable\n",
    "\n",
    "Definition: The comment’s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\n",
    "Examples:\n",
    "\n",
    "\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n",
    "\n",
    "\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n",
    "\n",
    "\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n",
    "\n",
    "5 - Fully Verifiable\n",
    "\n",
    "Definition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n",
    "\n",
    "Clear and precise reasoning or explanation.\n",
    "\n",
    "Specific and relevant references to external works or data.\n",
    "\n",
    "Logical and unassailable common-sense arguments.\n",
    "Examples:\n",
    "\n",
    "\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n",
    "\n",
    "\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n",
    "\n",
    "\"The paper’s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Use the scoring scale to evaluate the verifiability of each review point.\n",
    "\n",
    "Focus on how well-supported the claims are rather than the length of the comment.\n",
    "\n",
    "Consider whether the review point meaningfully helps the authors improve their draft.\n",
    "\n",
    "Generate a rationale and use it to output the score. \n",
    "{examples}\n",
    "\n",
    "Review Point:\n",
    "{review_point}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "all_incontext_examples = pd.read_excel('/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/test_data/in_context_examples.xlsx', sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['actionability', 'grounding_specificity', 'verifiability', 'claim_extraction', 'helpfulness'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_incontext_examples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prompt(review_point,aspect,prompt_type, in_context_examples):\n",
    "    prompt = ''\n",
    "    if prompt_type == 'extract_claim':\n",
    "        in_context_examples = in_context_examples['claim_extraction']\n",
    "        examples = ''\n",
    "        examples_str = []\n",
    "        ## choose two random samples with the label \"X\" that are not the same as the current review point\n",
    "        for _ in range(1):\n",
    "            while True:\n",
    "                x_sample = in_context_examples[in_context_examples[f'{aspect}_label'] == 'No'].sample(1).iloc[0]\n",
    "                if x_sample['review_point'] != review_point:\n",
    "                    break\n",
    "            ## change the score to No\n",
    "            x_sample[f'{aspect}_label'] = 'No'\n",
    "            examples_str.append(f'''\n",
    "Review Point: {x_sample['review_point']}\n",
    "rationale: {x_sample['rationale']}\n",
    "score: {x_sample[f'{aspect}_label']}\n",
    "''')\n",
    "        ## choose two random samples with any of the other labels that are not the same as the current review point\n",
    "        for _ in range(1):\n",
    "            while True:\n",
    "                non_x_samples = in_context_examples[in_context_examples[f'{aspect}_label'] != 'No']\n",
    "                non_x_sample = non_x_samples.sample(1).iloc[0]\n",
    "                if non_x_sample['review_point'] != review_point:\n",
    "                    break\n",
    "            non_x_sample[f'{aspect}_label'] = 'Yes'\n",
    "            examples_str.append(f'''\n",
    "Review Point: {non_x_sample['review_point']}\n",
    "rationale: {non_x_sample['rationale']}\n",
    "score: {non_x_sample[f'{aspect}_label']}\n",
    "''')\n",
    "\n",
    "\n",
    "\n",
    "        ## shuffle the list \n",
    "        random.shuffle(examples_str)\n",
    "        examples = '\\n'.join(examples_str)\n",
    "        prompt = EXTRACT_CLAIM.format(review_point=review_point,examples=examples)\n",
    "\n",
    "    elif prompt_type == 'claim_verification':\n",
    "        in_context_examples = in_context_examples[aspect]\n",
    "        examples = ''\n",
    "        examples_str = []\n",
    "        ##3 group examples by the label and choose a random example from each group\n",
    "        unique_labels = in_context_examples[f'{aspect}_label'].unique()\n",
    "        ## delete lebel X\n",
    "        unique_labels = [label for label in unique_labels if label != 'X']\n",
    "        for label in unique_labels:\n",
    "            for _ in range(1):\n",
    "                ## keep sampling a line till it is not the same as the currrent review point\n",
    "                while True:\n",
    "                    row = in_context_examples[in_context_examples[f'{aspect}_label']==label].sample(1)\n",
    "                    row = row.iloc[0]\n",
    "                    if row['review_point'] != review_point:\n",
    "                        break\n",
    "                \n",
    "                score = row[f'{aspect}_label']\n",
    "                rationale = row['rationale'] \n",
    "\n",
    "\n",
    "                examples_str.append(f'''\n",
    "    Review Point: {row['review_point']}\n",
    "    rationale: {rationale}\n",
    "    score: {score}\n",
    "    ''')\n",
    "        ## shuffle the list \n",
    "        random.shuffle(examples_str)\n",
    "        examples = '\\n'.join(examples_str)\n",
    "\n",
    "        prompt = CLAIM_VERIFICATION.format(review_point=review_point,examples=examples)\n",
    "\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "prompt_types = ['extract_claim','claim_verification']\n",
    "\n",
    "def chatgpt_inf(data, prompt_type, save_path):\n",
    "    output_df = []\n",
    "## iterate over the df \n",
    "    for idx ,row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "        fails = 0\n",
    "        review_point = row['review_point']\n",
    "        prompt = get_prompt(review_point=review_point,aspect='verifiability',prompt_type=prompt_type, in_context_examples=all_incontext_examples)\n",
    "\n",
    "        # print(f\"Prompt: {prompt}\")\n",
    "        # break\n",
    "\n",
    "        try:\n",
    "            clue_message = {\"role\": \"user\", \"content\": prompt}\n",
    "            completion = client.chat.completions.create(\n",
    "            # response_format={ \"type\": \"json_object\" },\n",
    "            temperature=0.0,\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                clue_message\n",
    "            ]\n",
    "            )\n",
    "            response = str(completion.choices[0].message.content.lower())\n",
    "\n",
    "            # if prompt_type == 'extract_claim':\n",
    "            #     assert response in ['yes','no']\n",
    "            #     cur_row = row.copy()\n",
    "            #     cur_row[f'chatgpt_verifiability_definitions_incontext_learning_score'] = response\n",
    "            #     output_df.append(cur_row)\n",
    "            # elif prompt_type == 'claim_verification':\n",
    "            rationale, score = response.split('score:')\n",
    "            rationale = rationale.split('rationale:')[1]\n",
    "            if rationale and score:\n",
    "                score = score.strip()\n",
    "                rationale = rationale.strip()\n",
    "\n",
    "\n",
    "        except Exception as e:      \n",
    "            print(response)\n",
    "            print(f\"Failed for {idx} with error {e}\")\n",
    "            fails += 1\n",
    "            score = 'NA'\n",
    "            rationale = 'NA'\n",
    "            \n",
    "        cur_row = row.copy()\n",
    "        cur_row[f'chatgpt_verifiability_definitions_incontext_learning_score'] = score\n",
    "        cur_row[f'chatgpt_verifiability_{prompt_type}_rationale'] = rationale\n",
    "        output_df.append(cur_row)\n",
    "\n",
    "    output_df = pd.DataFrame(output_df)\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/144 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 44/144 [01:15<03:07,  1.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 43 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 47/144 [01:19<02:33,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 46 with error not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 52/144 [01:28<02:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 51 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 54/144 [01:31<02:21,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 53 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 66/144 [01:50<02:25,  1.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 65 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 72/144 [02:02<02:30,  2.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 71 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 74/144 [02:05<02:07,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 73 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 95/144 [02:40<01:13,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 94 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 100/144 [02:48<01:10,  1.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 99 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 131/144 [03:47<00:27,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 130 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 133/144 [03:51<00:22,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 132 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 137/144 [03:57<00:12,  1.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 136 with error list index out of range\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [04:09<00:00,  1.73s/it]\n",
      " 59%|█████▊    | 71/121 [03:20<02:41,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 86 with error not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 82/121 [03:48<01:44,  2.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 98 with error not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 108/121 [05:06<00:36,  2.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed for 125 with error not enough values to unpack (expected 2, got 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [05:45<00:00,  2.86s/it]\n"
     ]
    }
   ],
   "source": [
    "##### Claim extraction\n",
    "output_df = chatgpt_inf(data, 'extract_claim', 'verifiability_claim_extraction_chatgpt.xlsx')\n",
    "\n",
    "## update the data with the new scores, only add the column named chatgpt_verifiability_definitions_incontext_learning_score\n",
    "data = data.merge(output_df[['review_point','chatgpt_verifiability_definitions_incontext_learning_score']], on='review_point', how='left')\n",
    "\n",
    "## only keep the rows that have a yes in the chatgpt_verifiability_definitions_incontext_learning_score\n",
    "verification_data = data[data['chatgpt_verifiability_definitions_incontext_learning_score']=='yes']\n",
    "\n",
    "verification_data = chatgpt_inf(verification_data, 'claim_verification', 'verifiability_claim_verification_chatgpt.xlsx')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = []\n",
    "for idx, row in data.iterrows():\n",
    "    cur_row = row.copy()\n",
    "    if row['chatgpt_verifiability_definitions_incontext_learning_score'] == 'yes':\n",
    "        ## get the new score and rationale\n",
    "        new_row = verification_data[verification_data['review_point']==row['review_point']].iloc[0]\n",
    "        score = new_row['chatgpt_verifiability_definitions_incontext_learning_score']\n",
    "        rationale = new_row['chatgpt_verifiability_claim_verification_rationale']\n",
    "        cur_row['chatgpt_verifiability_definitions_incontext_learning_score'] = score\n",
    "        cur_row['chatgpt_verifiability_claim_verification_rationale'] = rationale\n",
    "    temp_df.append(cur_row)\n",
    "data = pd.DataFrame(temp_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the vlue no with X in  chatgpt_verifiability_definitions_incontext_learning_score\n",
    "data['chatgpt_verifiability_definitions_incontext_learning_score'] = data['chatgpt_verifiability_definitions_incontext_learning_score'].replace('no','X')\n",
    "## convert the types of values to str\n",
    "data['chatgpt_verifiability_definitions_incontext_learning_score'] = data['chatgpt_verifiability_definitions_incontext_learning_score'].astype(str)\n",
    "\n",
    "data.to_excel('two_step_verifiability_all_gold_chatgpt_output_min_examples.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

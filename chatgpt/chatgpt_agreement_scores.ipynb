{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for actionability found\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score,cohen_kappa_score\n",
    "from scipy import stats\n",
    "\n",
    "# aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "aspects = [ 'actionability']\n",
    "types = ['definitions', 'definitions_examples', 'definitions_incontext_learning']\n",
    "\n",
    "results = {}\n",
    "## check if theres is file for each aspect\n",
    "for aspect in aspects:\n",
    "    try:\n",
    "        results[aspect] = pd.read_csv(f'outputs/{aspect}_results.csv')\n",
    "        print(f'File for {aspect} found')\n",
    "    except:\n",
    "        print(f'No file for {aspect}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats(pred, gold,aspect):\n",
    "\n",
    "    if aspect != 'verifiability':\n",
    "        f1 = f1_score(pred,gold, average=\"micro\")\n",
    "        kappa = cohen_kappa_score(pred,gold)\n",
    "        kappa_linear = cohen_kappa_score(pred,gold, weights='linear')\n",
    "        kappa_quadratic = cohen_kappa_score(pred,gold, weights='quadratic')\n",
    "        spearman = stats.spearmanr(pred, gold)\n",
    "        return f1, kappa, kappa_linear, kappa_quadratic, spearman\n",
    "\n",
    "    else:\n",
    "        new_pred = []\n",
    "        new_gold = []\n",
    "        new_pred_X = []\n",
    "        new_gold_X = []\n",
    "        for x,y in zip(pred,gold):\n",
    "            ## map values to X\n",
    "            if x in ['X','x', 'NO CLAIM']: x = 'X'\n",
    "            if y in ['X','x', 'NO CLAIM']: y = 'X'\n",
    "\n",
    "            # if one of the values is X, then add it to a differnt list\n",
    "            if x == 'X' or y == 'X':\n",
    "                x = 0 if x == 'X' else 1\n",
    "                y = 0 if y == 'X' else 1\n",
    "                new_pred_X.append(x)\n",
    "                new_gold_X.append(y)\n",
    "            else:\n",
    "                new_pred.append(x)\n",
    "                new_gold.append(y)\n",
    "        \n",
    "        gold = new_gold\n",
    "        pred = new_pred\n",
    "        f1 = f1_score(pred,gold, average=\"micro\")\n",
    "        kappa = cohen_kappa_score(pred,gold)\n",
    "        kappa_linear = cohen_kappa_score(pred,gold, weights='linear')\n",
    "        kappa_quadratic = cohen_kappa_score(pred,gold, weights='quadratic')\n",
    "        spearman = stats.spearmanr(pred, gold)\n",
    "        f1_X = f1_score(new_pred_X,new_gold_X, average=\"micro\")\n",
    "        return f1, kappa, kappa_linear, kappa_quadratic, spearman, f1_X\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('results/chatgpt_results.txt', 'w') as f:\n",
    "    for aspect, df in results.items():\n",
    "        f.write(f'Agreement Statistics for {aspect}\\n')\n",
    "        f.write(f' Total number of samples: {len(df)}\\n')\n",
    "\n",
    "        for type in ['definitions_incontext_learning']:\n",
    "            gold_labels = []\n",
    "            chatgpt_labels = []\n",
    "            for index, row in df.iterrows():\n",
    "                gold_label = str(int(float(row[f'{aspect}_label']))) if row[f'{aspect}_label'] not in ['X','x', 'NO CLAIM', 'no claim'] else 'X'\n",
    "                chatgpt_label = str(int(float(row[f'chatgpt_{aspect}_{type}_score']))) if row[f'chatgpt_{aspect}_{type}_score'] not in ['X','x', 'NO CLAIM','no claim'] else 'X'\n",
    "                gold_labels.append(gold_label)\n",
    "                chatgpt_labels.append(chatgpt_label)\n",
    "\n",
    "            f.write(f' Agreement Statistics for {type}\\n')\n",
    "            ## for verifiability we have one mroe measure\n",
    "            if aspect == 'verifiability':\n",
    "                f1, kappa, kappa_linear, kappa_quadratic, spearman, f1_X = get_stats(gold=gold_labels, pred=chatgpt_labels, aspect=aspect)\n",
    "            else:\n",
    "                f1, kappa, kappa_linear, kappa_quadratic, spearman = get_stats(gold=gold_labels, pred=chatgpt_labels, aspect=aspect)\n",
    "\n",
    "            f.write(f' F1 Score: {f1:.2f}\\n')\n",
    "            f.write(f' Kappa Score: {kappa:.2f}\\n')\n",
    "            f.write(f' Linear Kappa Score: {kappa_linear:.2f}\\n')\n",
    "            f.write(f' Quadratic Kappa Score: {kappa_quadratic:.2f}\\n')\n",
    "            f.write(f' Spearman Correlation: {spearman.correlation:.2f}\\n')\n",
    "            if aspect == 'verifiability':\n",
    "                f.write(f' F1 Score for X: {f1_X:.2f}\\n')\n",
    "            \n",
    "            f.write('-' * 50 + '\\n')\n",
    "                \n",
    "        f.write('=' * 50 + '\\n')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

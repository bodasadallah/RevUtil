{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prompt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m\n\u001b[1;32m      5\u001b[0m dotenv\u001b[38;5;241m.\u001b[39mload_dotenv()\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mprompt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      7\u001b[0m client \u001b[38;5;241m=\u001b[39m OpenAI(api_key\u001b[38;5;241m=\u001b[39mos\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreview_evaluation_mbzuai\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      9\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'prompt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability_verification','verifiability_extraction', 'helpfulness']\n",
    "all_incontext_examples = pd.read_excel('/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/test_data/in_context_examples.xlsx', sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prompt(review_point,aspect,prompt_type, in_context_examples,num_examples_per_label=1,):\n",
    "\n",
    "    prompt = ''\n",
    "    if prompt_type == 'definitions':\n",
    "        prompt = BASE_PROMPT.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[f'{aspect}_no_examples'])\n",
    "\n",
    "    elif prompt_type == 'definitions_examples':\n",
    "        prompt = BASE_PROMPT.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect])\n",
    "    elif prompt_type == 'definitions_incontext_learning' or prompt_type == 'chain_of_thoughts':\n",
    "        examples = ''\n",
    "        examples_str = []\n",
    "        ##3 group examples by the label and choose a random example from each group\n",
    "        for label in in_context_examples[f'{aspect}_label'].unique():\n",
    "            for _ in range(num_examples_per_label):\n",
    "                ## keep sampling a line till it is not the same as the currrent review point\n",
    "                while True:\n",
    "                    row = in_context_examples[in_context_examples[f'{aspect}_label']==label].sample(1)\n",
    "                    row = row.iloc[0]\n",
    "                    if row['review_point'] != review_point:\n",
    "                        break\n",
    "                \n",
    "                score = row[f'{aspect}_label']\n",
    "                rationale = row['rationale'] if prompt_type == 'definitions_incontext_learning' else row['chain_of_thoughts']\n",
    "\n",
    "\n",
    "                examples_str.append(f'''\n",
    "    Review Point: {row['review_point']}\n",
    "    rationale: {rationale}\n",
    "    score: {score}\n",
    "    ''')\n",
    "        ## shuffle the list \n",
    "        random.shuffle(examples_str)\n",
    "        examples = '\\n'.join(examples_str)\n",
    "        \n",
    "        ## for verifiability, we have two tasks\n",
    "        prompt = BASE_PROMPT_EXAMPLES.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect],examples=examples)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Chatgpt Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import json\n",
    "prompt_types = ['definitions', 'definitions_examples', 'definitions_incontext_learning']\n",
    "\n",
    "def chatgpt_inf(test_data, prompt_types, save_path):\n",
    "## iterate over the df \n",
    "    for aspect in test_data.keys():\n",
    "        responses = []\n",
    "        fails = 0\n",
    "        examples  = test_data[aspect]['examples']\n",
    "        data = test_data[aspect]['data']\n",
    "        for idx ,row in tqdm(data.iterrows(), total=data.shape[0]):\n",
    "            for prompt_type in prompt_types:\n",
    "                \n",
    "                review_point = row['review_point']\n",
    "                prompt = get_prompt(review_point=review_point,aspect=aspect,prompt_type=prompt_type, examples=examples)\n",
    "                # print(prompt_type)\n",
    "                # print(prompt)\n",
    "\n",
    "                ## call the gpt4 model,and output the error message if the call fails\n",
    "                try:\n",
    "                    clue_message = {\"role\": \"user\", \"content\": prompt}\n",
    "                    completion = client.chat.completions.create(\n",
    "                    response_format={ \"type\": \"json_object\" },\n",
    "                    model=model_name,\n",
    "                    messages=[\n",
    "                        clue_message\n",
    "                    ]\n",
    "                    )\n",
    "                    response = json.loads(completion.choices[0].message.content.lower())\n",
    "                    score = response['score']\n",
    "                    rationale = response['rationale']\n",
    "                except Exception as e:      \n",
    "                    print(f\"Failed for {idx} and {aspect} with error {e}\")\n",
    "                    fails += 1\n",
    "                    score = 'NA'\n",
    "                    rationale = 'NA'\n",
    "\n",
    "                row [ f'chatgpt_{aspect}_{prompt_type}_score'] = score\n",
    "                row [ f'chatgpt_{aspect}_{prompt_type}_rationale'] = rationale\n",
    "            responses.append(row)\n",
    "        responses = pd.DataFrame(responses)\n",
    "        responses.to_csv(f\"{save_path}/{aspect}_results.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chatgpt_batch_inf(test_data,aspect, prompt_type,data_type, save_path, temp=0, num_examples_per_label=1):\n",
    "   in_context_examples =  all_incontext_examples[aspect]\n",
    "   lines = []\n",
    "\n",
    "   for i,row in test_data.iterrows():\n",
    "      review_point = row['review_point']\n",
    "      prompt = get_prompt(review_point=review_point,aspect=aspect,prompt_type=prompt_type, in_context_examples=in_context_examples, num_examples_per_label=num_examples_per_label)   \n",
    "      line = {\n",
    "         \"custom_id\": f\"{row['id']}\", \n",
    "         \"method\": \"POST\", \n",
    "         \"url\": \"/v1/chat/completions\", \n",
    "         \"body\": {\"model\": model_name,\n",
    "         ########### UNCOMMENT AGAIN #########\n",
    "         # \"response_format\" :{ \"type\": \"json_object\" },\n",
    "         \"temperature\": temp,\n",
    "         \"messages\": \n",
    "         [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "         {\"role\": \"user\", \"content\": prompt}],}}\n",
    "      lines.append(line)\n",
    "\n",
    "   print(f'sample of the prompts is {lines[0]}')\n",
    "\n",
    "   ### Write batch input file\n",
    "   batch_file_path = f\"batch_data/{aspect}_{data_type}_batch_input.jsonl\"\n",
    "   with open(batch_file_path, 'w') as f:\n",
    "      for l in lines:\n",
    "         json.dump(l, f)\n",
    "         f.write('\\n')\n",
    "\n",
    "   ### upload the batch file\n",
    "   batch_input_file = client.files.create(\n",
    "   file=open(batch_file_path, \"rb\"),\n",
    "   purpose=\"batch\")\n",
    "\n",
    "   ### create the batch request\n",
    "   batch_input_file_id = batch_input_file.id\n",
    "   batch_data = client.batches.create(\n",
    "      input_file_id=batch_input_file_id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\",\n",
    "      metadata={\n",
    "         \"description\": f\"batch file for  {aspect} model gpt-4o, temperature {temp} for {data_type} data\",\n",
    "      })\n",
    "   batch_metadata = {\n",
    "      \"batch_id\": batch_data.id,\n",
    "      \"aspect\": aspect,\n",
    "      \"prompt_type\": prompt_type,\n",
    "      \"batch_input_file_id\": batch_input_file_id,\n",
    "      \"batch_file_path\": batch_file_path,\n",
    "      \"data_type\": data_type,\n",
    "   }\n",
    "\n",
    "   with open(f\"batch_data/{aspect}_{data_type}_batch_input_meta_data.json\", 'w') as f:\n",
    "      json.dump(batch_metadata, f, indent=4)\n",
    "      \n",
    "   print(f\"Batch file for {aspect}, {prompt_type}, and {data_type} data has been created and uploaded successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This will run the batch inference, and save the meta-data for the batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aspects = ['actionability', 'grounding_specificity', 'helpfulness','verifiability_extraction']\n",
    "# # aspects = ['verifiability_extraction']\n",
    "# prompt_types = ['definitions', 'definitions_examples', 'definitions_incontext_learning']\n",
    "\n",
    "\n",
    "import datasets\n",
    "prompt_type= 'definitions_incontext_learning'\n",
    "data_path = 'boda/review_evaluation_human_annotation'\n",
    "TEMP = 0.1\n",
    "DATA_TYPE = 'gold'\n",
    "num_examples_per_label = 5\n",
    "\n",
    "for aspect in aspects:\n",
    "\n",
    "    if aspect in ['verifiability_extraction', 'verifiability_verification']:\n",
    "        sheet_name = 'verifiability'\n",
    "    else:\n",
    "        sheet_name = aspect\n",
    "    # test_data = pd.read_excel(data_path, sheet_name=sheet_name)\n",
    "    test_data = datasets.load_dataset(data_path, name = sheet_name, split= DATA_TYPE).to_pandas()\n",
    "    ### check if test_data has id column and add one if not\n",
    "    if 'id' not in test_data.columns:\n",
    "        test_data['id'] = range(1, len(test_data) + 1)\n",
    "        with pd.ExcelWriter(data_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            test_data.to_excel(writer, sheet_name=aspect, index=False)\n",
    "        \n",
    "    chatgpt_batch_inf(test_data=test_data,\n",
    "                      aspect=aspect, \n",
    "                      prompt_type=prompt_type, \n",
    "                      temp=TEMP,data_type=DATA_TYPE,  \n",
    "                      save_path='results',\n",
    "                      num_examples_per_label=num_examples_per_label)\n",
    "\n",
    "\n",
    "\n",
    "############################### VERIFIABILITY CODE ########################################\n",
    "## check for data of verifiability_extraction if it's done, then do the verifiability_verification\n",
    "\n",
    "# extraction_results_path = 'outputs/gold_aspects_results.xlsx'\n",
    "# for file in os.listdir('batch_data'):\n",
    "#     if 'verifiability_extraction' in file and 'meta_data' in file :\n",
    "#         verifiability_extraction_batch_data = json.load(open(f'batch_data/{file}'))\n",
    "# if client.batches.retrieve(verifiability_extraction_batch_data['batch_id']).status == 'completed':\n",
    "#     ## get the saved data, and only consider the ones with labes yes\n",
    "#     test_data = pd.read_excel(extraction_results_path, sheet_name='verifiability_extraction')\n",
    "#     test_data = test_data[test_data['chatgpt_verifiability_extraction_definitions_incontext_learning_score']=='yes']\n",
    "\n",
    "#     chatgpt_batch_inf(test_data=test_data,aspect='verifiability_verification', \n",
    "#                       prompt_type=prompt_type, \n",
    "#                       temp=TEMP,data_type=DATA_TYPE,  \n",
    "#                       save_path='results',\n",
    "#                       num_examples_per_label=num_examples_per_label)\n",
    "                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_batch_and_save_results(batch_data, chatgpt_output_path):\n",
    "    aspect = batch_data['aspect']\n",
    "    prompt_type = batch_data['prompt_type']\n",
    "    batch_id = batch_data['batch_id']\n",
    "    data_type = batch_data['data_type']\n",
    "    output_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "    chatgpt_response =  client.files.content(output_file_id)\n",
    "    file_path = chatgpt_output_path\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(chatgpt_response.text + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_rationale_and_score(text):\n",
    "    # Extract rationale\n",
    "    rationale_match = re.search(r\"(?i)\\*?\\*?rationale:\\*?\\*?\\s*(.*?)(?=\\n\\s*\\*?\\*?(?i)score:\\*?\\*?|$)\", text, re.DOTALL)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else None\n",
    "    \n",
    "    # Extract score\n",
    "    score_match = re.search(r\"(?i)\\*?\\*?score:\\*?\\*?\\s*(\\d+|yes|no)\", text)\n",
    "    score = score_match.group(1).lower() if score_match else None\n",
    "    \n",
    "    \n",
    "    return {\"rationale\": rationale, \"score\": score}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "def write_results_in_original_file(batch_data, chatgpt_output_path, raw_data_path, chatgpt_input_path, write_path):\n",
    "    errors = 0\n",
    "    aspect = batch_data['aspect']\n",
    "    prompt_type = batch_data['prompt_type']\n",
    "    data_type = batch_data['data_type']\n",
    "\n",
    "    if aspect in ['verifiability_extraction', 'verifiability_verification']:\n",
    "        sheet_name = 'verifiability'\n",
    "    else:\n",
    "        sheet_name = aspect\n",
    "    \n",
    "    chatgpt_response = pd.read_json(chatgpt_output_path, lines=True)\n",
    "    raw_data_df = pd.read_excel(raw_data_path, sheet_name=sheet_name)\n",
    "    chatgpt_input = pd.read_json(chatgpt_input_path, lines=True)\n",
    "\n",
    "    ### iterate over the review_points in the raw dataframe and make sure they are aligned with the chatgpt input data\n",
    "    final_df = []\n",
    "    for i in range(raw_data_df.shape[0]):\n",
    "        id = raw_data_df.iloc[i]['id']\n",
    "\n",
    "        row = raw_data_df.iloc[i].copy()\n",
    "\n",
    "        ## if this is verifiability_verification, and we don't find the id, then the was an X case, and we don't do anything\n",
    "        if  aspect == 'verifiability_verification' and  id not in chatgpt_response['custom_id'].values:\n",
    "            final_df.append(row)\n",
    "            continue\n",
    "\n",
    "        chatgpt_row = chatgpt_response[chatgpt_response['custom_id']==id]\n",
    "\n",
    "        input = chatgpt_input[chatgpt_input['custom_id']==id].iloc[0]['body']['messages'][1]['content']\n",
    "\n",
    "        chatgpt_row = chatgpt_row.iloc[0].copy()\n",
    "\n",
    "        answer = chatgpt_row['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "        ## try to load the answer as a json object\n",
    "        rationale, score = None, None\n",
    "\n",
    "        extracted_output = extract_rationale_and_score(answer)\n",
    "        rationale, score = extracted_output['rationale'], extracted_output['score']\n",
    "\n",
    "\n",
    "        aspect_save_name = aspect\n",
    "        if aspect in ['verifiability_extraction', 'verifiability_verification']:\n",
    "            score = 'X' if score == 'no' else score\n",
    "            aspect_save_name = 'verifiability'\n",
    "\n",
    "\n",
    "        row [f'chatgpt_{aspect_save_name}_{prompt_type}_score'] = score\n",
    "        row [f'chatgpt_{aspect_save_name}_{prompt_type}_rationale'] = rationale\n",
    "        row [f'prompt'] = input\n",
    "\n",
    "        if not score:\n",
    "            errors += 1\n",
    "            print(f\"can't process {answer}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        final_df.append(row)\n",
    "        \n",
    "    final_df = pd.DataFrame(final_df)\n",
    "\n",
    "\n",
    "    if aspect in ['verifiability_extraction', 'verifiability_verification']:\n",
    "        aspect = 'verifiability'\n",
    "    ## remove all the columns that related to another aspects\n",
    "    # for any column if it has the aspect name of other aspects, remove it\n",
    "    ## grap the other aspects list\n",
    "    ## remove all cloulmns expptr for this hlise \n",
    "    exclude = ['review_point','paper_id','id','venue','focused_review','batch','prompt']\n",
    "    other_aspects = [a for a in aspects if a != aspect]\n",
    "    for col in final_df.columns:\n",
    "        if col in exclude:\n",
    "            continue\n",
    "        if aspect in col:\n",
    "            continue\n",
    "        final_df.drop(col, axis=1, inplace=True)\n",
    "                \n",
    "\n",
    "    ## rewrite the existing sheet, but keep the other sheets\n",
    "    file_path = write_path\n",
    "\n",
    "\n",
    "\n",
    "    ## if the file does not exist, create a new one\n",
    "    if not os.path.exists(file_path):\n",
    "        final_df.to_excel(file_path, sheet_name=aspect, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            final_df.to_excel(writer, sheet_name=aspect, index=False)\n",
    "    \n",
    "    print(f'number of errors for {aspect} and {prompt_type} is {errors}. The number of rows processed is {final_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run this every once and a while, and it will check if the batch file has been completed, and if so, it will retrieve the results and save them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file for verifiability_extraction and definitions_incontext_learning has been completed\n",
      "number of errors for verifiability and definitions_incontext_learning is 0. The number of rows processed is 144\n",
      "Batch file for verifiability_verification and definitions_incontext_learning has been completed\n",
      "can't process Rationale: The review point suggests conducting an ablation study on the visDial dataset to further support the proposed visual reference resolution model's effectiveness. It specifically mentions interest in the performance of ATT(+H) as shown in Figure 4 left, and questions the result if the model did not consider relevant attention retrieval from the attention memory. This comment is somewhat verifiable because it provides a specific suggestion for an ablation study, which is a common method for validating model components. However, it lacks detailed reasoning or references to support why this particular ablation study would be beneficial or necessary, leaving the authors to infer the importance of this analysis. Therefore, the comment is rated as 3: Somewhat Verifiable.\n",
      "can't process Rationale: The review point critiques the authors' claim of significant differences between methods without conducting significance testing to substantiate these claims. The reviewer provides specific examples, citing the d-BLEU and humeval scores for different models (ChatGPT, GPT-4, FeedME-2, and PPO), and highlights the minimal differences in these scores. This detailed comparison supports the reviewer's argument that the differences may not be significant without proper statistical testing. The comment is mostly verifiable as it provides a logical basis and specific data to question the claim, but it could be further strengthened by suggesting specific statistical tests or methodologies to verify the significance. Therefore, the score is 4: Mostly Verifiable.\n",
      "number of errors for verifiability and definitions_incontext_learning is 2. The number of rows processed is 142\n",
      "Batch file for actionability and definitions_incontext_learning has been completed\n",
      "number of errors for actionability and definitions_incontext_learning is 0. The number of rows processed is 227\n",
      "Batch file for grounding_specificity and definitions_incontext_learning has been completed\n",
      "number of errors for grounding_specificity and definitions_incontext_learning is 0. The number of rows processed is 287\n",
      "Batch file for helpfulness and definitions_incontext_learning has been completed\n",
      "number of errors for helpfulness and definitions_incontext_learning is 0. The number of rows processed is 187\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "for file in os.listdir('batch_data'):\n",
    "    if 'batch_input_meta_data.json' in file:\n",
    "        batch_data = json.load(open(f'batch_data/{file}'))\n",
    "\n",
    "        if client.batches.retrieve(batch_data['batch_id']).status == 'completed':\n",
    "            print(f\"Batch file for {batch_data['aspect']} and {batch_data['prompt_type']} has been completed\")\n",
    "            aspect = batch_data['aspect']\n",
    "            prompt_type = batch_data['prompt_type']\n",
    "            data_type = batch_data['data_type']\n",
    "            \n",
    "            chatgpt_output_path = f\"batch_output/{aspect}_{data_type}_chatgpt_output.jsonl\"\n",
    "\n",
    "            raw_data_path = f\"test_data/gold_human_annotations.xlsx\"\n",
    "\n",
    "            chatgpt_input_path =  f\"batch_data/{aspect}_{data_type}_batch_input.jsonl\"\n",
    "            write_path = f\"outputs/{data_type}_aspects_results.xlsx\"\n",
    "\n",
    "            retrive_batch_and_save_results (batch_data, chatgpt_output_path=chatgpt_output_path)\n",
    "\n",
    "            if aspect == 'verifiability_verification':\n",
    "                raw_data_path = write_path\n",
    "                \n",
    "            write_results_in_original_file(batch_data,\n",
    "                                        chatgpt_output_path=chatgpt_output_path,\n",
    "                                        raw_data_path=raw_data_path,\n",
    "                                        chatgpt_input_path=chatgpt_input_path,\n",
    "                                        write_path=write_path)\n",
    "            \n",
    "           \n",
    "            # ### merge the verifiability_extraction and verifiability_verification\n",
    "            # if aspect == 'verifiability_verification':\n",
    "            #     extraction_data = pd.read_excel(write_path, sheet_name='verifiability_extraction')\n",
    "            #     verification_data = pd.read_excel(write_path, sheet_name='verifiability_verification')\n",
    "            #     raw_verification_data = pd.read_excel(raw_data_path,sheet_name='verifiability')\n",
    "            #     final_verification_data = []\n",
    "\n",
    "            #     # for each row in raw_verification_data, check if the score in the extraction is \"no\", then put the score and rationale\n",
    "            #     # from the extraction, otherwise, put the score and rationale from the verification\n",
    "            #     for i in range(raw_verification_data.shape[0]):\n",
    "            #         id = raw_verification_data.iloc[i]['id']\n",
    "            #         extraction_row = extraction_data[extraction_data['id']==id]\n",
    "\n",
    "            #         if extraction_row.shape[0] == 0:\n",
    "            #             continue\n",
    "            #         extraction_row = extraction_row.iloc[0]\n",
    "\n",
    "            #         if extraction_row['chatgpt_verifiability_extraction_definitions_incontext_learning_score'] == 'no':\n",
    "            #             ## chagne the value to 'X' if the score is 'no'\n",
    "            #             extraction_row['chatgpt_verifiability_extraction_definitions_incontext_learning_score'] = 'X'\n",
    "\n",
    "            #             extraction_row['chatgpt_verifiability_definitions_incontext_learning_score'] = extraction_row['chatgpt_verifiability_extraction_definitions_incontext_learning_score']\n",
    "            #             extraction_row['chatgpt_verifiability_definitions_incontext_learning_rationale'] = extraction_row['chatgpt_verifiability_extraction_definitions_incontext_learning_rationale']\n",
    "            #             ## remove the old columns\n",
    "            #             extraction_row.drop(['chatgpt_verifiability_extraction_definitions_incontext_learning_score', 'chatgpt_verifiability_extraction_definitions_incontext_learning_rationale'], inplace=True)\n",
    "            #             final_verification_data.append(extraction_row)\n",
    "            #         else:\n",
    "            #             verification_row = verification_data[verification_data['id']==id]\n",
    "            #             if verification_row.shape[0] == 0:\n",
    "            #                 continue\n",
    "            #             verification_row = verification_row.iloc[0]\n",
    "            #             verification_row['chatgpt_verifiability_definitions_incontext_learning_score'] = verification_row['chatgpt_verifiability_verification_definitions_incontext_learning_score']\n",
    "            #             verification_row['chatgpt_verifiability_definitions_incontext_learning_rationale'] = verification_row['chatgpt_verifiability_verification_definitions_incontext_learning_rationale']\n",
    "            #             ## remove the old columns\n",
    "                        \n",
    "            #             final_verification_data.append(verification_row)\n",
    "                        \n",
    "            #     final_verification_data = pd.DataFrame(final_verification_data)\n",
    "            #     with pd.ExcelWriter(write_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            #         final_verification_data.to_excel(writer, sheet_name='verifiability', index=False)\n",
    "\n",
    "\n",
    "        else:\n",
    "            print(f\"Batch file for {batch_data['aspect']} and {batch_data['prompt_type']} is still running\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

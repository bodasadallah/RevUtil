{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# def split_into_batches(df, batch_size=1000):\n",
    "#     \"\"\"Yield consecutive batches of given size from DataFrame.\"\"\"\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         yield df.iloc[i:i + batch_size]\n",
    "\n",
    "# # Load the Excel file and sheet\n",
    "# file_path = \"main_data/human_chatgpt_filtered_data.csv\"  # Change this to your file\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Filter rows where chatgpt_discard == 0\n",
    "# filtered_df = df[df[\"chatgpt_discard\"] == 0]\n",
    "\n",
    "# print(f\"Original DataFrame: {df.shape}\")\n",
    "# print(f\"Filtered DataFrame: {filtered_df.shape}\")\n",
    "\n",
    "# ## shuffle the data\n",
    "# filtered_df = filtered_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# ## remove the chatgpt_discard column\n",
    "# filtered_df = filtered_df.drop(columns=['chatgpt_discard'])\n",
    "# # rename column point to review_point\n",
    "# filtered_df = filtered_df.rename(columns={\"point\": \"review_point\"})\n",
    "\n",
    "# # Split into batches of 1000\n",
    "# batches = list(split_into_batches(filtered_df, batch_size=1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# def clean_cell(value):\n",
    "#     \"\"\"Remove illegal characters from cell values.\"\"\"\n",
    "#     if isinstance(value, str):\n",
    "#         return re.sub(r'[\\x00-\\x1F\\x7F]', '', value)  # Remove control characters\n",
    "#     return value  # Return non-string values unchanged\n",
    "\n",
    "# for i, batch in enumerate(batches):\n",
    "#         file_path = f\"main_data/batches/batch_{i}.xlsx\"\n",
    "#         print(f\"Writing {file_path}...\")\n",
    "#         df = batch.copy()\n",
    "#         # Clean the dataframe to remove illegal characters\n",
    "#         df = df.applymap(clean_cell)\n",
    "\n",
    "#         with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "#             for aspect in ['actionability', 'grounding_specificity', 'verifiability', 'helpfulness']:\n",
    "#                 df.to_excel(writer, sheet_name=aspect, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "aspects = [ 'actionability', 'grounding_specificity','verifiability_extraction', 'helpfulness']\n",
    "all_incontext_examples = pd.read_excel('/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/test_data/in_context_examples.xlsx', sheet_name=None)\n",
    "\n",
    "\n",
    "\n",
    "BATCH = \"random\"\n",
    "data_path = f'main_data/batches/batch_{BATCH}.xlsx'\n",
    "TEMP = 0.1\n",
    "num_examples_per_label = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prompt(review_point,aspect, in_context_examples,num_examples_per_label=1,):\n",
    "\n",
    "    prompt = ''\n",
    "    examples = ''\n",
    "    examples_str = []\n",
    "    ##3 group examples by the label and choose a random example from each group\n",
    "    for label in in_context_examples[f'{aspect}_label'].unique():\n",
    "        for _ in range(num_examples_per_label):\n",
    "            ## keep sampling a line till it is not the same as the currrent review point\n",
    "            while True:\n",
    "                row = in_context_examples[in_context_examples[f'{aspect}_label']==label].sample(1)\n",
    "                row = row.iloc[0]\n",
    "                if row['review_point'] != review_point:\n",
    "                    break\n",
    "            \n",
    "            score = row[f'{aspect}_label']\n",
    "            rationale = row['rationale']\n",
    "\n",
    "\n",
    "            examples_str.append(f'''\n",
    "Review Point: {row['review_point']}\n",
    "rationale: {rationale}\n",
    "score: {score}\n",
    "''')\n",
    "    ## shuffle the list \n",
    "    random.shuffle(examples_str)\n",
    "    examples = '\\n'.join(examples_str)\n",
    "    \n",
    "    ## for verifiability, we have two tasks\n",
    "    prompt = BASE_PROMPT_EXAMPLES.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS[aspect],examples=examples)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_batch_inf(test_data,aspect, batch,temp=0, num_examples_per_label=1):\n",
    "   in_context_examples =  all_incontext_examples[aspect]\n",
    "   lines = []\n",
    "\n",
    "   for i,row in test_data.iterrows():\n",
    "      review_point = row['review_point']\n",
    "      prompt = get_prompt(review_point=review_point,aspect=aspect, in_context_examples=in_context_examples, num_examples_per_label=num_examples_per_label)   \n",
    "      line = {\n",
    "         \"custom_id\": f\"{row['id']}\", \n",
    "         \"method\": \"POST\", \n",
    "         \"url\": \"/v1/chat/completions\", \n",
    "         \"body\": {\"model\": model_name,\n",
    "         \"temperature\": temp,\n",
    "         \"messages\": \n",
    "         [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "         {\"role\": \"user\", \"content\": prompt}],}}\n",
    "      lines.append(line)\n",
    "\n",
    "   print(f'sample of the prompts is {lines[0]}')\n",
    "\n",
    "   ### Write batch input file\n",
    "   batch_file_path = f\"batch_data/main_data_batch_{batch}_{aspect}_input.jsonl\"\n",
    "   with open(batch_file_path, 'w') as f:\n",
    "      for l in lines:\n",
    "         json.dump(l, f)\n",
    "         f.write('\\n')\n",
    "\n",
    "   ### upload the batch file\n",
    "   batch_input_file = client.files.create(\n",
    "   file=open(batch_file_path, \"rb\"),\n",
    "   purpose=\"batch\")\n",
    "\n",
    "   ### create the batch request\n",
    "   batch_input_file_id = batch_input_file.id\n",
    "   batch_data = client.batches.create(\n",
    "      input_file_id=batch_input_file_id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\",\n",
    "      metadata={\n",
    "         \"description\": f\"batch file for  {aspect} model gpt-4o, temperature {temp} for batch {batch}\",\n",
    "      })\n",
    "   batch_metadata = {\n",
    "      \"batch_id\": batch_data.id,\n",
    "      \"aspect\": aspect,\n",
    "      \"batch_input_file_id\": batch_input_file_id,\n",
    "      \"batch_file_path\": batch_file_path,\n",
    "      \"batch\": batch,\n",
    "   }\n",
    "\n",
    "   with open(f\"batch_data/main_data_batch_{batch}_{aspect}_input_meta_data.json\", 'w') as f:\n",
    "      json.dump(batch_metadata, f, indent=4)\n",
    "      \n",
    "   print(f\"Batch file for {aspect}, and  batch {batch} is created and uploaded to the server\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for aspect in aspects:\n",
    "    # if aspect in ['verifiability_extraction']:\n",
    "    #     sheet_name = 'verifiability'\n",
    "    # else:\n",
    "    #     sheet_name = aspect\n",
    "    # test_data = pd.read_excel(data_path, sheet_name=sheet_name)\n",
    "    # ### check if test_data has id column and add one if not\n",
    "    # if 'id' not in test_data.columns:\n",
    "    #     test_data['id'] = range(1, len(test_data) + 1)\n",
    "    #     with pd.ExcelWriter(data_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "    #         test_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        \n",
    "#     chatgpt_batch_inf(test_data=test_data,\n",
    "#                         aspect=aspect, \n",
    "#                         batch=BATCH,\n",
    "#                         temp=TEMP,\n",
    "#                         num_examples_per_label=num_examples_per_label)\n",
    "\n",
    "\n",
    "\n",
    "############################# VERIFIABILITY CODE ########################################\n",
    "# check for data of verifiability_extraction if it's done, then do the verifiability_verification\n",
    "\n",
    "# extraction_results_path = f'outputs/main_data_batch_{BATCH}_results.xlsx'\n",
    "# for file in os.listdir('batch_data'):\n",
    "#     if 'verifiability_extraction' in file and 'meta_data' in file :\n",
    "#         verifiability_extraction_batch_data = json.load(open(f'batch_data/{file}'))\n",
    "# if client.batches.retrieve(verifiability_extraction_batch_data['batch_id']).status == 'completed':\n",
    "#     ## get the saved data, and only consider the ones with labes yes\n",
    "#     test_data = pd.read_excel(extraction_results_path, sheet_name='verifiability_extraction')\n",
    "#     test_data = test_data[test_data['chatgpt_verifiability_extraction_score']!='X']\n",
    "\n",
    "#     print(f'Verifiability extraction is done, moving to verifiability verification')\n",
    "#     print(f'Size of verifiability verification data is {test_data.shape[0]}')\n",
    "#     chatgpt_batch_inf(test_data=test_data,aspect='verifiability_verification', \n",
    "#                       batch=BATCH, \n",
    "#                       temp=TEMP,\n",
    "#                       num_examples_per_label=num_examples_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_rationale_and_score(text):\n",
    "    valid_score_values = [\"yes\", \"no\",'X', '1', '2', '3', '4', '5']\n",
    "    # Extract rationale\n",
    "    rationale_match = re.search(r\"(?i)\\*?\\*?rationale:\\*?\\*?\\s*(.*?)(?=\\n\\s*\\*?\\*?(?i)score:\\*?\\*?|$)\", text, re.DOTALL)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else None\n",
    "    \n",
    "    # Extract score (original pattern)\n",
    "    score_match = re.search(r\"(?i)\\*?\\*?score:\\*?\\*?\\s*(\\d+|yes|no)[\\s.]*$\", text)\n",
    "    score = score_match.group(1).lower() if score_match else None\n",
    "    \n",
    "    if not score:\n",
    "        # Try to extract a number within five words after \"score\" or \"rated\"\n",
    "        score_context_match = re.search(r'(?i)(score|rated)[:\\s]+((?:\\S+\\s+){0,5}?)[\"\\']?(\\d+)', text)\n",
    "        if score_context_match:\n",
    "            score = score_context_match.group(3)  # Extract the number if found\n",
    "\n",
    "    if score:\n",
    "        ## remove trailing spaces and dots from the score\n",
    "        score = score.strip().rstrip('.')\n",
    "        score = str(score)\n",
    "        # Ensure final score is valid\n",
    "        if score not in valid_score_values:\n",
    "            score = None  # Invalidate if it's not a valid score\n",
    "    \n",
    "    return {\"rationale\": rationale, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rationale': 'The review point suggests investigating the use of contextualized vectors for note embeddings by proposing three additional ablation studies. However, it does not provide any justification or reasoning for why these specific ablations are necessary or how they would contribute to the understanding of the issue. The lack of explanation or supporting evidence makes it difficult for the authors to grasp the importance or relevance of the suggested ablations. Consequently, the comment is rated as \"1: Unverifiable\" due to the absence of verifiable support or rationale.',\n",
       " 'score': '1'}"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '''rationale: The review point suggests investigating the use of contextualized vectors for note embeddings by proposing three additional ablation studies. However, it does not provide any justification or reasoning for why these specific ablations are necessary or how they would contribute to the understanding of the issue. The lack of explanation or supporting evidence makes it difficult for the authors to grasp the importance or relevance of the suggested ablations. Consequently, the comment is rated as \"1: Unverifiable\" due to the absence of verifiable support or rationale.'''\n",
    "extract_rationale_and_score(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_batch_and_save_results(batch_data, chatgpt_output_path):\n",
    "    aspect = batch_data['aspect']\n",
    "    batch_id = batch_data['batch_id']\n",
    "    output_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "    chatgpt_response =  client.files.content(output_file_id)\n",
    "    file_path = chatgpt_output_path\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(chatgpt_response.text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "def write_results_in_original_file(batch_data, chatgpt_output_path, raw_data_path, chatgpt_input_path, write_path):\n",
    "    errors = 0\n",
    "    aspect = batch_data['aspect']\n",
    "\n",
    "    if aspect in ['verifiability_extraction']:\n",
    "        sheet_name = 'verifiability'\n",
    "    elif aspect in ['verifiability_verification']:\n",
    "        sheet_name = 'verifiability_extraction'\n",
    "    else:\n",
    "        sheet_name = aspect\n",
    "    \n",
    "    chatgpt_response = pd.read_json(chatgpt_output_path, lines=True)\n",
    "    raw_data_df = pd.read_excel(raw_data_path, sheet_name=sheet_name)\n",
    "    chatgpt_input = pd.read_json(chatgpt_input_path, lines=True)\n",
    "\n",
    "    ### iterate over the review_points in the raw dataframe and make sure they are aligned with the chatgpt input data\n",
    "    final_df = []\n",
    "    for i in range(raw_data_df.shape[0]):\n",
    "        id = raw_data_df.iloc[i]['id']\n",
    "\n",
    "        row = raw_data_df.iloc[i].copy()\n",
    "\n",
    "        ## if this is verifiability_verification, and we don't find the id, then the was an X case, and we don't do anything\n",
    "        if  aspect == 'verifiability_verification' and  id not in chatgpt_response['custom_id'].values:\n",
    "            continue\n",
    "\n",
    "        chatgpt_row = chatgpt_response[chatgpt_response['custom_id']==id]\n",
    "\n",
    "        input = chatgpt_input[chatgpt_input['custom_id']==id].iloc[0]['body']['messages'][1]['content']\n",
    "\n",
    "        chatgpt_row = chatgpt_row.iloc[0].copy()\n",
    "\n",
    "        answer = chatgpt_row['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "        extracted_output = extract_rationale_and_score(answer)\n",
    "        rationale, score = extracted_output['rationale'], extracted_output['score']\n",
    "        aspect_save_name = aspect\n",
    "        if aspect in ['verifiability_extraction', 'verifiability_verification']:\n",
    "            score = 'X' if score == 'no' else score\n",
    "        if aspect in ['verifiability_verification']:\n",
    "            aspect_save_name = 'verifiability'\n",
    "        row [f'chatgpt_{aspect_save_name}_score'] = score\n",
    "        row [f'chatgpt_{aspect_save_name}_rationale'] = rationale\n",
    "        row [f'prompt'] = input\n",
    "\n",
    "        if not score:\n",
    "            errors += 1\n",
    "            print(f\"can't process {answer}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        final_df.append(row)\n",
    "\n",
    "    final_df = pd.DataFrame(final_df)\n",
    "\n",
    "    ## if the aspect is verifiability_verification, we need to merge the results with the extraction results\n",
    "    ## rename the aspect to verifiability\n",
    "    if aspect == 'verifiability_verification':\n",
    "        extraction_results = raw_data_df[raw_data_df['chatgpt_verifiability_extraction_score']=='X']\n",
    "        extraction_results['chatgpt_verifiability_score'] = extraction_results['chatgpt_verifiability_extraction_score']\n",
    "        extraction_results['chatgpt_verifiability_rationale'] = extraction_results['chatgpt_verifiability_extraction_rationale']\n",
    "        extraction_results.drop(['chatgpt_verifiability_extraction_score','chatgpt_verifiability_extraction_rationale'], axis=1, inplace=True)\n",
    "        final_df = pd.concat([final_df, extraction_results], axis=0)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    if aspect in [ 'verifiability_verification']:\n",
    "        aspect = 'verifiability'\n",
    "    ## remove all the columns that related to another aspects\n",
    "    # for any column if it has the aspect name of other aspects, remove it\n",
    "    ## grap the other aspects list\n",
    "    ## remove all cloulmns expptr for this hlise \n",
    "    exclude = ['review_point','paper_id','id','venue','focused_review','batch','prompt']\n",
    "    for col in final_df.columns:\n",
    "        if col in exclude:\n",
    "            continue\n",
    "        if aspect in col:\n",
    "            continue\n",
    "        final_df.drop(col, axis=1, inplace=True)         \n",
    "    ## rewrite the existing sheet, but keep the other sheets\n",
    "    file_path = write_path\n",
    "\n",
    "    ## if the file does not exist, create a new one\n",
    "    if not os.path.exists(file_path):\n",
    "        final_df.to_excel(file_path, sheet_name=aspect, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            final_df.to_excel(writer, sheet_name=aspect, index=False)\n",
    "    \n",
    "    print(final_df[f'chatgpt_{aspect_save_name}_score'].value_counts())\n",
    "\n",
    "    print(f'number of errors for {aspect} is {errors}. The number of rows processed is {final_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file for grounding_specificity and batch random has been completed\n",
      "chatgpt_grounding_specificity_score\n",
      "3    88\n",
      "5    65\n",
      "1    24\n",
      "2    15\n",
      "4     8\n",
      "Name: count, dtype: int64\n",
      "number of errors for grounding_specificity is 0. The number of rows processed is 200\n",
      "Batch file for helpfulness and batch random has been completed\n",
      "chatgpt_helpfulness_score\n",
      "3    97\n",
      "4    71\n",
      "2    19\n",
      "5    10\n",
      "1     3\n",
      "Name: count, dtype: int64\n",
      "number of errors for helpfulness is 0. The number of rows processed is 200\n",
      "Batch file for verifiability_verification and batch random has been completed\n",
      "can't process rationale: The review point presents a claim regarding the difficulty of implementing and testing predictions with simulations in the context of recognition lists and old vs. new judgments. The reviewer acknowledges that the argument makes sense in principle but raises concerns about practical implementation. However, the comment lacks specific examples, references, or detailed reasoning to support the claim about the challenges of implementing such a list. Without these elements, the claim remains somewhat abstract and difficult for authors to address effectively. Therefore, the comment is classified as 2: Borderline Verifiable, as it provides an initial rationale but lacks sufficient detail to be fully verifiable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3689161/329380370.py:64: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extraction_results['chatgpt_verifiability_score'] = extraction_results['chatgpt_verifiability_extraction_score']\n",
      "/tmp/ipykernel_3689161/329380370.py:65: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extraction_results['chatgpt_verifiability_rationale'] = extraction_results['chatgpt_verifiability_extraction_rationale']\n",
      "/tmp/ipykernel_3689161/329380370.py:66: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extraction_results.drop(['chatgpt_verifiability_extraction_score','chatgpt_verifiability_extraction_rationale'], axis=1, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt_verifiability_score\n",
      "3    92\n",
      "1    40\n",
      "2    24\n",
      "X    24\n",
      "4    13\n",
      "5     6\n",
      "Name: count, dtype: int64\n",
      "number of errors for verifiability is 1. The number of rows processed is 199\n",
      "Batch file for verifiability_extraction and batch random has been completed\n",
      "chatgpt_verifiability_extraction_score\n",
      "yes    176\n",
      "X       24\n",
      "Name: count, dtype: int64\n",
      "number of errors for verifiability_extraction is 0. The number of rows processed is 200\n",
      "Batch file for actionability and batch random has been completed\n",
      "chatgpt_actionability_score\n",
      "3    72\n",
      "5    41\n",
      "1    40\n",
      "2    27\n",
      "4    20\n",
      "Name: count, dtype: int64\n",
      "number of errors for actionability is 0. The number of rows processed is 200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "for file in os.listdir('batch_data'):\n",
    "    if 'input_meta_data.json' in file and str(BATCH) in file:\n",
    "        batch_data = json.load(open(f'batch_data/{file}'))\n",
    "\n",
    "        if client.batches.retrieve(batch_data['batch_id']).status == 'completed':\n",
    "            print(f\"Batch file for {batch_data['aspect']} and batch {batch_data['batch']} has been completed\")\n",
    "            aspect = batch_data['aspect']\n",
    "\n",
    "            chatgpt_output_path = f\"batch_output/main_data_batch_{BATCH}_{aspect}_chatgpt_output.jsonl\"\n",
    "\n",
    "            raw_data_path = f\"main_data/batches/batch_{BATCH}.xlsx\"\n",
    "\n",
    "            chatgpt_input_path =  f\"batch_data/main_data_batch_{BATCH}_{aspect}_input.jsonl\"\n",
    "            write_path = f\"outputs/main_data_batch_{BATCH}_results.xlsx\"\n",
    "\n",
    "            retrive_batch_and_save_results (batch_data, chatgpt_output_path=chatgpt_output_path)\n",
    "\n",
    "            if aspect == 'verifiability_verification':\n",
    "                raw_data_path = write_path\n",
    "                \n",
    "            write_results_in_original_file(batch_data,\n",
    "                                        chatgpt_output_path=chatgpt_output_path,\n",
    "                                        raw_data_path=raw_data_path,\n",
    "                                        chatgpt_input_path=chatgpt_input_path,\n",
    "                                        write_path=write_path)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Batch file for {batch_data['aspect']} and batch {batch_data['batch']} has not been completed yet\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

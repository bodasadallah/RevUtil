{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# def split_into_batches(df, batch_size=1000):\n",
    "#     \"\"\"Yield consecutive batches of given size from DataFrame.\"\"\"\n",
    "#     for i in range(0, len(df), batch_size):\n",
    "#         yield df.iloc[i:i + batch_size]\n",
    "\n",
    "# # Load the Excel file and sheet\n",
    "# file_path = \"main_data/500_points.csv\"  # Change this to your file\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Filter rows where chatgpt_discard == 0\n",
    "# filtered_df = df[df[\"chatgpt_discard\"] == 0]\n",
    "\n",
    "# print(f\"Original DataFrame: {df.shape}\")\n",
    "# print(f\"Filtered DataFrame: {filtered_df.shape}\")\n",
    "\n",
    "# ## shuffle the data\n",
    "# filtered_df = filtered_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# ## remove the chatgpt_discard column\n",
    "# filtered_df = filtered_df.drop(columns=['chatgpt_discard'])\n",
    "# # rename column point to review_point\n",
    "# filtered_df = filtered_df.rename(columns={\"point\": \"review_point\"})\n",
    "\n",
    "# # Take only the first 10000 rows\n",
    "# filtered_df = filtered_df.head(500)\n",
    "\n",
    "# # Split into batches of 1000\n",
    "# batches = list(split_into_batches(filtered_df, batch_size=5000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import re\n",
    "\n",
    "# def clean_cell(value):\n",
    "#     \"\"\"Remove illegal characters from cell values.\"\"\"\n",
    "#     if isinstance(value, str):\n",
    "#         return re.sub(r'[\\x00-\\x1F\\x7F]', '', value)  # Remove control characters\n",
    "#     return value  # Return non-string values unchanged\n",
    "\n",
    "# for i, batch in enumerate(batches):\n",
    "#         file_path = f\"main_data/batches/batch_{i+3}.xlsx\"\n",
    "#         print(f\"Writing {file_path}...\")\n",
    "#         df = batch.copy()\n",
    "#         # Clean the dataframe to remove illegal characters\n",
    "#         df = df.applymap(clean_cell)\n",
    "\n",
    "#         with pd.ExcelWriter(file_path, engine='openpyxl') as writer:\n",
    "#             for aspect in ['actionability', 'grounding_specificity', 'verifiability', 'helpfulness']:\n",
    "#                 df.to_excel(writer, sheet_name=aspect, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from openai import OpenAI, AsyncOpenAI\n",
    "import os\n",
    "import sys\n",
    "import dotenv\n",
    "import datasets\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "dotenv.load_dotenv()\n",
    "from prompt import *\n",
    "client = OpenAI(api_key=os.environ.get(\"review_evaluation_mbzuai\"))\n",
    "\n",
    "model_name = 'gpt-4o'\n",
    "\n",
    "import json \n",
    "import pandas as pd\n",
    "aspects = [ 'actionability', 'grounding_specificity','verifiability_extraction', 'helpfulness']\n",
    "# aspects = ['helpfulness']\n",
    "\n",
    "all_incontext_examples = pd.read_excel('/home/abdelrahman.sadallah/mbzuai/review_rewrite/chatgpt/test_data/in_context_examples.xlsx', sheet_name=None)\n",
    "\n",
    "\n",
    "\n",
    "BATCH = \"silver\"\n",
    "# data_path = f'main_data/batches/batch_{BATCH}.xlsx'\n",
    "TEMP = 0.1\n",
    "num_examples_per_label = 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_prompt(review_point,aspect, in_context_examples,num_examples_per_label=1,):\n",
    "\n",
    "    prompt = ''\n",
    "    examples = ''\n",
    "    examples_str = []\n",
    "    ##3 group examples by the label and choose a random example from each group\n",
    "    for label in in_context_examples[f'{aspect}_label'].unique():\n",
    "        for _ in range(num_examples_per_label):\n",
    "            ## keep sampling a line till it is not the same as the currrent review point\n",
    "            while True:\n",
    "                row = in_context_examples[in_context_examples[f'{aspect}_label']==label].sample(1)\n",
    "                row = row.iloc[0]\n",
    "                if row['review_point'] != review_point:\n",
    "                    break\n",
    "            \n",
    "            score = row[f'{aspect}_label']\n",
    "            rationale = row['rationale']\n",
    "\n",
    "\n",
    "            examples_str.append(f'''\n",
    "Review Point: {row['review_point']}\n",
    "rationale: {rationale}\n",
    "score: {score}\n",
    "''')\n",
    "    ## shuffle the list \n",
    "    random.shuffle(examples_str)\n",
    "    examples = '\\n'.join(examples_str)\n",
    "    \n",
    "    ## for verifiability, we have two tasks\n",
    "    prompt = BASE_PROMPT_EXAMPLES.format(review_point=review_point,aspect=aspect,aspect_description=ASPECTS_WITH_EXAMPLES[aspect],examples=examples)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_batch_inf(test_data,aspect, batch,temp=0, num_examples_per_label=1):\n",
    "   in_context_examples =  all_incontext_examples[aspect]\n",
    "   lines = []\n",
    "\n",
    "   for i,row in test_data.iterrows():\n",
    "      review_point = row['review_point']\n",
    "      prompt = get_prompt(review_point=review_point,aspect=aspect, in_context_examples=in_context_examples, num_examples_per_label=num_examples_per_label)   \n",
    "      line = {\n",
    "         \"custom_id\": f\"{row['id']}\", \n",
    "         \"method\": \"POST\", \n",
    "         \"url\": \"/v1/chat/completions\", \n",
    "         \"body\": {\"model\": model_name,\n",
    "         \"temperature\": temp,\n",
    "         \"messages\": \n",
    "         [{\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "         {\"role\": \"user\", \"content\": prompt}],}}\n",
    "      lines.append(line)\n",
    "\n",
    "   print(f'sample of the prompts is {lines[0]}')\n",
    "\n",
    "   ### Write batch input file\n",
    "   batch_file_path = f\"batch_data/main_data_batch_{batch}_{aspect}_input.jsonl\"\n",
    "   with open(batch_file_path, 'w') as f:\n",
    "      for l in lines:\n",
    "         json.dump(l, f)\n",
    "         f.write('\\n')\n",
    "\n",
    "   ### upload the batch file\n",
    "   batch_input_file = client.files.create(\n",
    "   file=open(batch_file_path, \"rb\"),\n",
    "   purpose=\"batch\")\n",
    "\n",
    "   ### create the batch request\n",
    "   batch_input_file_id = batch_input_file.id\n",
    "   batch_data = client.batches.create(\n",
    "      input_file_id=batch_input_file_id,\n",
    "      endpoint=\"/v1/chat/completions\",\n",
    "      completion_window=\"24h\",\n",
    "      metadata={\n",
    "         \"description\": f\"batch file for  {aspect} model gpt-4o, temperature {temp} for batch {batch}\",\n",
    "      })\n",
    "   batch_metadata = {\n",
    "      \"batch_id\": batch_data.id,\n",
    "      \"aspect\": aspect,\n",
    "      \"batch_input_file_id\": batch_input_file_id,\n",
    "      \"batch_file_path\": batch_file_path,\n",
    "      \"batch\": batch,\n",
    "   }\n",
    "\n",
    "   with open(f\"batch_data/main_data_batch_{batch}_{aspect}_input_meta_data.json\", 'w') as f:\n",
    "      json.dump(batch_metadata, f, indent=4)\n",
    "      \n",
    "   print(f\"Batch file for {aspect}, and  batch {batch} is created and uploaded to the server\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for aspect in aspects:\n",
    "#     if aspect in ['verifiability_extraction']:\n",
    "#         sheet_name = 'verifiability'\n",
    "#     else:\n",
    "#         sheet_name = aspect\n",
    "\n",
    "\n",
    "    # # test_data = pd.read_excel(data_path, sheet_name=sheet_name)\n",
    "    # test_data = datasets.load_dataset('boda/review_evaluation_human_annotation', name=sheet_name, split=BATCH).to_pandas()\n",
    "    # ### check if test_data has id column and add one if not\n",
    "    # if 'id' not in test_data.columns:\n",
    "    #     test_data['id'] = range(1, len(test_data) + 1)\n",
    "    #     with pd.ExcelWriter(data_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "    #         test_data.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    # print(f'Size of {aspect} data is {test_data.shape[0]}')\n",
    "    # chatgpt_batch_inf(test_data=test_data,\n",
    "    #                     aspect=aspect, \n",
    "    #                     batch=BATCH,\n",
    "    #                     temp=TEMP,\n",
    "    #                     num_examples_per_label=num_examples_per_label)\n",
    "\n",
    "\n",
    "\n",
    "######################### VERIFIABILITY CODE ########################################\n",
    "# check for data of verifiability_extraction if it's done, then do the verifiability_verification\n",
    "\n",
    "# extraction_results_path = f'outputs/main_data_batch_{BATCH}_results.xlsx'\n",
    "# for file in os.listdir('batch_data'):\n",
    "#     if 'verifiability_extraction' in file and 'meta_data' in file and BATCH in file :\n",
    "#         verifiability_extraction_batch_data = json.load(open(f'batch_data/{file}'))\n",
    "# if client.batches.retrieve(verifiability_extraction_batch_data['batch_id']).status == 'completed':\n",
    "#     ## get the saved data, and only consider the ones with labes yes\n",
    "#     test_data = pd.read_excel(extraction_results_path, sheet_name='verifiability_extraction')\n",
    "#     test_data = test_data[test_data['chatgpt_verifiability_extraction_score']!='X']\n",
    "\n",
    "#     print(f'Verifiability extraction is done, moving to verifiability verification')\n",
    "#     print(f'Size of verifiability verification data is {test_data.shape[0]}')\n",
    "#     chatgpt_batch_inf(test_data=test_data,aspect='verifiability_verification', \n",
    "#                       batch=BATCH, \n",
    "#                       temp=TEMP,\n",
    "#                       num_examples_per_label=num_examples_per_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_rationale_and_score(text):\n",
    "    valid_score_values = {\"yes\", \"no\", \"x\", \"1\", \"2\", \"3\", \"4\", \"5\"}\n",
    "    \n",
    "    # Extract rationale\n",
    "    rationale_match = re.search(r\"(?i)\\*?\\*?rationale:\\*?\\*?\\s*(.*?)(?=\\n\\s*\\*?\\*?(?i)score:\\*?\\*?|$)\", text, re.DOTALL)\n",
    "    rationale = rationale_match.group(1).strip() if rationale_match else None\n",
    "    \n",
    "    # Extract score\n",
    "    score_match = re.search(r\"(?i)\\*?\\*?score:\\*?\\*?\\s*([a-zA-Z0-9]+)\", text)\n",
    "    score = score_match.group(1).strip().lower() if score_match else None\n",
    "    \n",
    "    if not score:\n",
    "        # Try to extract a number within five words after \"score\" or \"rated\"\n",
    "        score_context_match = re.search(r'(?i)(score|rated)[:\\s]+((?:\\S+\\s+){0,5}?)[\"\\']?(\\d+)', text)\n",
    "        if score_context_match:\n",
    "            score = score_context_match.group(3)  # Extract the number if found\n",
    "\n",
    "    if score:\n",
    "        ## remove trailing spaces and dots from the score\n",
    "        score = score.strip().rstrip('.')\n",
    "        score = str(score)\n",
    "        # Ensure final score is valid\n",
    "        if score not in valid_score_values:\n",
    "            score = None  # Invalidate if it's not a valid score\n",
    "    \n",
    "    return {\"rationale\": rationale, \"score\": score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rationale': 'The review point suggests investigating the use of contextualized vectors for note embeddings by proposing three additional ablation studies. However, it does not provide any justification or reasoning for why these specific ablations are necessary or how they would contribute to the understanding of the issue. The lack of explanation or supporting evidence makes it difficult for the authors to grasp the importance or relevance of the suggested ablations. Consequently, the comment is rated as \"1: Unverifiable\" due to the absence of verifiable support or rationale.',\n",
       " 'score': '1'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = '''rationale: The review point suggests investigating the use of contextualized vectors for note embeddings by proposing three additional ablation studies. However, it does not provide any justification or reasoning for why these specific ablations are necessary or how they would contribute to the understanding of the issue. The lack of explanation or supporting evidence makes it difficult for the authors to grasp the importance or relevance of the suggested ablations. Consequently, the comment is rated as \"1: Unverifiable\" due to the absence of verifiable support or rationale.'''\n",
    "extract_rationale_and_score(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrive_batch_and_save_results(batch_data, chatgpt_output_path):\n",
    "    aspect = batch_data['aspect']\n",
    "    batch_id = batch_data['batch_id']\n",
    "    output_file_id = client.batches.retrieve(batch_id).output_file_id\n",
    "    chatgpt_response =  client.files.content(output_file_id)\n",
    "    file_path = chatgpt_output_path\n",
    "    with open(file_path, 'w') as file:\n",
    "        file.write(chatgpt_response.text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def remove_illegal_chars(value):\n",
    "    \"\"\"Remove illegal characters from strings.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        return \"\".join(c for c in value if c.isprintable())\n",
    "    return value\n",
    "\n",
    "\n",
    "\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness']\n",
    "def write_results_in_original_file(batch_data, chatgpt_output_path, raw_data_path, chatgpt_input_path, write_path):\n",
    "    errors = 0\n",
    "    aspect = batch_data['aspect']\n",
    "\n",
    "    if aspect in ['verifiability_extraction']:\n",
    "        sheet_name = 'verifiability'\n",
    "    elif aspect in ['verifiability_verification']:\n",
    "        sheet_name = 'verifiability_extraction'\n",
    "    else:\n",
    "        sheet_name = aspect\n",
    "    \n",
    "    chatgpt_response = pd.read_json(chatgpt_output_path, lines=True)\n",
    "\n",
    "    if 'boda' in raw_data_path and aspect not in ['verifiability_verification']:\n",
    "        raw_data_df = datasets.load_dataset('boda/review_evaluation_human_annotation', name=sheet_name, split=BATCH).to_pandas()\n",
    "    else:\n",
    "        raw_data_df = pd.read_excel(raw_data_path, sheet_name=sheet_name)\n",
    "\n",
    "        \n",
    "    chatgpt_input = pd.read_json(chatgpt_input_path, lines=True)\n",
    "\n",
    "    ### iterate over the review_points in the raw dataframe and make sure they are aligned with the chatgpt input data\n",
    "    final_df = []\n",
    "    for i in range(raw_data_df.shape[0]):\n",
    "        id = raw_data_df.iloc[i]['id']\n",
    "\n",
    "        row = raw_data_df.iloc[i].copy()\n",
    "\n",
    "        ## if this is verifiability_verification, and we don't find the id, then the was an X case, and we don't do anything\n",
    "        if  aspect == 'verifiability_verification' and  id not in chatgpt_response['custom_id'].values:\n",
    "            continue\n",
    "\n",
    "        chatgpt_row = chatgpt_response[chatgpt_response['custom_id']==id]\n",
    "\n",
    "        ## if the row is not found, then it was failed, then skip it\n",
    "        if chatgpt_row.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        input = chatgpt_input[chatgpt_input['custom_id']==id].iloc[0]['body']['messages'][1]['content']\n",
    "\n",
    "        chatgpt_row = chatgpt_row.iloc[0].copy()\n",
    "\n",
    "        answer = chatgpt_row['response']['body']['choices'][0]['message']['content']\n",
    "\n",
    "        extracted_output = extract_rationale_and_score(answer)\n",
    "        rationale, score = extracted_output['rationale'], extracted_output['score']\n",
    "        aspect_save_name = aspect\n",
    "        if aspect in ['verifiability_extraction', 'verifiability_verification']:\n",
    "            score = 'X' if score == 'no' else score\n",
    "        if aspect in ['verifiability_verification']:\n",
    "            aspect_save_name = 'verifiability'\n",
    "        row [f'chatgpt_{aspect_save_name}_score'] = score\n",
    "        row [f'chatgpt_{aspect_save_name}_rationale'] = rationale\n",
    "        row [f'prompt'] = input\n",
    "\n",
    "        if not score:\n",
    "            errors += 1\n",
    "            print(f\"can't process {answer}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        final_df.append(row)\n",
    "\n",
    "    final_df = pd.DataFrame(final_df)\n",
    "\n",
    "    ## if the aspect is verifiability_verification, we need to merge the results with the extraction results\n",
    "    ## rename the aspect to verifiability\n",
    "    if aspect == 'verifiability_verification':\n",
    "        extraction_results = raw_data_df[raw_data_df['chatgpt_verifiability_extraction_score']=='X']\n",
    "        extraction_results['chatgpt_verifiability_score'] = extraction_results['chatgpt_verifiability_extraction_score']\n",
    "        extraction_results['chatgpt_verifiability_rationale'] = extraction_results['chatgpt_verifiability_extraction_rationale']\n",
    "        extraction_results.drop(['chatgpt_verifiability_extraction_score','chatgpt_verifiability_extraction_rationale'], axis=1, inplace=True)\n",
    "        final_df = pd.concat([final_df, extraction_results], axis=0)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "    if aspect in [ 'verifiability_verification']:\n",
    "        aspect = 'verifiability'\n",
    "    ## remove all the columns that related to another aspects\n",
    "    # for any column if it has the aspect name of other aspects, remove it\n",
    "    ## grap the other aspects list\n",
    "    ## remove all cloulmns expptr for this hlise \n",
    "    main_aspect_name = aspect if aspect not in ['verifiability_extraction'] else 'verifiability'\n",
    "    exclude = ['review_point','paper_id','id','venue','focused_review','batch','prompt', f'{main_aspect_name}_label']\n",
    "\n",
    "    for col in final_df.columns:\n",
    "\n",
    "        if col in exclude:\n",
    "            continue\n",
    "        if aspect in col:\n",
    "            continue\n",
    "        if col in aspect:\n",
    "            continue\n",
    "        final_df.drop(col, axis=1, inplace=True)         \n",
    "    ## rewrite the existing sheet, but keep the other sheets\n",
    "    file_path = write_path\n",
    "\n",
    "    final_df = final_df.applymap(remove_illegal_chars)\n",
    "\n",
    "\n",
    "    ## if the file does not exist, create a new one\n",
    "    if not os.path.exists(file_path):\n",
    "        final_df.to_excel(file_path, sheet_name=aspect, index=False)\n",
    "    else:\n",
    "        with pd.ExcelWriter(file_path, mode='a', engine='openpyxl', if_sheet_exists='replace') as writer:\n",
    "            final_df.to_excel(writer, sheet_name=aspect, index=False)\n",
    "    \n",
    "    print(final_df[f'chatgpt_{aspect_save_name}_score'].value_counts())\n",
    "    print(final_df.columns)\n",
    "\n",
    "    print(f'number of errors for {aspect} is {errors}. The number of rows processed is {final_df.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file for helpfulness and batch silver has been completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1043679/2945641965.py:113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  final_df = final_df.applymap(remove_illegal_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt_helpfulness_score\n",
      "3    363\n",
      "4    266\n",
      "2     99\n",
      "5     53\n",
      "1     30\n",
      "Name: count, dtype: int64\n",
      "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'batch',\n",
      "       'helpfulness', 'helpfulness_label', 'helpfulness_label_type', 'id',\n",
      "       'chatgpt_helpfulness_score', 'chatgpt_helpfulness_rationale', 'prompt'],\n",
      "      dtype='object')\n",
      "number of errors for helpfulness is 0. The number of rows processed is 811\n",
      "Batch file for actionability and batch silver has been completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1043679/2945641965.py:113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  final_df = final_df.applymap(remove_illegal_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt_actionability_score\n",
      "3    252\n",
      "2    169\n",
      "1    142\n",
      "5    113\n",
      "4     88\n",
      "Name: count, dtype: int64\n",
      "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'batch',\n",
      "       'actionability', 'actionability_label', 'actionability_label_type',\n",
      "       'id', 'chatgpt_actionability_score', 'chatgpt_actionability_rationale',\n",
      "       'prompt'],\n",
      "      dtype='object')\n",
      "number of errors for actionability is 0. The number of rows processed is 764\n",
      "Batch file for verifiability_verification and batch silver has been completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1043679/2945641965.py:84: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extraction_results['chatgpt_verifiability_score'] = extraction_results['chatgpt_verifiability_extraction_score']\n",
      "/tmp/ipykernel_1043679/2945641965.py:85: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extraction_results['chatgpt_verifiability_rationale'] = extraction_results['chatgpt_verifiability_extraction_rationale']\n",
      "/tmp/ipykernel_1043679/2945641965.py:86: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  extraction_results.drop(['chatgpt_verifiability_extraction_score','chatgpt_verifiability_extraction_rationale'], axis=1, inplace=True)\n",
      "/tmp/ipykernel_1043679/2945641965.py:113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  final_df = final_df.applymap(remove_illegal_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt_verifiability_score\n",
      "3    304\n",
      "X    206\n",
      "1    200\n",
      "4     64\n",
      "2     52\n",
      "5     12\n",
      "Name: count, dtype: int64\n",
      "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'batch',\n",
      "       'verifiability', 'verifiability_label', 'id',\n",
      "       'chatgpt_verifiability_extraction_score',\n",
      "       'chatgpt_verifiability_extraction_rationale', 'prompt',\n",
      "       'chatgpt_verifiability_score', 'chatgpt_verifiability_rationale'],\n",
      "      dtype='object')\n",
      "number of errors for verifiability is 0. The number of rows processed is 838\n",
      "Batch file for verifiability_extraction and batch silver has been completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1043679/2945641965.py:113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  final_df = final_df.applymap(remove_illegal_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt_verifiability_extraction_score\n",
      "yes    632\n",
      "X      206\n",
      "Name: count, dtype: int64\n",
      "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'batch',\n",
      "       'verifiability', 'verifiability_label', 'id',\n",
      "       'chatgpt_verifiability_extraction_score',\n",
      "       'chatgpt_verifiability_extraction_rationale', 'prompt'],\n",
      "      dtype='object')\n",
      "number of errors for verifiability_extraction is 0. The number of rows processed is 838\n",
      "Batch file for grounding_specificity and batch silver has been completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1043679/2945641965.py:113: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  final_df = final_df.applymap(remove_illegal_chars)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chatgpt_grounding_specificity_score\n",
      "3    334\n",
      "5    155\n",
      "1     79\n",
      "2     66\n",
      "4     37\n",
      "Name: count, dtype: int64\n",
      "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'batch',\n",
      "       'grounding_specificity', 'grounding_specificity_label',\n",
      "       'grounding_specificity_label_type', 'id',\n",
      "       'chatgpt_grounding_specificity_score',\n",
      "       'chatgpt_grounding_specificity_rationale', 'prompt'],\n",
      "      dtype='object')\n",
      "number of errors for grounding_specificity is 0. The number of rows processed is 671\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "for file in os.listdir('batch_data'):\n",
    "    if 'main_data' in file and 'input_meta_data.json' in file and str(BATCH) in file:\n",
    "        batch_data = json.load(open(f'batch_data/{file}'))\n",
    "        # print(file)\n",
    "        if client.batches.retrieve(batch_data['batch_id']).status == 'completed':\n",
    "            print(f\"Batch file for {batch_data['aspect']} and batch {batch_data['batch']} has been completed\")\n",
    "            aspect = batch_data['aspect']\n",
    "\n",
    "            chatgpt_output_path = f\"batch_output/main_data_batch_{BATCH}_{aspect}_chatgpt_output.jsonl\"\n",
    "\n",
    "            # raw_data_path = f\"main_data/batches/batch_{BATCH}.xlsx\"\n",
    "            raw_data_path = f'boda/review_evaluation_human_annotation'\n",
    "            \n",
    "\n",
    "            chatgpt_input_path =  f\"batch_data/main_data_batch_{BATCH}_{aspect}_input.jsonl\"\n",
    "            write_path = f\"outputs/main_data_batch_{BATCH}_results.xlsx\"\n",
    "\n",
    "            retrive_batch_and_save_results (batch_data, chatgpt_output_path=chatgpt_output_path)\n",
    "\n",
    "            if aspect == 'verifiability_verification':\n",
    "                raw_data_path = write_path\n",
    "                \n",
    "            write_results_in_original_file(batch_data,\n",
    "                                        chatgpt_output_path=chatgpt_output_path,\n",
    "                                        raw_data_path=raw_data_path,\n",
    "                                        chatgpt_input_path=chatgpt_input_path,\n",
    "                                        write_path=write_path)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Batch file for {batch_data['aspect']} and batch {batch_data['batch']} has not been completed yet\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_excel('outputs/main_data_batch_gold_results.xlsx', sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['review_point', 'paper_id', 'venue', 'focused_review', 'batch',\n",
       "       'verifiability', 'verifiability_label', 'id',\n",
       "       'chatgpt_verifiability_extraction_score',\n",
       "       'chatgpt_verifiability_extraction_rationale', 'prompt',\n",
       "       'chatgpt_verifiability_score', 'chatgpt_verifiability_rationale'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['verifiability'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pick a random row from the data, and print the revew point, and the rationale. do this for each asepct\n",
    "## write this into a file\n",
    "with open('outputs/sample_results.txt', 'w') as f:\n",
    "    for aspect in data.keys():\n",
    "        sample = data[aspect].sample(1)\n",
    "        review_point = sample['review_point'].values[0]\n",
    "        rationale = sample[f'chatgpt_{aspect}_rationale'].values[0]\n",
    "        f.write(f'Aspect: {aspect}\\n')\n",
    "        f.write(f'Review Point: {review_point}\\n\\n')\n",
    "        f.write(f'Rationale: {rationale}\\n\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

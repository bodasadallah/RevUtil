{"custom_id": "1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point:\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\n\n"}]}}
{"custom_id": "2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point:\n- Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.\n\n"}]}}
{"custom_id": "3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point:\n2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.\n\n"}]}}
{"custom_id": "4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point:\n- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).\n\n"}]}}
{"custom_id": "5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point:\n- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.\n\n"}]}}
{"custom_id": "6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point:\n- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n\n"}]}}
{"custom_id": "7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point:\n1. Symbols are a little bit complicated and takes a lot of time to understand.\n\n"}]}}
{"custom_id": "8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point:\n- A number of claims from this paper would benefit from more in-depth analysis.\n\n"}]}}
{"custom_id": "9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point:\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n\n"}]}}
{"custom_id": "10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point:\n2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.\n\n"}]}}
{"custom_id": "11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point:\n- Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n\n"}]}}
{"custom_id": "12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point:\n- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.\n\n"}]}}
{"custom_id": "13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point:\n2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n\n"}]}}
{"custom_id": "14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point:\n- The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning\n\n"}]}}
{"custom_id": "15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point:\n1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n\n"}]}}
{"custom_id": "16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point:\n- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].\n\n"}]}}
{"custom_id": "17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point:\n1) The proposed methods - contrastive training objective and contrastive search - are two independent methods that have little inner connection on both the intuition and the algorithm.\n\n"}]}}
{"custom_id": "18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point:\n- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.\n\n"}]}}
{"custom_id": "19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point:\n- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n\n"}]}}
{"custom_id": "20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point:\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n\n"}]}}
{"custom_id": "21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point:\n1. The introduction to orthogonality in Part 2 could be more detailed.\n\n"}]}}
{"custom_id": "22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point:\n2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive.\n\n"}]}}
{"custom_id": "23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point:\n1.The main part can be more concise (especially for the introduction part)and including empirical results.\n\n"}]}}
{"custom_id": "24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point:\n1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.\n\n"}]}}
{"custom_id": "25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point:\n- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?\n\n"}]}}
{"custom_id": "26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point:\n- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.\n\n"}]}}
{"custom_id": "27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point:\n- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.\n\n"}]}}
{"custom_id": "28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point:\n* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.\n\n"}]}}
{"custom_id": "29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point:\n1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not ablated. How important is the added complexity? Will one IN do?\n\n"}]}}
{"custom_id": "30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point:\n3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.\n\n"}]}}
{"custom_id": "31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point:\n1: \"The uncertainty is defined based on the posterior distribution.\" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data. p.\n\n"}]}}
{"custom_id": "32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point:\n3. Sensitivity to other hyper-parameters. Minor Comments on language usage: (for example) 1. \"we typically considers\" in the above of (7) 2. \"two permutation\" in the above of Theorem 1 3. \"until converge\" in the above of (14) 4. ...... Please proofread the paper and fix all language problems.\n\n"}]}}
{"custom_id": "33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point:\n* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.\n\n"}]}}
{"custom_id": "34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point:\n- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.\n\n"}]}}
{"custom_id": "35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point:\n- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.\n\n"}]}}
{"custom_id": "36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point:\n1. Missing some baselines: For code completion tasks, I suggest that the authors compare with existing code completion commercial applications, such as Copilot. It can be tested on a smaller subset of RepoEval, and it is essential to compare with these state-of-the-art code completion systems.\n\n"}]}}
{"custom_id": "37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: - The hGRU architecture seems pretty ad-hoc and not very well motivated.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that the \"hGRU architecture seems pretty ad-hoc and not very well motivated.\" However, it does not provide any specific details or examples to clarify what makes the architecture appear \"ad-hoc\" or \"not well motivated.\" \n   - The lack of specific guidance or examples makes the action vague. The authors do not have a clear understanding of what specific changes or improvements are needed.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely expresses a general concern or opinion about the architecture.\n   - There is no implicit action that can be easily inferred either, as the comment does not hint at any specific direction or area that needs attention.\n\n3. **Conclusion**:\n   - Given the lack of explicit or implicit actions and the vagueness of the comment, it does not provide any meaningful information or guidance for the authors to improve their paper.\n   - According to the definitions provided, this comment fits the description of \"Unactionable\" (rated as 1), as the authors do not know what they should do after reading the comment.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point:\n681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 \"latent in verbs\": why don't you mention objects here?\n\n"}]}}
{"custom_id": "38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point:\n1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].\n\n"}]}}
{"custom_id": "39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point:\n2. \u00c2 Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \\sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \\lambda changes the performance, but it would be nice to see how \\eta and \\tau in equation (7) affect performance. Minor comments:\n\n"}]}}
{"custom_id": "40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point:\n- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.\n\n"}]}}
{"custom_id": "41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point: - Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\nrationale: To evaluate the review comment according to the given definitions, we need to assess both the explicitness and concreteness of the action suggested by the reviewer.\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is concrete. The reviewer identifies a specific issue: a discrepancy between lines 226-238 and lines 242-244 regarding the data source and its syntactic information. The reviewer provides a clear suggestion on how to address this issue: revise the description to mention Li et al. (2019a) earlier. This gives the authors a precise direction on what needs to be changed and how to implement the change.\n\n2. **Explicit vs. Implicit**:\n   - The action is explicit. The reviewer directly states that the description should be revised and provides a specific example of how to do so (mentioning Li et al. (2019a) earlier). There is no need for the authors to infer what needs to be done; the action is clearly articulated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have a clear understanding of what changes are needed and how to implement them.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states, \"They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.\" This is a description of what the authors have done, but it does not provide any guidance on what needs to be changed or improved. There is no action suggested, so it is vague in terms of providing actionable steps.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state any action that the authors should take. It merely describes the current state of the draft. There is no implicit suggestion either, as the comment does not hint at any issues or areas for improvement that the authors could infer. \n\n3. **Conclusion**:\n   - Given that the comment neither provides explicit nor implicit actions and is vague in terms of guiding the authors on what to do, it fits the definition of \"Unactionable.\" According to the scale, a rating of 1 is appropriate because the comment lacks any meaningful information to help the authors improve their paper.\nscore: 1\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point:\n2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps? [1] https://github.com/watchernyu/REDQ\n\n"}]}}
{"custom_id": "42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point:\n2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n\n"}]}}
{"custom_id": "43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: 3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests including examples of the system on actual texts. This is a clear action because it specifies what the authors should add to their paper (examples of the system on actual texts). The authors know what needs to be done, which makes the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state, \"Add examples of the system on actual texts.\" Instead, it suggests that it \"will be nice to see\" these examples. The action is implied rather than explicitly stated. The authors can infer that they should add these examples, but the comment does not directly demand this change.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what to add) but implicit (the suggestion is not directly stated as an action), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action. In this case, the action is clear (add examples), but the suggestion is implied.\nscore: 4\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point:\n- The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text.\n\n"}]}}
{"custom_id": "44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) It seems that there is still room to improve the complexity of Algorithm 2;\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that there is \"room to improve the complexity of Algorithm 2.\" However, it does not provide specific details on what aspects of the algorithm are inefficient or how to improve them. There are no concrete suggestions, such as identifying specific bottlenecks, proposing alternative methods, or referencing relevant literature. Therefore, the action is vague because, after identifying the action (improving complexity), the authors still do not know how to carry it out.\n\n2. **Explicit vs. Implicit**: \n   - The action to \"improve the complexity\" is implied rather than explicitly stated. The comment suggests that improvement is needed, but it does not directly state what actions should be taken to achieve this improvement. The authors have to infer that they need to analyze and optimize the algorithm's complexity.\n\n3. **Conclusion**:\n   - Given that the action is vague and implicitly stated, the comment falls into the category of \"Borderline Actionable.\" According to the definitions, a comment rated as 2 includes an implicitly stated action that is vague and lacks detail on how to apply it. This aligns with the feedback's conclusion that the comment is barely actionable.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break down the process step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" While it provides a general direction (more in-depth analysis), it does not specify which claims need this analysis or how to conduct it. This lack of specificity makes the action vague. The authors are left without clear guidance on which parts of their paper need attention or how to enhance the analysis.\n\n2. **Explicit vs. Implicit**:\n   - The action suggested by the comment is explicit in the sense that it directly states that more in-depth analysis is needed. However, it does not specify which claims require this analysis, making it less straightforward for the authors to act upon. The authors know they need to perform more analysis, but they must infer which parts of the paper this applies to.\n\n3. **Conclusion**:\n   - Given the definitions, the comment explicitly states an action (perform more in-depth analysis) but is vague about how to execute it (which claims to analyze and how to do so). This aligns with the definition of \"Somewhat Actionable,\" where the action is explicit but lacks detailed guidance for execution.\nscore: 3\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point:\n2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison? Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison: Guo Lu, et al., \"Content Adaptive and Error Propagation Aware Deep Video Compression.\" ECCV 2020.\n\n"}]}}
{"custom_id": "45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the dataset might have noise and provides examples such as misinformation and out-of-context images. However, it does not specify what kind of analysis should be conducted to address these issues. The lack of detail on how to perform the analysis makes the action vague. The authors are left uncertain about the specific steps they need to take to improve the dataset.\n\n2. **Explicit vs. Implicit**:\n   - The comment explicitly states a concern about the dataset's noise and explicitly requests more analysis. However, it does not explicitly state what kind of analysis is needed. The action (conducting more analysis) is explicit, but the details on how to execute it are missing.\n\n3. **Conclusion**:\n   - The comment explicitly requests an action (more analysis) but is vague about how to carry out this action. According to the definitions, this aligns with a \"Somewhat Actionable\" comment, which is rated as a 3. The authors know they need to perform additional analysis but lack guidance on the specifics.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests adding an example of inconsistency in the abstract. It specifies the type of example needed: \"such as models get ranked differently.\" This provides a clear and concrete action for the authors to take. The authors know exactly what kind of example to include and where to include it in the abstract. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment directly instructs the authors to include an example of inconsistency in the abstract. It does not require the authors to infer what needs to be done; the action is stated explicitly. The suggestion is clear and leaves no room for interpretation about what is being asked.\n\n3. **Conclusion**:\n   - Given that the comment is both explicit and concrete, it meets the criteria for being highly actionable. The authors know exactly what to do and how to do it, with no ambiguity or need for further inference.\n\nBased on this analysis, the feedback correctly identifies the comment as \"Highly Actionable\" with a rating of 5. The feedback accurately reflects the explicit and concrete nature of the comment, confirming that it provides clear, specific, and directly actionable guidance.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment asks for clarification on the use of the Challenge Set, specifically whether it is used for evaluation or also for augmenting the training material. It further asks for details on the data split if applicable. This indicates a concrete action because the authors know exactly what information is being requested: an explanation of the Challenge Set's use and details on the data split. Therefore, the action is concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to add this information to the paper. Instead, it poses questions that imply the need for additional details. The authors can infer that they need to provide clarification and possibly revise the paper to include this information. Thus, the action is implicit.\n\n3. **Conclusion**:\n   - Given that the action is concrete (the authors know what information is needed) but implicit (the comment does not directly state that this information should be added), the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment implicitly states an action but concretely states how to implement the inferred action.\nscore: 4\n\n\nReview Point:\n3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.\n\n"}]}}
{"custom_id": "46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper should \"dig into the theory proofs and show the convergence properties of the proposed algorithm.\" However, it does not specify which aspects of the theory proofs need more exploration or how to demonstrate the convergence properties. This lack of detail makes the action vague. The authors are left without clear guidance on how to address the comment, as they do not know which specific areas to focus on or the method to demonstrate convergence properties.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action. Instead, it implies that the authors need to enhance the theoretical discussion and demonstrate convergence properties. The authors must infer the specific actions required to address the comment, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking specific guidance on how to address the issue) and implicit (requiring inference to determine the action), it falls into the category of \"Borderline Actionable.\" According to the definitions, a \"Borderline Actionable\" comment includes an implicitly stated action that is vague and lacks detail on how to apply it.\nscore: 2\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point:\n2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating.\n\n"}]}}
{"custom_id": "47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "response_format": {"type": "json_object"}, "temperature": 0.0, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nactionability:\nMeasures the level of actionability in the review point. We evaluate actionability according to two points: \n1. Is the action stated directly, or does the author need to infer it? (Explicit vs. Implicit). \n2.  After identifying the action, do you know how to apply it, or the action is vague? (Concrete vs. Vague)\n- It\u2019s more important for actions to be concrete so the authors know how to apply them. It\u2019s also preferred that actions be stated directly rather than inferred.\nDefinitions:\nExplicit:  Direct or apparent actions or suggestions. Authors can directly identify modifications that they should apply to their draft. Clarification questions should be treated as explicit statements if they give a direct action.\nImplicit:  Actions that can be deduced. This can be in the form of questions that need to be addressed or missing parts that need to be added. Actions are not stated directly, but the authors can infer what needs to be done after reading the comment.\nConcrete: After Identifying the action, the authors know exactly what needs to be done and how to apply the action.\nVague: After Identifying the action, the authors still don\u2019t know how to carry out this action.\n Actionability is rated on a scale from 1-5, and we will now provide a definition for each.\n1: Unactionable\nDefinition: The comment lacks any meaningful information to help the authors to improve the paper. After reading the comment, the authors do not know what they should do.\nExamples: \nThe best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\nThe idea of using positional encodings (PEs) for GNNs on molecular graph regression is not new.\nThe differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.\n2: Borderline Actionable\nDefinition: The comment includes an implicitly stated action, or the action can be inferred. Further, the action itself is vague and lacks detail on how to apply it.\nExamples:\nIt is not clear if this trend holds across different model architectures.\nI wonder what would happen if you used another dataset.\nI am curious about the outcome of changing some of the hyperparameter values.\n3: Somewhat Actionable\nDefinition: The comment explicitly states an action but is vague on how to execute it.\nExamples:\nYou should address the lack of technical novelty in this paper.\nI think some parameter values should be looked at.\nComparing your method to other previous related work is highly suggested.\n4: Mostly Actionable\nDefinition: The comment implicitly states an action but concretely states how to implement the inferred action.\nExamples:\nThere are some very relevant baselines like X and Y that other people have been comparing their results to.\nSome items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\nIt is hard to understand under what conditions one should use SynTextBench over other metrics (e.g., MMLU / Big Bench for language generation).\n5: Highly Actionable\nDefinition: The comment contains explicit action and concrete details on how to implement it. The authors know exactly how to apply it.\nExamples:\nGiven that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.\nWhat will happen if you use the evaluation metric X instead of Y?\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Finally, and similarly to above, i\u2019d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range \u201cinference\u201d capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I\u2019d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The action suggested by the reviewer is to conduct an experiment where the image is occluded by randomly blacking out half of it. This is a specific and detailed instruction, leaving no ambiguity about what needs to be done. The authors know exactly how to implement this change, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The reviewer explicitly states the action that needs to be taken: conducting an experiment with a specific modification (occluding half of the image). There is no need for the authors to infer what is required, as the action is directly stated.\n\n3. **Conclusion**:\n   - Given that the action is both explicit and concrete, the comment is highly actionable. The authors have clear guidance on what to do and how to do it, fulfilling the criteria for a rating of 5 on the actionability scale.\nscore: 5\n\n\nReview Point: Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment raises a question about the necessity of treating concept map extraction as a separate task and mentions the challenge of distinguishing concept maps as the number of nodes increases. However, it does not provide specific guidance or steps on how to address these issues. The comment lacks detailed instructions or suggestions on what changes should be made, making it vague. The authors are left without a clear understanding of how to implement any potential changes.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly state an action that the authors should take. Instead, it poses a question and provides observations about the current state of concept map extraction and summarization systems. The authors would need to infer what actions, if any, are necessary based on the comment. This makes the action implicit, as the authors must deduce the potential need for changes or improvements.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete steps) and implicit (requiring inference to determine any action), it falls into the category of being barely actionable. According to the definitions provided, a comment that includes an implicitly stated action and is vague on how to apply it would be rated as \"2: Borderline Actionable.\"\nscore: 2\n\n\nReview Point: - the required implicit call to the Witness oracle is confusing.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment states that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any details on why it is confusing or how the authors might address this confusion. There is no guidance on what specific changes should be made to improve clarity. Therefore, the action is vague because, even if the authors identify the issue, they do not know how to carry out any corrective action.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not explicitly state an action. It mentions a problem (confusion regarding the implicit call), but it does not directly suggest any modifications or actions that the authors should take. The authors would need to infer what needs to be done to address the confusion, making it implicit.\n\n3. **Conclusion**:\n   - Given that the comment is both vague (lacking concrete details on how to address the issue) and implicit (requiring the authors to infer the necessary action), it fits the definition of \"Unactionable\" (1). The authors do not have enough information to know what they should do to improve the paper.\nscore: 1\n\n\nReview Point: 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. \nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment identifies specific comparisons that are missing: \"Campos et al. (2020)\" and \"other domain adaptation methods cited in Section 8.\" This provides a clear indication of what is lacking in the paper. The authors know exactly which comparisons need to be made, making the action concrete.\n\n2. **Explicit vs. Implicit**:\n   - The comment does not directly instruct the authors to make these comparisons. Instead, it points out the absence of these comparisons, implying that they should be included. Therefore, the action is implicit because the authors need to infer that they should add these comparisons.\n\n3. **Conclusion**:\n   - Given that the action is implicit but concrete, the comment fits the definition of \"Mostly Actionable.\" According to the definitions provided, a \"Mostly Actionable\" comment (rated as 4) implicitly states an action but concretely states how to implement the inferred action. In this case, the action is inferred (implicit) but clear (concrete) in terms of what needs to be done.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: To evaluate the review comment according to the given definitions, let's break it down step by step:\n\n1. **Concrete vs. Vague**: \n   - The comment suggests that the paper would be stronger with the inclusion of more baselines based on related work. However, it does not specify which baselines should be included. This makes the action vague because, while the authors understand that they need to add more baselines, they do not have clear guidance on which specific baselines to consider. The lack of detail on how to execute the action (i.e., which baselines to include) makes it vague.\n\n2. **Explicit vs. Implicit**:\n   - The action of including more baselines is explicitly stated. The reviewer directly mentions that the paper would benefit from the inclusion of more baselines. There is no need for the authors to infer this action; it is clearly stated.\n\n3. **Conclusion**:\n   - Given that the action is explicitly stated but lacks concrete details on how to execute it, the comment fits the definition of \"Somewhat Actionable.\" According to the definitions provided, a \"Somewhat Actionable\" comment explicitly states an action but is vague on how to execute it. This aligns with the label of 3.\nscore: 3\n\n\nReview Point:\n- Limited Experiments - Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.\n\n"}]}}

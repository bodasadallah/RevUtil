{"custom_id": "1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n1.) What is the domain of the inputs? It seems they are lying in the same sphere, not mentioned in the paper.\n"}]}}
{"custom_id": "2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate.\n"}]}}
{"custom_id": "3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point:\n3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the \u00e2\u0080\u009celements\u00e2\u0080\u009d equivalent to \u00e2\u0080\u009cstates\u00e2\u0080\u009d? or \u00e2\u0080\u009cactions\u00e2\u0080\u009d? More should be elaborated.\n"}]}}
{"custom_id": "4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. If possible, results on ImageNet can be more convincing of the proposed method\n"}]}}
{"custom_id": "5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n3. It is suggested that the authors provide a brief introduction to energy models in the related work section. In Figure 1, it is not mentioned which points different learning rates in the left graph and different steps in the right graph correspond to. [1] Context-aware robust fine-tuning. [2] Fine-tuning can cripple your foundation model; preserving features may be the solution. [3] Robust fine-tuning of zero-shot models.\n"}]}}
{"custom_id": "6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point:\n- The caption for Figure 7 is incorrect, and should be corrected to \"Edge Dynamics\" from \"Node Dynamics\".\n"}]}}
{"custom_id": "7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n4. The paper would benefit from a more detailed comparison with related work, in particular making a detailed comparison to the time complexity and competitiveness of prior art. Minor:\n"}]}}
{"custom_id": "8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The introduction claims that \"these shape constraints do not require tuning a free parameter\". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned.\n"}]}}
{"custom_id": "9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2. It appears that the output from the algorithm depends on the order in which the data are processed. This should be clarified.\n"}]}}
{"custom_id": "10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2. Model-Specific Insights: The paper focuses on generic findings across models, but a deeper investigation into how specific models (e.g., GPT-4o vs. InternVL2) behave differently when ReGuide is applied could add nuance to the conclusions. For example, the differences in false positive rates (FPR) between models with and without ReGuide should be presented for a better comparison.\n"}]}}
{"custom_id": "11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The experiments are only done on one game environment. More experiments are necessary.\n"}]}}
{"custom_id": "12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. Figure 3 presents EEG topography plots for both the input and output during the EEG token quantization process, leading to some ambiguity in interpretation. I would recommend the authors to elucidate this procedure in greater detail. Specifically, it would be insightful to understand whether the spatial arrangement of the EEG sensors played any role in this process.\n"}]}}
{"custom_id": "13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n5. It's unclear why there is a base layer GNN encoding in the proposed method. An ablation study on the necessity of the base layer GNN encoding would be helpful.\n"}]}}
{"custom_id": "14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n* The paper lacks an ablation study explaining why they chose the prompt in this specific way, e.g., few-shot examples for CoT might improve performance.\n"}]}}
{"custom_id": "15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark:\n"}]}}
{"custom_id": "16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- PPG outputs a complete pose relative to every part\u2019s center. Thus O_{up} should contain the offset for every keypoint with respect to the center of the upper part. In Eq.2 of the supplementary material, it seems that O_{up} is trained to output the offset for the keypoints that are not farther than a distance \\textit{r) to the center of corresponding part. How are the groundtruths actually built? If it is the latter, how can the network parts responsible for each part predict all the keypoints of the pose.\n"}]}}
{"custom_id": "17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. Limited datasets and models. The bias benchmarks only assess gender, race, and religion. Other important biases and datasets are not measured. Also missing are assessments on state-of-the-art generative models like GPT.\n"}]}}
{"custom_id": "18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n2. It would be nice to see some results in other modalities (e.g., maybe some language related tasks. Aside: for language related tasks, people care about OOD performance as well, so maybe expected test loss is not as meaningful?)\n"}]}}
{"custom_id": "19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point:\n2.The motivation is unclear. Why the adversarial network is needed in this model? the comparison of experimental results is unfair. The proposed model is equipped with newly-added CAT and GAN, which is a bigger model than others. Even the pre-trained model is compared with other models.\n"}]}}
{"custom_id": "20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. The writing is difficult to follow in many places and can be simplified.\n"}]}}
{"custom_id": "21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n* The authors combine two existing techniques to get the framework without innovation. The adversarial attack or correction method and the domain adaptation method used by the authors are proposed by prior work. And the adopted domain adaptation method here is a very old and simple method which is proposed eight years ago. Considering there were so many effective domain adaptation methods proposed in the recent few years, why don't you use other domain adaptation methods to further improve the performance?\n"}]}}
{"custom_id": "22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n"}]}}
{"custom_id": "23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point:\n- I might be helpful to quantify and clarify the claim \"ReLU does not work very well in very deep or in convolutional networks.\" ReLUs were used in the AlexNet paper which, at the time, was considered deep and makes use of convolution (with pooling rather than ReLUs for the convolutional layers).\n"}]}}
{"custom_id": "24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. The Sec. 3.1 for 3D Gaussians generation seems to just follow the previous work, Luciddreamer. Please correct me there is any additional novel effort for this part.\n"}]}}
{"custom_id": "25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. For the Appendix H section, it should be reorganized which is difficult to follow.\n"}]}}
{"custom_id": "26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point:\n- When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex.\n"}]}}
{"custom_id": "27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n- In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor:\n"}]}}
{"custom_id": "28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. **Limited Applicability**: While the paper claims that SGC offers a more flexible, fine-grained tradeoff, PEFT methods typically target compute-constrained scenarios, where such granular control may require extra tuning that reduces practicality. It would be beneficial to include a plot with sparsity on the x-axis and performance on the y-axis to directly compare the flexibility of SGC with LoRA. This visualization could more intuitively demonstrate whether SGC\u2019s fine-grained control offers practical performance benefits at different sparsity levels.\n"}]}}
{"custom_id": "29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2 The authors should discuss the iteration cost (computational budge) of the proposed method. It will be great if the authors discuss the iteration cost of all related methods including baseline methods.\n"}]}}
{"custom_id": "30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The main contribution of combining attention with other linear mechanisms is not novel, and, as noted in the paper, a lot of alternatives exist.\n"}]}}
{"custom_id": "31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2) The effectiveness of the proposed approach for other language families remains unknown.\n"}]}}
{"custom_id": "32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- You write: \"Evidently, replacing any of the procedure steps of XAIFOOLER with a random mechanism dropped its performance\" I'm unsure that \"better than random\" is a strong demonstration of capability.\n"}]}}
{"custom_id": "33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point:\n1) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).\n"}]}}
{"custom_id": "34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point:\n245 The GAT is trained with the whole model? Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity.\n"}]}}
{"custom_id": "35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point:\n- The choice to evaluate on only a subset of the Massive Text Embedding Benchmark (MTEB) raises questions about generalizability; it would be helpful to understand the criteria behind this selection and whether other tasks or datasets might yield different insights.\n"}]}}
{"custom_id": "36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. It is not clear to me whether such a model could generate novel knowledge or testable hypothesis about neuron data.\n"}]}}
{"custom_id": "37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point:\n- Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials.\n"}]}}
{"custom_id": "38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point:\n- Nitpicking: I find \"above/below diagonal\" (add a thin gray diagonal to the plot) easier to interpret than \"above/below 45 degree\", which sounds like a local property (e.g., not the case where the red line saturates and has \"0 degrees\").\n"}]}}
{"custom_id": "39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point:\n1) One of the key components is the matching metric, namely, the Pearson correlation coefficient (PCC). However, the assumption that PCC is a more relaxed constraint compared with KL divergence because of its invariance to scale and shift is not convincing enough. The constraint strength of a loss function is defined via its gradient distribution. For example, KL divergence and MSE loss have the same optimal solution while MSE loss is stricter than KL because of stricter punishment according to its gradient distribution. From this perspective, it is necessary to provide the gradient comparison between KL and PCC.\n"}]}}
{"custom_id": "40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point:\n- Unclear whether bringing connections to human cognition makes sense As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against \"previously appreciated\".\n"}]}}
{"custom_id": "41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2. The improvements on different datasets are trivial and the novelty of this paper is limited. Lots of previous works focus on this topic. Just adding topic entities seems incremental.\n"}]}}
{"custom_id": "42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n5. Some of the ablations mentioned in previous sections are hard to locate in the following contents, the writing can be improved in this part.\n"}]}}
{"custom_id": "43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n- I found the application to differential privacy unconvincing (see comments below) - Experimental validation was a bit light and felt preliminary RECOMMENDATION: I think this paper should be accepted into the NIPS program on the basis of the online algorithm and analysis. However, I think the application to differential privacy, without experimental validation, should be omitted from the main paper in favor of the preliminary experimental evidence of the tensor method. The results on privacy appear too preliminary to appear in a \"conference of record\" like NIPS. TECHNICAL COMMENTS:\n"}]}}
{"custom_id": "44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. It is stated both that the multi-env model has an inevitable performance loss and that the multi-env model outperforms the single-env model due to knowledge sharing. These two statements seem to be conflicting. Please clarify.\n"}]}}
{"custom_id": "45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point:\n1. The motivation of this work should be further justified. In few-shot learning, we usually consider how to leverage a few instances to learn a generalizable model. This paper defines and creates a few-shot situation for graph link prediction, but the proposed method does not consider how to effectively use \u201cfew-shot\u201d and how to guarantee the trained model can be generalized well to new tasks with 0/few training steps.\n"}]}}
{"custom_id": "46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n7. Experiments - Ablation - missing components: There should be experiments and explanation regarding the different queries used in spatio-temporal representation, i.e., spatial, temporal and summary. That is the key difference to VideoChatGPT and other works. What if only have spatial one, or temporal and summary one?\n"}]}}
{"custom_id": "47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n- The experiments are not sufficient. More empirical experiments or toy experiments (for the simplified self-attention model considered in the theoretical analysis) need to be done to show the validity of the model relaxations and the consistence of the theoretical analysis with empirical results, besides citing the result in Kaplan et al. 2020.\n"}]}}
{"custom_id": "48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Figure 3 is challenging to understand. The workflow and captions are unclear, and the representation of communication modes on the left side is confusing.\n"}]}}
{"custom_id": "49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n2) It seems that performance and sample efficiency are sensitive to \u03bb parameters. (Page 9, lines 310-313) I don't understand how the process of calculating the \u03bb is done. How is \u03bb computed from step here? (Page 8 lines 281-285) The authors explain why ELLA does not increase sample efficiency in a COMBO environment, but I don't quite understand what it means. [1] Yuri Burda et al, Exploration by Random Network Distillation, ICLR 2019 [2] Deepak Pathak et al, Curiosity-driven Exploration by Self-supervised Prediction, ICML 2017 [3] Roberta Raileanu et al, RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments, ICLR 2020\n"}]}}
{"custom_id": "50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2) The regularization term seems a bit ad-hoc. Although the author has provided some intuitive explanation of the regularization, it seems lack of theoretical support. There are some other statistics which may be used to replace role of the mean and standard derivation in the regularization. Why they are not adapted in the regularization? For example, the median which is not sensitive to outlier value of data can be used to replace mean value.\n"}]}}
{"custom_id": "51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point:\n3. The Atari game result (Section 7.2) is limited to a single game and a single baseline. It is very hard to interpret this. Less major weaknesses:\n"}]}}
{"custom_id": "52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point:\n1. Replacing `n^2/(2*s^2)` with an arbitrary parameter `lambda` (lines 119-121) 2. Taking SGD learning rate ~ 0.1 (line 164) \u2014 unlike the Adam default value, it is unclear what the justification behind this value is.\n"}]}}
{"custom_id": "53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n- Can we run VGAE with a vamp prior to more accurately match the doubly stochastic construction in this work? That would help inform if the benefits are coming from a better generative model or better inference due to doubly-semi implicit variational inference. Minor Points - Figure 3: It might be nice to keep the generative model fixed and then optimize only the inference part of the model, parameterizing it as either SIG-VAE or VGAE to compare the representations. Its impossible to know / compare representations when the underlying generative models are also potentially different.\n"}]}}
{"custom_id": "54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- It would have been interesting to see not only the retrieved and final attentions but also the tentative attention maps in the qualitative figures.\n"}]}}
{"custom_id": "55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n- In section 4.3 and 4.4, words such as \u201csomewhat\u201d and \u201cgood generative ability\u201d appears in the description yet I am concerned that even with beam search, only 77% of the result lists contain the ground truth logical forms. If the relationships and entities were replaced, how do we ensure that the plugged-in entities/relationships were the right one? In what percentage were the right entities/relationships being plugged in if no ground truth is available?\n"}]}}
{"custom_id": "56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n1. Transformer has been adopted for lots of NLP and vision tasks, and it is no longer novel in this field. Although the authors made a modification on the transformer, i.e. cross-layer, it does not bring much insight in aspect of machine learning. Besides, in ablation study (table4 and 5), the self-cross attention brings limited improvement (<1%). I don\u2019t think this should be considered as significant improvement. It seems that the main improvements over other methods come from using a na\u00efve transformer instead of adding the proposed modification.\n"}]}}
{"custom_id": "57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point:\n* (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods.\n"}]}}
{"custom_id": "58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n2. Secondly, what relevance does the framework have with problems of non-convex losses and/or non-norm type defenses? Would the non-vanishing duality gap and the difficulty of maximization over non-norm type constraints make the algorithm irrelevant? Or would it still give some intuitions on the risk upperbound? p.3, binary classification: If the true mean is known through an oracle, can one use the covariance or any other statistics to design a better defense?\n"}]}}
{"custom_id": "59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n1. Add a few more sentences explaining the experimental setting for continual learning 2. In Fig 3, explain the correspondence between the learning curves and M-PHATE. Why do you want to want me to look at the learning curves? Does worse performing model always result in structural collapse? What is the accuracy number? For the last task? or average?\n"}]}}
{"custom_id": "60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point:\n1) Building text descriptions for each task still requires human labor. We do not know what textual format is optimal for policy learning. It varies from task to task, model to model. On the other hand, as I stated in Question 1, the long-text input could restrict the scalability of this framework.\n"}]}}
{"custom_id": "61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n- I wonder how crucial the annealing scheme from the last paragraph in Section 4 is. Especially when $\\alpha$ is not decreased to $0$ I imagine this could induce a bias which may be so large that it outweighs the bias reductions attained by using IWAE in the first place.\n"}]}}
{"custom_id": "62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. I understand this paper targets a problem which somewhat differs from general segmentation problems. And I do very much appreciate its potential benefit to the neuroscience community. This is indeed a plus for the paper. However, an important question is how much this paper can really improve over the existing solutions. Therefore, to demonstrate that the algorithm is able to correctly find closed contours, and really show stronger robustness against weak boundaries (This is especially important for bottom up methods), the authors do need to refer to more recent trends in the vision community.\n"}]}}
{"custom_id": "63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n3. Also, the theoretical discussions can use some improvements. Although directly related to fair representation, the current theorems follow directly from the algorithm design itself and the well-known property of mutual information to $\\Delta_{DP}$. For instance, I was expecting some sample complexity-type results for not returning NSF, e.g., given confidence levels, what is the sufficient amount of training data points that would not return NSF.\n"}]}}
{"custom_id": "64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point:\n\u2022 It seems that ODA, one of the methods of solving the MOIP problem, has learned the policy to imitate the problem-solving method, but it did not clearly suggest how the presented method improved the performance and computation speed of the solution rather than just using ODA.\n"}]}}
{"custom_id": "65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. No adversarial loss to guarantee the perturbed data being similar to the authentic data.\n"}]}}
{"custom_id": "66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Clarity: Related work could be improved. Some related works are mainly named but their differences are not described enough.\n"}]}}
{"custom_id": "67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point:\n- Figure 1 would be much stronger if there were error bars and/or if there were more random trials that would (potentially) get rid of some of the (most likely) random fluctuations in the results.\n"}]}}
{"custom_id": "68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Results should be averaged over multiple runs to determine statistical significance.\n"}]}}
{"custom_id": "69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The paper opens that learning long-range dependencies is important for powerful predictors. In the example of semantic segmentation I can see that this is actually happening, e.g., in the visualisations in table 3; but I am not sure if it is fully required. Probably the truth lies somewhere in between and I miss a discussion about this. If no form of locality with respect to the 2d image space is encoded in the graph structure, I suspect that prediction suddenly depends on the image size.\n"}]}}
{"custom_id": "70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n4. The experimental results are unreliable, especially in Table 1, where the MSE is significantly smaller than the MAE, which raises concerns about their validity.\n"}]}}
{"custom_id": "71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- I doubt the proposed method can be trained without using any camera information (Line 223, the so called \"knowledge of CAD model correspondences\"). Without knowing the viewpoint, how is it possible to perform ray marching? How do you know where the ray comes from?\n"}]}}
{"custom_id": "72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n- The details of the forward-prediction model is not well explained. In particular, Figure 2(b) does not really show the schematic representation of the forward prediction model; the figure should be redrawn. It was hard to connect the pieces of the text with the figure as well as the equations.\n"}]}}
{"custom_id": "73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. The paper lacks discussion on the theoretical guarantee about the approximation ratio of the hierarchical strategy to the global optimal of original QUBO.\n"}]}}
{"custom_id": "74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n1 The traditional DCI framework may already be considered explicitness(E) and size(S). For instance, to evaluate the disentanglement (D) of different representation methods, you may need to use a fixed capacity of probing (f), and the latent size should also be fixed. DCI and ES may be entangled with each other. For instance, if you change the capacity of probing or the latent size, then the DCI evaluation also changes correspondingly. The reviewer still needs clarification on the motivation for considering explicitness(E) and size(S) as extra evaluation.\n"}]}}
{"custom_id": "75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n* The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM?\n"}]}}
{"custom_id": "76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding) Comment:\n"}]}}
{"custom_id": "77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n3. The authors describe an online version of the algorithm because it is impractical to train multiple iterations/epochs with large models and datasets. Is it true that the proposed method requires much more computation than other methods? Please compare the computational complexity with other methods.\n"}]}}
{"custom_id": "78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.\n"}]}}
{"custom_id": "79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. The first claimed contribution of the paper is that unlike other existing algorithms, the proposed algorithm does not take as many points or does not need apriori knowledge about dimensions of subspaces. It would have been better if there were some empirical justification about this.\n"}]}}
{"custom_id": "80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point:\n- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2\u0080\u009cattribute\u00e2\u0080\u009d. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2\u0080\u009cLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2\u0080\u009d. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n"}]}}
{"custom_id": "81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n4) It seems that the weakness that this method is addressing would be more prominent in images comprised of multiple objects, or cluttered scenes. It would be very interesting to compare this approach to previous ones on few-shot classification on such a dataset!\n"}]}}
{"custom_id": "82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The authors take time to discuss how KG handles the continuous task setting, but there are no experiments with continuous tasks - It\u2019s great that entropy methods for conditional optimization are derived in Section 7 in the appendix, but why are these not included in the experiments? How does the empirical performance of these methods compare to ConBO?\n"}]}}
{"custom_id": "83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point:\n- In the paper it is mentioned that the obtained core tensors can be rounded to smaller ranks with a given accuracy by clustering the values of the domain sets or imposing some error decision epsilon if the values are not discrete. It is not clear what is, in theory, the effect on the approximation in the full tensor error. Is there any error bound in terms of epsilon?\n"}]}}
{"custom_id": "84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n2) analyze the domain gap. It would be nice to add some discussions about the gap between datasets. Some datasets are closer to each other thus the adaption may not be a big issue. Also, if the method is able to finetune a pre-trained model on synthetic data, then the value of the approach would be much higher.\n"}]}}
{"custom_id": "85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n* L248: What does \"wrong\" mean here? The paper gets into some of the nuance of this position at L255, but it would be helpful to clarify what is meant by a good/bad/wrong explanation before using those concepts.\n"}]}}
{"custom_id": "86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point:\n\u2022 Supervised pretraining based on the prediction of homo-lumo gap may lead to negative transfer. For example, on QM9 in downstream experiments, Transformer-M performs poorly on most tasks other than homo, lumo, and gap. This may be contradictory to the description \"general-purpose neural network model\" claimed in this paper.\n"}]}}
{"custom_id": "87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The biggest weakness of the paper is that it does not compare to simple feature acquisition baselines like expected utility or some such measure to prove the effectiveness of the proposed approach. Writing style and other issues:\n"}]}}
{"custom_id": "88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n5 shows evidence that some information is learned before the model is able to use the concepts.\" --> I think \"evidence\" may be too strong here, and would say something more like \"Fig.\n"}]}}
{"custom_id": "89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- I would say that section 3.2 can be eliminated - I think that at this point readers can be presumed to know about the Gumbel-Softmax/Concrete distribution.\n"}]}}
{"custom_id": "90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n2. The authors' derivation falls into classical learning theory-based bounds, which, to the best of my knowledge, does not yield realistic bounds, unless Bayesian considerations are taken into account (e.g. Bayesian-PAC based bounds).\n"}]}}
{"custom_id": "91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. Writtings could be improved in some places. For two examples, * In definition 2.1, what are the \"relevant\" auxiliary model weights? The current definition is a bit difficult for me to interpret.\n"}]}}
{"custom_id": "92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].\n"}]}}
{"custom_id": "93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n3. The paper is locally well-written and the technical presentation flows easily: I can understand the statement of each theorem without having to wade through too much notation, and the authors do a good job of conveying the gist of the proofs. Second, the weaknesses:\n"}]}}
{"custom_id": "94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point:\n- The proposed algorithm is well constructed and is a step-forward from the main reference [1]. While not having a major practical impact (has an high regret in stationary regime and in highly non-stationary regime, even if the best action does not change) it resolves and open problem and gives some closure the the analysis using the variation.\n"}]}}
{"custom_id": "95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- In the reported ablation studies in Table 2, for CUB and SOP datasets, the complete loss function performed even worse than those with some terms missing. That does not appear to make sense. Why?\n"}]}}
{"custom_id": "96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper.\n"}]}}
{"custom_id": "97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2) How does the PL condition you use compare with the PL conditions proposed in \u00e2\u0080\u009cGlobal Convergence of Arbitrary-Block Gradient Methods for Generalized Polyak-\u00c5\u0081ojasiewicz Functions\u00e2\u0080\u009d, arXiv:1709.03014 ?\n"}]}}
{"custom_id": "98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.\n"}]}}
{"custom_id": "99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- While the types of interventions included in the paper are reasonable computationally, it would be important to think about whether they are practical and safe for querying in the real world.\n"}]}}
{"custom_id": "100", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The presentation of the simulation study is not really doing a favor to the authors. Specifically, the authors do not really comment on why the GPC (benchmark) is performing better than BPC (their method). It would be worth re-iterating that this is b/c of the bandit feedback and not using information about the form of the cost function.\n"}]}}
{"custom_id": "101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2.The proposed method comprises several complicated modules and has more parameters than the baselines. It remains unclear whether the main performance gain originates from a particular module or if the improvement is merely due to having more parameters. The current version of the ablation study does not provide definitive answers to these questions.\n"}]}}
{"custom_id": "102", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. It would be better to compare with other self-supervised learning methods that are not based on contrastive learning.\n"}]}}
{"custom_id": "103", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n- In the introduction the authors mention the fact that tensor decomposition is in general harder in the symmetric than in the non-symmetric case. How is this connected with recent findings about the `nice' landscape of the objective function associated with the decomposition of symmetric (orthogonal) order-4 tensors [1]?\n"}]}}
{"custom_id": "104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point:\n- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).\n"}]}}
{"custom_id": "105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n1. The authors propose a new classification network, but I a bit doubt that its classification error is universally as good as the standard softmax network. It is a bit dangerous to build a new model for better detecting out-of-distribution samples, while losing its classification accuracy. Could the authors report the classification accuracy of the proposed classifier on ImageNet data? Some theoretical justifications, if possible, would be great for the issue.\n"}]}}
{"custom_id": "106", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. It is unclear how these theoretical findings relate to real-world deep learning models, I would suggest the authors verify the conclusion about the label noise and model size on MNIST and CNN as well.\n"}]}}
{"custom_id": "107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point:\n8) l 196-7: this requires more explanation. Why exactly are the two quantities different, and why does this capture the difference in learning settings? ---- I still lean toward acceptance. I think NIPS should have room for a few \"pure theory\" papers.\n"}]}}
{"custom_id": "108", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n104 it is said that every kernel can be described by a feature space parameterized by a neural network, but this is trivially not true. For instance, for RBF kernels, the RKHS is famously infinite-dimensional, such that one would need an NN with infinite width to represent it. So at most, NNs can represent finite-dimensional RKHSs in practice. This limitation should be made more clear. l.\n"}]}}
{"custom_id": "109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n- There are lots of hyperparameters that need to be tuned (step size, N, $\\delta t$) and as the optimization needs to be solved on a sample-by-sample basis it is not clear how much variation in optimal hyperparameters can occur.\n"}]}}
{"custom_id": "110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n2. Several curriculum learning methods have been discussed in Section 1. However, the need for designing a new curriculum learning method for text graphs is not justified. The research gap, e.g., why existing methods can\u2019t be applied, is not discussed.\n"}]}}
{"custom_id": "111", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- My biggest concern with this paper is the fact that it motivates \u00e2\u0080\u009cdiversity\u00e2\u0080\u009d extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity.\n"}]}}
{"custom_id": "112", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed.\n"}]}}
{"custom_id": "113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n- a few more datasets would've been appreciated, especially concerning the cross-task transferability\n"}]}}
{"custom_id": "114", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help?\n"}]}}
{"custom_id": "115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n1. For the captioning experiment, the paper compares to related work only on some not official test set or dev set, however the final results should be compared on the official COOC leader board on the blind test set: https://competitions.codalab.org/competitions/3221#results e.g. [5,17] have won this challenge and have been evaluated on the blind challenge set. Also, several other approaches have been proposed since then and significantly improved (see leaderboard, the paper should at least compare to the once where an corresponding publication is available).\n"}]}}
{"custom_id": "116", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n5. Competitive Baselines: According to the experimental results, the learning-based solvers are much better than the heuristic-based solvers. However, for the single objective TSP, the SOTA heuristic-solver (e.g., Concorde) usually has the best performance. Since the obtained Pareto front is not highly non-convex (as in Figure 2), the results for linear scalarization + Concorde should be included for a better comparison.\n"}]}}
{"custom_id": "117", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n. From the experimental results, without the prior information (the same as the benchmarks) the proposed method has no advantage compared to the SOTA. The advantage only shows when using the prior knowledge. Such comparison is a bit unfair, because in this case the proposed method essentially requires two representation models learned based on each dataset, ie., VAE/GAN + CL. Such extra complexity and cost need to be considered.\n"}]}}
{"custom_id": "118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes:\n"}]}}
{"custom_id": "119", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n5. The student and refinement networks are trained simultaneously. Which may improve the performance of the teacher network. Is the comparison fair? Please provide KID/FID metrics of your teacher network.\n"}]}}
{"custom_id": "120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. The authors have reprt significane testing but I think the choice of test might be incorrect. Since the comparision is to be done between two samples generated from same input why not some paired test setting was used like wilcoxon signed ranked test?\n"}]}}
{"custom_id": "121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. The extra two hyperparameters introduced k and \u03b7 require finetuning, which depends on availability to the environment or a good OPE method.\n"}]}}
{"custom_id": "122", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point:\n2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.\n"}]}}
{"custom_id": "123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. It will be good to see some failure cases and related discussion.\n"}]}}
{"custom_id": "124", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n- **lack of meaningful baselines**: despite mentioning various model criticism techniques in Section 2, the authors limit their comparisons to simple naive baselines. For example, the authors could compare with a chain-of-thought prompting approach.\n"}]}}
{"custom_id": "125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point:\n\u2022 How did you calculate precision/recall/F1-score for 4-class classification of breast density? Also, for breast cancer detection, researchers usually report AUC with sensitivity and specificity at different operating points to compare model performance. It might be more informative to provide AUC results for comparisons.\n"}]}}
{"custom_id": "126", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions).\n"}]}}
{"custom_id": "127", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n- In terms of the experiments, I consider the number of tasks quite limited. To be convinced I would like to see several tasks (at least 10) and sequential results in terms of tasks learned rather than epochs. Questions for authors: Please address my comments on the weaknesses above.\n"}]}}
{"custom_id": "128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point:\n2. The model seems overly simple. This is both a feature and a bug.\n"}]}}
{"custom_id": "129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.\n"}]}}
{"custom_id": "130", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n3. At a high-level, I don\u2019t see a clear connection between \u201cimproved variance control of prediction y^ or the smoothness of loss landscape\u201d and \u201czero-shot learning effectiveness.\u201d Details below. This is in part due to poor clarity.\n"}]}}
{"custom_id": "131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019).\n"}]}}
{"custom_id": "132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n- The poor long-range modelling ability of DGNs is attributed to oversquashing and vanishing/exploding gradients but the poor performance could also be due to oversmoothing, another phenomenon observed in the context of very deep graph networks [Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, In AAAI'18].\n"}]}}
{"custom_id": "133", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point:\n2. In the experimental part, this paper verifies different metrics for different OPE methods. However, in Figure 4 and Figure 5, the different methods in the two sets of benchmarks proposed in this article are quite different in different OPE methods. I hope the author can give some comments on the differences between the two sets of evaluation methods.\n"}]}}
{"custom_id": "134", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n* Label the X and Y axes of plots. This paper makes an important step towards safe RL. While this paper builds upon much previous work, it clearly documents and discusses comparisons to previous work. While the results are principally theoretical, I believe it will inspire both more theoretical work and practical applications.\n"}]}}
{"custom_id": "135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point:\n2. Unlike phi-divergence or Wasserstein uncertainty sets, MMD DRO seems not enjoy a tractable exact equivalent reformulation, which seems to be a severe drawback to me. The upper bound provided in Theorem 3.1 is crude especially because it drops the nonnegative constraint on the distribution, and further approximation is still needed even applied to a simple kernel ridge regression problem. Moreover, it seems restrictive to assume the loss \\ell_f belongs to the RKHS as already pointed out by the authors.\n"}]}}
{"custom_id": "136", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point:\n1) Some observations and subsequent design decisions might be hardware and software dependent;\n"}]}}
{"custom_id": "137", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n6. The first two bullets about contributions (at the end of the intro) can be combined together.\n"}]}}
{"custom_id": "138", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n3) extending the equalized odds results to more datasets (why are Parkinsons and Retired Adult results only reported for demographic parity? it seems like equalized odds should also apply here, and an empirical story built on 2 datasets seems thin). I think\n"}]}}
{"custom_id": "139", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n- Missing supervised baselines. Since most experiments are done on datasets of scale ~100k images, it is reasonable to assume that full annotation is available for a dataset at this scale in practice. Even if it isn\u2019t, it\u2019s an informative baseline to show where these self-supervised methods are at comparing to a fully supervised pre-trained network.\n"}]}}
{"custom_id": "140", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. The method seems to be quite sensitive to hyperparameters, so in order to apply it method for a new problem, one has to perform some careful hyperparameter search to find a proper $\\alpha$.\n"}]}}
{"custom_id": "141", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n1. How does your linear attention handle the autoregressive decoding? As of training, you can feed the network with a batch of inputs with long token dimensions. But when it comes to the generation phase, I am afraid that only limited tokens are used to generate the next token. Then do you still have benefits for inference?\n"}]}}
{"custom_id": "142", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1.The paper consider the node importance among nodes with same label in support set. In 1-shot scenario, how node importance can be used? I also find that the experiment part in the paper does not include the 1-shot paper setting, but related works such as RALE have 1-shot setting, why?\n"}]}}
{"custom_id": "143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2. The authors claim that there is still no research focusing on the joint error for UDA. But this problem of arbitrarily increased joint error has already been studied in previous works like \u201cDomain Adaptation with Asymmetrically-Relaxed Distribution Alignment\u201d, in ICML2019. The authors should discuss on that work and directly illustrate the relationship between that work and the proposed one, and why the proposed method is better.\n"}]}}
{"custom_id": "144", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1.The problem formulation is somewhat unclear in the statement and introduction examples.\n"}]}}
{"custom_id": "145", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point:\n* The motivation for analyzing only the last convolutional layer is not clear. Why would numerosity not appear in earlier layers?\n"}]}}
{"custom_id": "146", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. The proposed FRM is a simple combination of channel attention and spatial attention. The innovative should be given in detail.\n"}]}}
{"custom_id": "147", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n4. This paper is not well organized. The layout of this paper is a bit rushed. For example, The font size of some annotations of Figure1 and Figure 2 is relatively small. And these two figures are not drawn explicitly enough. Table 2 is inserted wrongly inside of a paragraph. Top two lines on page 6 are in the wrong format.\n"}]}}
{"custom_id": "148", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point:\n- The references list contains duplicates and the publication venues and/or the publication years of many of the papers are missing.\n"}]}}
{"custom_id": "149", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n* Unclear Literature Review: The literature review in the paper needs improvement. It is not adequately clear what the main contribution of the proposed method is, and how it distinguishes itself from existing work, particularly in relation to the utilization of GFlowNet for sequence generation. The paper should provide a more explicit and comparative analysis of related work.\n"}]}}
{"custom_id": "150", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Fig. 1 can also be drawn better to show the processing pipeline (prompt generation and manual check, demonstration selection with ground truth scores, and automatic scoring alongwith showing where model training is being used to optimize the selection modules.\n"}]}}
{"custom_id": "151", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Figure 3 is very hard to read anything on the figure.\n"}]}}
{"custom_id": "152", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point:\n- line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n"}]}}
{"custom_id": "153", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. I can't find details on how they make the network fit the residual instead of directly learning the input - output mapping.\n"}]}}
{"custom_id": "154", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it\u2019s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.\n"}]}}
{"custom_id": "155", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n2) Lacking detail in experiment description: Description of experimental details would significantly benefit from increased clarity to allow the user to better judge the results, which is very difficult in the manuscript\u2019s current state; See \"Questions\" for further details.\n"}]}}
{"custom_id": "156", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The reported results seem to be partially derivative: extension to hyper-networks of results already presented in the literature for standard networks.\n"}]}}
{"custom_id": "157", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n1). Technically speaking, the contribution of this work is incremental. The proposed pipeline is not that impressive or novel; rather, it seems to be a pack of tricks to improve defense evaluation.\n"}]}}
{"custom_id": "158", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n1) Scalability. As shown in Table 3 (a), the performance is getting worse with growth of the maximum number of identities. It means that the capacity should be preset to some small number (e.g., 10). In real-world scenario, we can have more than 10 objects and most of the time we don't know how many objects we will need to handle in the future. Have the authors thought about how to scale up without compromising performance?\n"}]}}
{"custom_id": "159", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that whether there is information redundancy and interference in the multi-sphere icosahedral discretization process.\n"}]}}
{"custom_id": "160", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point:\n- The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 \u201cBetter still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets\u201d), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes.\n"}]}}
{"custom_id": "161", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point:\n- Given that many methods aim at one of the three tasks, having 5, 6 and 4 datasets for the tasks respectively, might not be enough for a very rigorous evaluation, in particular if some of the datasets are so large that not all algorithms can be used on them. Addendum: Thank you to the authors for their detailed reply. A repository and online platform for reproducing the experiments was provided, and it was clarified that the datasets are substantially novel. Motivations for the number and choice of datasets were given and I updated my assessment to reflect that.\n"}]}}
{"custom_id": "162", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- There are important information about the empirical study missing that should be mentioned in the supplement, such as recording parameters for the MRI, preprocessing steps, was the resting-state recorded under eyes-open or eyes-closed condition? A brief explanation of the harmonization technique would also be appreciated. It would also be helpful to mention the number of regions in the parcellation in the main text.\n"}]}}
{"custom_id": "163", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152.\n"}]}}
{"custom_id": "164", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1) \u201cexpected\u201d. Therefore, this paper should provide more explanation to make it clear. One of the main contributions of this paper is the CBR, so different optimization strategies and the corresponding results should discussion. For example, what will happen by minimizing both of the inter and intra terms in Eq 3 or only minimizing the first term?\n"}]}}
{"custom_id": "165", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point:\n* The empirical analysis in Figure 3 is confusing to the reviewer. Could the authors provide additional clarification on how the adjustments to the amplitudes of the input series and forecasting target, based on the Frequency Stability score, affect model prediction accuracy? Additionally, could you explain why these adjustments are effective in enhancing the model's performance? Both Equations (9) and (10) have large spacing from the preceding text. [1] Liu, Yong, et al. \"Non-stationary transformers: Exploring the stationarity in time series forecasting.\" Advances in Neural Information Processing Systems 35 (2022): 9881-9893.\n"}]}}
{"custom_id": "166", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n2) how the number of graph neural layers affects the overall performance. Overall, the paper is easy to read. The idea of integrating semantic information in few-shot classification is interesting while it has been widely explored in existing works. Given the reported results of the proposed model and lack of analyses in the proposed model, I am inclined to the score \"Ok but not good enough - rejection\". [1] \u201cCross Attention Network for Few-shot Classification\u201d, Hou et al., NeurIPS \u201819\n"}]}}
{"custom_id": "167", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The related work discusses other methods for training NMT models beyond MLE (e.g., RL methods) but none of them is used as a baseline.\n"}]}}
{"custom_id": "168", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. This author should add more description about the contribution of this paper.\n"}]}}
{"custom_id": "169", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Do you pretrain the cardiac signal representation learning model on the entire dataset or just the training set? If the entire set, how well does this generalize to setting where you don\u2019t have the associated labels?\n"}]}}
{"custom_id": "170", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. Regarding the abstention process, it appears to be based on a prediction probability threshold, where if the probability is lower than the threshold, the prediction is abstained? How does it different from a decision threshold used by the models? Can authors clarify that?\n"}]}}
{"custom_id": "171", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. The paper is well-organized. The writing is good and most of the content is very clear to me. Weaknesses/Feedback 1. The writing could be improved. It would be helpful to draw a table to compare different CoT prompting methods across different dimensions. How and why shall we make an assumption that \u201cquestions of all the wrong demonstrations fall into the same frequent-error cluster\u201d? Is the selection criteria in section 4.2 reasonable? Namely, why do we not choose questions with more than 60 tokens and rationales with more than 5 reasoning steps?\n"}]}}
{"custom_id": "172", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n* synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by \"support data\" and what by \"predicted training count data\"? Could you write down the model used here explicitly, e.g. add it to the appendix?\n"}]}}
{"custom_id": "173", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point:\n1) experimental settings for Figure 1 to Figure 9 are totally missing, which makes them hard to be convincing;\n"}]}}
{"custom_id": "174", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point:\n3. I am concerned whether the similarity-aware positive sample selection will accelerate GNN-based encoder over-smoothing, i.e., similar nodes or graphs will be trained with features that converge excessively and discard their own unique features. In addition, whether selecting positive samples in the same dataset without introducing some perturbation noise would lead to lower generalization performance. The authors experimented with the transfer performance of the model on the graph classification task, though it still did not allay my concerns about the model generalization. I hope there will be more experiments on different downstream tasks and across different domains. Remarks:\n"}]}}
{"custom_id": "175", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. The author only did experiment on two typical games, the ReBeL's performance on more complex problem. Especially when the game has bigger depth which will cause huge inputs of the value and policy function.\n"}]}}
{"custom_id": "176", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2. Since the sampling size 's' depends on the exponential term 2^[1/(e^2K-2)] (in Theorem 3.4), it could be very large if one requires the error tolerance 'e' to be relatively small and the order of tensor 'K' to be high. In that situation, there won't be much benefits to use this algorithm. Question:\n"}]}}
{"custom_id": "177", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n4) Some other representative panoptic segmentation models are not compared, like PanopticFPN, Mask2Former, etc.\n"}]}}
{"custom_id": "178", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1.There are missing details about division to train and test sets, numbers as well as how the division was made (simply random? Any other considerations?). These details should be added.\n"}]}}
{"custom_id": "179", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. As (suboptimally) weight decay is applied to all layers, we would expect a large training loss and thus suboptimal cosine similarities for large weight decay parameters. Conveniently, cosine similarities for such large weight decay strengths are not reported and the plots end at a weight decay strength where cosine similarities are still close to optimal.\n"}]}}
{"custom_id": "180", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- The experiments of the paper are done only on neural networks and image classification tasks. It would be interesting to see the performance of attack on other architecture and classification tasks.\n"}]}}
{"custom_id": "181", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n- Clarity: The submission is well written and easy to follow, the concept of coresets is well motivated and explained. While some more implementation details could be provided (source code is intended to be provided with camera-ready version), a re-implementation of the method appears feasible.\n"}]}}
{"custom_id": "182", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n- The performance is only compared with few methods. And the proposed is not consistently better than other methods. For those inferior results, some analysis should be provided since the results violate the motivation. I am willing to change my rating according to the feedback from authors and the comments from other reviewers.\n"}]}}
{"custom_id": "183", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n- Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.\n"}]}}
{"custom_id": "184", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n\u2022 How is the information redundancy built into the Fill, Propagate, Decode algorithms? o In reference to the sentence \u201c Finally, by comparing the performance of the secret model with or without fusion, we conclude that the robustness of Cans largely comes from the information redundancy implemented in our design of the weight pool\u201d\n"}]}}
{"custom_id": "185", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2) It is not clear regarding the choice of 20 distribution sets. Can we control the number of distribution sets for each class? What if you select only few number of distribution set?\n"}]}}
{"custom_id": "186", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim.\n"}]}}
{"custom_id": "187", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.\n"}]}}
{"custom_id": "188", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. It is hard to follow Sec. 3.2. The author may improve it and give more illustrations and examples.\n"}]}}
{"custom_id": "189", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n2.The effectiveness of lower bound double q-learning is doubtful. In MsPacman of Figure2, the algorithm shows slight performance decrease of Clipped DDQN, in some environment such as WizardOfWor, Zaxxon RoadRunner and BattleZone, these algorithms seems converge into same solutions. Besides, the algorithm would cause the overestimate the true maximum value.\n"}]}}
{"custom_id": "190", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. Sensitivity to other hyper-parameters. Minor Comments on language usage: (for example) 1. \"we typically considers\" in the above of (7) 2. \"two permutation\" in the above of Theorem 1 3. \"until converge\" in the above of (14) 4. ...... Please proofread the paper and fix all language problems.\n"}]}}
{"custom_id": "191", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\nrationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world.\n"}]}}
{"custom_id": "192", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point:\n1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters?\n"}]}}
{"custom_id": "193", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point:\n- The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.\n"}]}}
{"custom_id": "194", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\nrationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\nscore: No\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point:\n1. Novelty seems incremental to me. What are the ways in which this paper differs from https://aclanthology.org/2021.findings-acl.57.pdf? Is it just applying a very similar methodology to new task?\n"}]}}
{"custom_id": "195", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\nscore: No\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\nrationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point:\n4. Missing discussion about Set Transformer (https://arxiv.org/abs/1810.00825) and other related works that also uses summary tokens.\n"}]}}
{"custom_id": "196", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point:\n2. Experimental section: a. Need to report average over multiple runs. Results are very close together and it is hard to favor one method. b. Sec. 3.1: Since this is the toy-dataset, a discussion why the decision boundaries look as they do, would be interesting. c. Sec. 3.3: What information is in Fig. 9 middle and right?\n"}]}}
{"custom_id": "197", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\nrationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point:\n2. Figure 5 is hard to comprehend. I would like to see more details about the two baselines presented in Figure 5. The authors only study CATER for the English-centric datasets. However, as we know, the widespread text generation APIs are for translation, which supports multiple languages. Probably, the authors could extend CATER to other languages in the future.\n"}]}}
{"custom_id": "198", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\nrationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n1.) Theoretical comparisons to adaptive learning of GPRGNN is not clear.\n"}]}}
{"custom_id": "199", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\nrationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\nscore: No\n\n\nReview Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\nrationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\nscore: No\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\nrationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\nscore: No\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\nscore: Yes\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point:\n3. The proposed method might struggle to detect hallucinations in open-ended responses, for example, the prompt \"introduce a sports celebrity to me\". In this case, the sampled responses could pertain to different individuals, making it challenging to identify shared information for consistency checking.\n"}]}}
{"custom_id": "200", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: If the training data is imbalanced, the model might exhibit biased predictions.\nrationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\nscore: No\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\nscore: Yes\n\n\nReview Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\nrationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\nscore: No\n\n\nReview Point: Larger models generally require more computational resources for training and inference.\nrationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\nscore: No\n\n\nReview Point: When the number of training samples is small, overfitting is more likely to occur.\nrationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\nscore: No\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\nscore: No\n\n\nReview Point:\n3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that\u2019s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper. Detailed Comments:\n"}]}}

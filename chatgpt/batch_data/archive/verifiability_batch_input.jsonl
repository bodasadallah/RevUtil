{"custom_id": "1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point:\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n\n"}]}}
{"custom_id": "2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point:\n- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n\n"}]}}
{"custom_id": "3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point:\n- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.\n\n"}]}}
{"custom_id": "4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point:\n3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\n\n"}]}}
{"custom_id": "5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point:\n- A number of claims from this paper would benefit from more in-depth analysis.\n\n"}]}}
{"custom_id": "6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point:\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n\n"}]}}
{"custom_id": "7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point:\n- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).\n\n"}]}}
{"custom_id": "8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point:\n- the required implicit call to the Witness oracle is confusing.\n\n"}]}}
{"custom_id": "9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point:\n1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n\n"}]}}
{"custom_id": "10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point:\n- The writing should be improved. Some points in the paper is unclear to me.\n\n"}]}}
{"custom_id": "11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point:\n2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems.\n\n"}]}}
{"custom_id": "12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point:\n6) Adding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution.\n\n"}]}}
{"custom_id": "13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point:\n1) Some observations and subsequent design decisions might be hardware and software dependent;\n\n"}]}}
{"custom_id": "14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point:\n2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty.\n\n"}]}}
{"custom_id": "15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point:\n1) It seems that there is still room to improve the complexity of Algorithm 2;\n\n"}]}}
{"custom_id": "16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point:\n3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.\n\n"}]}}
{"custom_id": "17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point:\n* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.\n\n"}]}}
{"custom_id": "18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point:\n- The modulator is heuristically designed. It is hard to justify if there is a scalability issue that might need tedious hyperparameter tuning for diverse training data.\n\n"}]}}
{"custom_id": "19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point:\n* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.\n\n"}]}}
{"custom_id": "20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point:\n2. The synthetic experiment in a non-separable case seems to be a problem. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model?\n\n"}]}}
{"custom_id": "21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point:\n- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).\n\n"}]}}
{"custom_id": "22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point:\n- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.\n\n"}]}}
{"custom_id": "23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point:\n- \"semantic\" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper.\n\n"}]}}
{"custom_id": "24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point:\n\u2022 I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.\n\n"}]}}
{"custom_id": "25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point:\n- I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\n\n"}]}}
{"custom_id": "26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point:\n781 \"both tasks\": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.\n\n"}]}}
{"custom_id": "27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point:\n1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n\n"}]}}
{"custom_id": "28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point:\n1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2\u00d7) and ResNet50 (4\u00d7). It would be interesting to see the results of proposed InvP with these wider backbones.\n\n"}]}}
{"custom_id": "29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point:\n- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n\n"}]}}
{"custom_id": "30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point:\n- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice.\n\n"}]}}
{"custom_id": "31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point:\n3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.\n\n"}]}}
{"custom_id": "32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point:\n1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].\n\n"}]}}
{"custom_id": "33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point:\n- In figure 5, the y-axis label may use \"Exact Match ratio\" directly.\n\n"}]}}
{"custom_id": "34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point:\n4) The analysis from line 128 to 149 is not convincing enough. From the histogram as shown in Fig 3, the GS-P-50 model has smaller class selectivity score, which means GS-P-50 shares more features and ResNet-50 learns more class specific features. And authors hypothesize that additional context may allow the network to reduce its dependency. What is the reason such an observation can indicate GS-P-50 learns better representation? Reference: [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, CVPR, 2018. [2] W. Luo et al., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS, 2016.\n\n"}]}}
{"custom_id": "35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point:\n- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.\n\n"}]}}
{"custom_id": "36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point:\n2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.\n\n"}]}}
{"custom_id": "37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point:\n4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately.\n\n"}]}}
{"custom_id": "38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point:\n4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n\n"}]}}
{"custom_id": "39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point:\n2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that \"Both CNNs and ViTs seem to benefit similarly from increased model capacity\". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.\n\n"}]}}
{"custom_id": "40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point:\n- line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n\n"}]}}
{"custom_id": "41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point:\n3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\n\n"}]}}
{"custom_id": "42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point:\n- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers? I thank the authors for their response.\n\n"}]}}
{"custom_id": "43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point:\n2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.\n\n"}]}}
{"custom_id": "44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point:\n- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?\n\n"}]}}
{"custom_id": "45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point:\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\n\n"}]}}
{"custom_id": "46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point:\n- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\n\n"}]}}
{"custom_id": "47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point:\n3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.\n\n"}]}}
{"custom_id": "48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\nrationale: ### Step 1: Identify if the Comment Contains a Claim\n- The comment includes a subjective opinion: \"The evaluation on the word analogy task seems a bit unfair.\"\n- The reason given for this opinion is: \"the semantic relations are explicitly encoded by the sememes.\"\n\n### Step 2: Determine How Well the Claim is Verified\n- **Claim**: The evaluation seems unfair.\n- **Support**: The comment attempts to provide justification by pointing out that semantic relations are encoded by sememes.\n- **Analysis**:\n  - The justification is present but lacks depth and specificity. It hints at a reason why the evaluation might be unfair but does not provide detailed explanations or evidence.\n  - There is no external reference or detailed logical reasoning that thoroughly supports or explains why the sememes encoding explicitly leads to an unfair evaluation.\n  - The comment provides a starting point for understanding the claim, but the reasoning is incomplete and underdeveloped.\n\n### Conclusion\nGiven the above analysis, the claim is certainly more than just stated; it is partially supported with a brief rationale. However, it falls short of being fully articulated and requires additional evidence or a detailed explanation for complete clarity. Thus, the level of verification aligns with \"3: Somewhat Verifiable,\" consistent with the definitions.\nscore: 3\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point:\n4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.\n\n"}]}}
{"custom_id": "49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point:\n3. I didn't find all parameter values. What are the model parameters for task 1? What lambda was chosen for the Boltzmann policy. But more importantly: How were the parameters chosen? Maximum likelihood estimates?\n\n"}]}}
{"custom_id": "50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point:\n4. Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?\n\n"}]}}
{"custom_id": "51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point:\n3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.\n\n"}]}}
{"custom_id": "52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\nrationale: **Step 1: Identify Claims in the Comment**\n\n- The comment states, \"The main part can be more concise,\" which is a subjective opinion or suggestion, as it implies that the author should make changes to the paper for improvement. This is a claim because it's not an objective statement and requires justification.\n- The comment further specifies that this suggestion applies \"especially for the introduction part,\" providing some context for the claim.\n- There is an implied claim that empirical results should be included, though this is less explicit. It assumes empirical results are currently lacking and that their inclusion would be beneficial.\n\n**Step 2: Evaluate the Verification of Claims**\n\n- Regarding the claim that \"the main part can be more concise,\" the reviewer gives an example from the introduction, indicating which part of the paper they believe could be improved. However, the reviewer does not provide a rationale or specific reasoning as to why conciseness is necessary for this part. Without specific examples or reasoning, such as noting redundancy, lack of clarity, or overly complex language, the claim lacks sufficient support.\n- For the claim regarding including empirical results, there is no explanation of why empirical results are needed or what specific benefits their inclusion would provide. This part of the comment is open-ended and lacks any justification or reference to standards or practices within the field.\n\n**Step 3: Conclusion**\n\nAccording to the definitions provided, the comment has claims but fails to robustly justify them. The lack of detailed explanation or supporting evidence makes it challenging for the authors to understand the precise issue and how addressing it would enhance the paper. Therefore, this comment falls under category 2: Borderline Verifiable.\nscore: 2\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point:\n2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I\u2019m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?\n\n"}]}}
{"custom_id": "53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\nrationale: 1. **Identify Claims:**\n   - The comment states that \"the novelty is limited.\" This is a subjective statement and constitutes a claim about the paper's originality.\n   - The comment further supports this claim by mentioning that \"interpreting the prediction of deep neural networks using a linear model is not a new approach for model interpretation.\" This statement is also a claim and serves as a form of justification for the primary claim about limited novelty.\n\n2. **Evaluate Verification:**\n   - The justification provided mentions a general method (\"interpreting the prediction of deep neural networks using a linear model\") but does not offer specific examples, detailed explanations, or references to previous works that have employed this approach, which would strengthen the claim.\n   - While the comment does provide a direction for why the novelty might be limited, it lacks depth and specificity. It doesn't elaborate on why this approach is not novel or cite any existing works that have previously used this approach.\n\n3. **Assessment of Verification Level:**\n   - Given the definitions, the comment falls under \"2: Borderline Verifiable.\" This is because, although there is some support provided for the claim, it is insufficient and vague. The authors might struggle to understand or accept the justification without further details or references.\n\n4. **Conclusion:**\n   - The review comment contains claims but insufficient verification for those claims.\n   - The justification provided is borderline because it lacks specific examples and detailed explanations.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point:\n- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \\gamma^1 and \\gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).\n\n"}]}}
{"custom_id": "54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\nrationale: 1. **Identify Claims in the Comment:**\n   - The reviewer states, \"The paper does not dig into the theory proofs and show the convergence properties of the proposed algorithm.\" This statement is a claim because it implicitly suggests that the paper should include these elements or that they are important aspects missing from the paper. The reviewer is taking a subjective stand about the shortcomings of the paper.\n\n2. **Determine Verification:**\n   - The claim in the comment is not accompanied by any supporting evidence, logical reasoning, references, or common-sense arguments. The statement is made without further explanation or justification as to why the inclusion of theory proofs or convergence properties is necessary or important. Therefore, the claim stands without verification.\n\n3. **Consider Definitions and Categorize:**\n   - According to the definitions, comments with claims require verification to help authors understand or act on them. Here, the verification is absent, as the reviewer provides no support, examples, or references.\n   - This lack of verification aligns with the category \"1: Unverifiable,\" where the comment contains a claim without any supporting evidence or justification.\n\n**Conclusion:**\nThe comment makes a claim but fails to provide any verification, supporting evidence, or reasoned argument. This makes it fall under the \"Unverifiable\" category.\nscore: 1\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\nrationale: 1. **Identification of Claims:**\n   - First, we must determine if the comment contains any claims. Claims are subjective statements that require justification or evidence, such as opinions, suggestions, or judgments about the paper. \n\n2. **Reviewing the Comment:**\n   - The comment suggests adding a baseline smaller PCFG (Probabilistic Context-Free Grammar) with certain parameter settings for each PCFG with rank r. This part is more of a procedural suggestion or instruction rather than a claim that needs verification.\n   - The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" is an assessment based on the described setup. It provides a logical observation on the outcomes of using a specific method.\n\n3. **Evaluation of Claims or Need for Justification:**\n   - The procedural suggestion to add a baseline smaller PCFG involves a specific methodological approach but isn't a subjective claim needing verification in the context of improving the paper.\n   - The latter part about parsing F-1 and perplexity comparison is a logical inference about the expected outcome of the proposed setup. It is not an opinion or a stance requiring validation but rather a factual observation within the context of the technical setup.\n\n4. **Conclusion:**\n   - Given that the comment doesn't present a subjective opinion or a claim needing evidence or support\u2014it is a logical analysis or observation rooted in the technical details\u2014the comment indeed contains no claims as per the definitions.\n   - Therefore, this aligns with the label \"X - No Claim,\" as the comment is composed of descriptive, factual statements that do not require further justification.\nscore: X\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment suggests that \"the dataset itself might have a lot of noise.\" This is a subjective opinion because it's not stating an objective fact; rather, it's implying a potential issue with the dataset.\n   - The comment also implies a suggestion for improvement, as the reviewer would have liked more analysis of the dataset's quality and noise level.\n\n2. **Assess the Verification of the Claim:**\n   - The comment provides reasoning by pointing out that the \"collected 'pristine' set of tweets may not be pristine enough\" and could contain \"misinformation as well as out-of-context images.\" This reasoning supports the claim of potential noise in the dataset.\n   - Although an example is given, the comment does not go further to provide empirical evidence, specific instances, or references that could augment the validity of the claim. However, reasoning is based on logical assumptions about the challenges in curating datasets, which aligns with common sense about the difficulties in achieving a truly \"pristine\" dataset, especially when dealing with dynamic and user-generated content like tweets.\n\n3. **Conclusion:**\n   - The claim is considered \"Mostly Verifiable.\" The reasoning is quite logical and aligns with common sense, but the assessment could benefit from more detailed evidence or references. The explanation is sufficient yet leaves minor gaps that could be filled with specific examples or external references.\nscore: 4\n\n\nReview Point:\n- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.\n\n"}]}}
{"custom_id": "55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: 1. **Identifying Claims:**\n   - The comment states, \"A number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective statement and expresses an opinion or suggestion for improvement. It implies that the current level of analysis is inadequate according to the reviewer, which is a judgment about the paper.\n\n2. **Verification of the Claim:**\n   - The comment does not provide any logical reasoning, common-sense knowledge, or external references to justify why the claims in the paper would benefit from more in-depth analysis.\n   - There is no detailed explanation or examples given to support the assertion.\n   - Therefore, this is an opinion without any supporting evidence or justification.\n\n3. **Conclusion:**\n   - According to the categories defined, the comment falls under Category 1: Unverifiable. This is because the comment makes a claim without providing any supporting evidence or justification.\nscore: 1\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\nrationale: 1. **Identify Claims:**\n   The comment makes a clear claim: \"these baselines seem weak.\" This is an opinion or judgment about the baselines used in the research in question.\n\n2. **Verification of the Claim:**\n   - The reviewer supports the claim by comparing results between the baselines in Tables 3, 4, and 6 with the results achieved by Voita et al. (2019) using the CADec model. \n   - Specific numerical results are provided for deixis, lexical cohesion, ellipsis (inflectional), and ellipsis (VP), showcasing that Voita et al. (2019) achieved significantly higher scores (81.6, 58.1, 72.2, and 80.0) compared to the current work (64.7, 46.3, 65.9, and 53.0).\n   - This comparison serves as logical reasoning and provides strong quantitative evidence supporting the claim that the baselines used in the reviewed paper are weak.\n\n3. **Conclusion:**\n   - The claim is \"Fully Verifiable\" as it is thoroughly supported by explicit, sufficient, and robust evidence. The reviewer provides a clear and precise reasoning by directly comparing performance metrics between the baselines being critiqued and a stronger model from another referenced work (Voita et al. 2019).\nscore: 5\n\n\nReview Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\nrationale: 1. **Identify Claims:**\n   - The reviewer states that the sentence in line 212 is \"not strictly correct.\" This is a subjective claim expressing disagreement with how something is stated in the paper.\n   - The reviewer suggests an alternative wording, \"the correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors,\" which is a suggestion or request for change.\n\n2. **Verification:**\n   - The claim is that the current statement in the paper is incorrect. The reviewer partially justifies this by referring to Figure 2 as evidence that supports their suggested alternative.\n   - The verification is based on the reviewer's interpretation of Figure 2, which may suggest the use of a bidirectional encoder and encoding into a set of vectors. However, this verification lacks a detailed explanation of why the suggested method is more accurate, and there are no external references provided.\n   - The justification given is somewhat based on common sense within the field (about encoding techniques) but isn't fully articulated or referenced to specific works.\n\n3. **Categorization:**\n   - The comment is mostly verifiable because the reviewer explains why the original sentence is incorrect and suggests an alternative, offering some level of justification. However, the evidence is based solely on their interpretation of an internal figure, lacking explicit, robust evidence or detailed explanation.\n\n4. **Conclusion:**\n   - Based on the definitions and analysis above, the review comment contains claims primarily concerning the correctness of a statement and provides partial verification without full articulation or external references. It is classified as \"4: Mostly Verifiable\" because it offers a sufficient basis for the claims with minor gaps in justification and detail.\nscore: 4\n\n\nReview Point:\n4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand. Fig.\n\n"}]}}
{"custom_id": "56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\nrationale: 1. **Identifying Claims:**\n   - **Claim Identified:** The comment claims that \"better metadata embedding options are available\" for the results of zero-shot learning on the CUB dataset.\n   - **Nature of Claim:** This is a subjective opinion as the reviewer suggests that alternative metadata embeddings could yield better performance.\n\n2. **Assessing Verification:**\n   - **Verification Provided:** The comment supports this claim by referring to an external work, namely the paper by Reed et al., CVPR 2016, which presumably discusses better metadata embedding options.\n   - **Specific Reference:** The reviewer directs the authors to Table 1 of the referenced paper, providing a very specific and relevant external source to justify their claim.\n\n3. **Conclusion on Verification Level:**\n   - Given the specific direction to an external source that is expected to contain relevant information supporting the claim, the comment provides explicit and robust evidence. Thus, the claim is thoroughly supported.\n\nBased on these observations, the label \"5: Fully Verifiable\" is appropriate as the claim is completely substantiated by clear reasoning and specific reference to an external work.\nscore: 5\n\n\nReview Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\nrationale: To evaluate the given review comment, we must first identify whether the comment contains any claims. The comment states, \"Lines 102-106 is misleading,\" which implies a subjective opinion or judgment about the clarity or accuracy of these lines in the paper. As such, this is indeed a claim because it judges a part of the paper as misleading.\n\nNext, we examine how well this claim is verified:\n\n1. **Claim Identification**: The comment claims that a specific part of the paper (Lines 102-106) is misleading. The reviewer provides partial support by stating that, while certain elements like \"intersection and probs are true,\" the phrase \"'such distribution' cannot refer to the discussion above.\"\n\n2. **Verification Assessment**:\n   - **Logical Explanation**: The reviewer's comment that \"'such distribution' cannot refer to the discussion above\" attempts to provide a rationale but lacks detail. There is insufficient explanation about why this creates a misalignment or what makes it misleading, which leaves the authors needing more clarification to fully understand the issue.\n   - **Missing Elements**: The explanation stops short of offering specific examples or more detailed reasoning about why the reference to \"such distribution\" is problematic.\n\nBased on the evaluation, the claim is present but not fully substantiated. While there is an attempt to support the claim, it lacks detailed explanation or references, making it challenging for the authors to follow the reviewer's justification effortlessly. Thus, the comment fits into the \"3: Somewhat Verifiable\" category, as the claim is partially supported, but key elements like detailed examples or a robust explanation are missing.\nscore: 3\n\n\nReview Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\nrationale: 1. **Identify Claims:**  \n   - The comment essentially consists of questions about why certain operators (the \"and\" operator or element-wise max) were not considered. It mentions that these operators seem to correspond to the ideas of union and intersection from the \"or\" operator and element-wise min. The commenter states that it wasn't clear why the chosen ones were better options.\n   - The comment is primarily asking for clarification rather than presenting a direct or indirect claim. While there is an implicit query about the justification for choosing certain methods over others, this does not constitute a claim as defined, because it does not suggest a subjective stance, a specific judgment, or a recommendation.\n\n2. **Verification Assessment:**  \n   - Since no claim is made, there is no need to assess verification. The comment does not put forth an opinion, suggestion, or judgment that requires support or verification.\n\n3. **Conclusion:**  \n   - The comment is essentially seeking clarification and is objective in nature. It does not express a subjective opinion, take a stand, or provide judgment on the methods used. Therefore, according to the definitions provided, this comment does not contain any claims.\nscore: X\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point:\n3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation.\n\n"}]}}
{"custom_id": "57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nverifiability:\nThis aspect measures whether there is a claim (i.e. a subjective opinion) in the comment and how well it is verified. You need to detect first whether this review comment contains any claims. If there are any, evaluate how well the reviewer justifies or proves this claim by providing logical reasoning, using common sense or providing references. The claims' justification or validation can come before or after the claim. Claims don\u2019t need to be stated directly; they can also be inferred.\nDefinitions:\nOpinion & Claims\nSubjective statements. For example, an opinion or a stand that the reviewer takes (like a disagreement with an experimental choice).\nAny suggestions or requests for changes. For example, stating that something is worth discussing, should be removed, or added.\nAny comments judging some parts of the paper. For example, stating something is hard to read, not detailed enough, or comments about how good or bad some section of the paper is.\nAny deductions or inferred observations that go beyond just stating facts or results from the paper.\nGenerally, any phrases where the reviewer should provide evidence to back up their claim and help the authors understand it better. This can be direct or indirect:\n    - Ex: \u201cImportant methods like X are not discussed\u201d. We can infer that the reviewer suggests that method X should be discussed. Hence, the reviewer should state why this method should be discussed.\nVerification\nThe claim is verified by providing logical reasoning.\nThe claim is verified through common sense knowledge in the field. For example, referring to certain commonly used practices or standards.\nThe claim is verified by providing external references.\nNormal Statements\nNormal statements should be given the label \"No Claim\".\nIndicating that something exists, or missing without indicating that it should be removed or included.\nGeneral statements about the paper, that don\u2019t include an opinion.\nObjective and factual statements that don\u2019t need any kind of verification.\nAsking for clarifications and general questions.\nLogical statements, or things that can be inferred directly.\nWe treat positive claims as normal sentences, as they are of little use to the authors to improve their paper.\n    - Example: This paper is well written, and the experimentation methods are well designed.\nVerifiability is rated on a scale from 1-5, and \"No Claim\" (for normal statements). We will now provide a definition for each.\n1: Unverifiable\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples\nThe results fall behind previous work, and the reasons for this should be investigated.\nFor many of the datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\nWhile the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\n2: Borderline Verifiable\nDefinition: The comment provides some support for its claim, but it is insufficient, vague, or not fully articulated. The authors will struggle to follow the justification.\nExamples\nThis method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\nIt is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\nIn the experiments, the transfer tasks are too artificial. \u201cAt the pretraining stage, we train the models with examples from two classes (\u201cbird\" vs. \u201cfrog\") for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST\u201d.\n3: Somewhat Verifiable\nDefinition: The comment provides support for its claim, but one or more key elements are missing, such as specific examples, detailed explanations, or supporting references. It requires significant effort from the authors to follow the justification.\nExamples\nThe evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models.\nThe nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data\nThe approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table.\n4: Mostly Verifiable\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference to support their claims.\nExamples\nThe statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\nThe two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream\nAs the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given in the paper suggests that double Q learning resolves the overestimation problem by achieving a fast convergence rate.\n5: Fully Verifiable\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be done by:\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nThe landscape results in parameter space looks very surprising because it has no assumptions on the generator and discriminator architecture except for enough representation. This looks surprising to me because usually, this kind of global optimization result for neural networks needs strong assumptions on the architecture.\nThe first weakness of this work is that the wish list presented in the Introduction is a bit wider than the real techniques proposed by this work because the key difference of this work lies in the dynamic prior. The three properties were mentioned and basically solved by previous work like reference [21] and [27].\nThe paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. There have already been multiple papers based on this idea [9, 19]. This paper simply proposes to combine the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA), which results in SimBA++, which is the main gain over the previous approaches reported in the paper.\n No Claim\nDefinition: The comment does not contain any claim, opinion, or suggestion and consists of only factual, descriptive statements that do not require any justification.\nClear and precise reasoning or explanation.\nReferences to external works/data, when applicable, are specific and relevant.\nCommon-sense arguments are logically unassailable.\nExamples\nHow would this method perform empirically with multiple constraints?\nEntropy requires significant computation.\nThis algorithm is slow, as it relies on an O(N\u00b2) algorithm.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\nrationale: 1. **Identify Claims:**\n   - The comment suggests that \"using integrated gradients to measure the attribution has been studied in existing papers.\" This is a claim since it is an assertion about the state of research that invites verification.\n   - It then points out that \"the paper doesn\u2019t show how important these post-processing steps are,\" and suggests that \"an ablation study may be needed.\" These statements imply a judgment about the sufficiency of the current paper's analysis, suggesting that additional work is necessary. This is again a claim, as it involves a subjective evaluation and a suggestion.\n\n2. **Verification of Claims:**\n   - The first claim about integrated gradients lacks verification because no specific studies or references are provided to substantiate the assertion that integrated gradients have been studied in existing papers. Without supporting documentation or references, this claim is unverifiable.\n   - The second claim about the importance of post-processing steps and the need for an ablation study is partially verifiable. The suggestion of an ablation study is a common practice to verify the effectiveness of certain methods or components, which is a shared understanding in the research field. This implies some level of justification, albeit without detailed explanation or reference, making it somewhat verifiable.\n\n3. **Conclusion:**\n   - The first claim is completely unsupported by evidence or references and thus falls under the \"Unverifiable\" category.\n   - The second claim, suggesting an ablation study, is indirectly supported by the common practice of using ablation studies for verification. However, due to the absence of robust justification or references, it is better classified as \"Borderline Verifiable.\"\n\n4. **Final Label Decision:**\n   - Considering the major focus in the feedback provided, the overall claim about integrated gradients is highlighted as \"Unverifiable\" due to the lack of supporting references.\nscore: 1\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: 1. **Identify Claims:**\n   - The comment contains a claim about the \"lack of novelty\" in the paper. This is clearly a subjective opinion and a critical judgment about the paper's contribution.\n\n2. **Assess Justification/Verification:**\n   - The reviewer states that adversarial attacks by perturbing text have been previously applied to many NLP models and image-text models, and this is summarized in the related work of the paper. This statement acts as part of the reasoning for why the work may lack novelty.\n   - The reviewer mentions that the only new effort is applying similar ideas to video-text models. This suggests that while there may be some novelty, it is limited, and the major techniques have been previously explored, which contributes to the overall claim of lack of novelty.\n\n3. **Verification Level:**\n   - The comment provides logical reasoning by mentioning that the idea has been applied in related areas (NLP and image-text models) and is summarized in the related work, which is part of the paper itself. This shows that the reviewer has a basis for their claim.\n   - However, the reasoning could be further strengthened by specifying particular studies or works that have previously applied these ideas, or by discussing how significantly different or similar the current application is to previous ones.\n   - The rationale is sufficiently clear but could benefit from more detailed examples or explicit references for a fully robust justification.\n\n4. **Conclusion:**\n   - The claim about lack of novelty is supported by some evidence and logical reasoning; however, there is room for providing more detailed support, such as explicit examples from the related work or additional references. Therefore, the comment is best labeled as \"Mostly Verifiable.\"\nscore: 4\n\n\nReview Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\nrationale: 1. **Claim Identification**: \n   - The claim centers around the clarity and logical consistency of Sec. 4.2 regarding the use of a question to learn attention on an image feature.\n   \n2. **Verification of the Claim**:\n   - The comment provides a thorough justification for the claim by explaining the mismatch: \"Specifically, the equation does not have any term for r^q, which is the question representation.\" This points to a specific element (r^q) that is missing from the equation, thus heavily supporting the claim.\n   - The justification is both specific and detailed, addressing both the absence of a term in an equation and the lack of clarity regarding another part (\\sigma), as well as its implication on numerical stability.\n   - This detailed explanation demonstrates the logical reasoning required to verify the claim, making it precise and robust.\n\nGiven the detailed nature of the verification, the comment can be considered \"Fully Verifiable\" as it meets the criteria of providing clear, precise reasoning without leaving significant gaps in the explanation.\n\n**Refined Feedback**:\nThe comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\nscore: 5\n\n\nReview Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\nrationale: 1. **Identify Claims:**\n   - The comment expresses a claim when the reviewer says, \"The relatively poor performance on nouns makes me uneasy.\" This is a claim expressing a subjective opinion regarding the performance on nouns.\n   - There is another claim in the statement: \"This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\" Here, the reviewer claims that the paper's assertion about generalizability is contradicted by the observed performance.\n\n2. **Evaluate Verification:**\n   - The first claim about \"poor performance on nouns\" is a subjective feeling (\"makes me uneasy\") without justification; thus, it's mostly unverifiable on its own.\n   - For the second claim about contradiction, there is a partial verification: the reviewer attempts to justify this claim by pointing out that \"the performance clearly isn't uniform.\" However, this justification lacks detail. There's no specific evidence or examples given to fully support this observation, nor are there references to data or results. The reviewer doesn't specify what makes the performance non-uniform or elaborate on how it contradicts generalizability claims.\n\n3. **Conclusion:**\n   - Given the lack of detailed explanation or specific evidence for the claims made, particularly the second one, the comment offers some insight and direction (indicating a concern with generalizability based on performance), but this insight is vague and inadequately justified. Therefore, it fits into the \"Borderline Verifiable\" category, as identified in the feedback.\nscore: 2\n\n\nReview Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\nrationale: 1. **Identify Claims in the Comment:**\n   - The comment expresses dissatisfaction with the lack of comparison between the proposed models and models that consider different senses but not sememes. It suggests that the MST baseline might be an example of such a model but notes the lack of detailed description. \n   - There is a suggestion that the paper would be stronger with the inclusion of more baselines based on related work. This is a suggestion/opinion and hence a claim.\n\n2. **Evaluate the Verification of Claims:**\n   - The claim that the paper would be stronger with more baselines is partially supported by the observation that the MST baseline is not sufficiently described, and the emphasis is on soft vs. hard word sense disambiguation. \n   - However, the comment does not provide specific examples or detailed references from related work that could serve as additional baselines, nor does it articulate why including these baselines would strengthen the paper besides the broad mention of comparison necessity.\n   - This lack of specific examples or detailed explanations means the claim is somewhat verifiable; it's clear there is an issue, but the support provided is insufficient to fully guide the authors.\n\n3. **Conclusion:**\n   - Given the identification of claims and analysis of their verification, the label \"3: Somewhat Verifiable\" is appropriate. The comment suggests an improvement area and provides some reasoning but lacks detailed support or specific examples.\nscore: 3\n\n\nReview Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\nrationale: 1. **Identify the Presence of Claims:**\n   - The comment asks two questions: \"What is the purpose of the average duration reported in Table 1?\" and \"Does it include time spent by the user waiting for the model to generate a response?\"\n   - The comment also includes a statement: \"There is no supporting explanation about it.\"\n\n2. **Analysis:**\n   - The questions are seeking clarification and do not express an opinion or a subjective stance. They are straightforward inquiries aimed at understanding specific information.\n   - The statement \"There is no supporting explanation about it\" is an observation or factual description of the lack of explanation in the paper about the average duration reported in Table 1. It does not express a subjective judgment or inference that goes beyond stating the observed fact.\n   - There is no suggestion for changes, nor is there an opinion about the value, quality, or necessity of the data in Table 1.\n\n3. **Verification of Claims:**\n   - Since there are no claims identified, there is nothing to verify.\n\n4. **Conclusion:**\n   - The comment comprises factual observations and straightforward questions that do not require justification, and it lacks any form of opinion, claim, or suggestion. As such, it fits the criteria for classification as \"X - No Claim.\"\nscore: X\n\n\nReview Point:\n- Search models comparison 5.1: what does 100 steps here mean? Is it 100 sampled strategies?\n\n"}]}}

{"custom_id": "1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n\n"}]}}
{"custom_id": "2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n\n"}]}}
{"custom_id": "3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.\n\n"}]}}
{"custom_id": "4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point:\n- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.\n\n"}]}}
{"custom_id": "5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point:\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\n\n"}]}}
{"custom_id": "6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point:\n- A number of claims from this paper would benefit from more in-depth analysis.\n\n"}]}}
{"custom_id": "7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).\n\n"}]}}
{"custom_id": "8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point:\n- the required implicit call to the Witness oracle is confusing.\n\n"}]}}
{"custom_id": "9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new.\n\n"}]}}
{"custom_id": "10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point:\n3. The innovations of network architecture design and constraint embedding are rather limited. The authors discussed that the performance is limited by the performance of the oracle expert.\n\n"}]}}
{"custom_id": "11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point:\n1) there is a drop of correlation after a short period of training, which goes up with more training iterations;\n\n"}]}}
{"custom_id": "12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point:\n1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification\n\n"}]}}
{"custom_id": "13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n- The hGRU architecture seems pretty ad-hoc and not very well motivated.\n\n"}]}}
{"custom_id": "14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n1) Some observations and subsequent design decisions might be hardware and software dependent;\n\n"}]}}
{"custom_id": "15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n- Very difficult to follow the motivation of this paper. And it looks like an incremental engineering paper.\n\n"}]}}
{"custom_id": "16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n2) the results in section 4 apply only to shallow fully-connected ReLU networks;\n\n"}]}}
{"custom_id": "17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point:\n- The applicability of the robust training scheme seems unlikely to scale to practical datasets, particularly those supported on high-dimensional domains. It seems like the accuracy would scale unfavorably unless the size of V scales exponentially with the dimension.\n\n"}]}}
{"custom_id": "18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n2. The results, while mostly based on \"standard\" techniques, are not obvious a priori, and require a fair degree of technical competency (i.e., the techniques are really only \"standard\" to a small group of experts).\n\n"}]}}
{"custom_id": "19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point:\n- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).\n\n"}]}}
{"custom_id": "20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point:\n- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice.\n\n"}]}}
{"custom_id": "21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (\"strategic predictions\", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player).\n\n"}]}}
{"custom_id": "22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n- The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions:\n\n"}]}}
{"custom_id": "23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n\n"}]}}
{"custom_id": "24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point:\n- You write: \"Evidently, replacing any of the procedure steps of XAIFOOLER with a random mechanism dropped its performance\" I'm unsure that \"better than random\" is a strong demonstration of capability.\n\n"}]}}
{"custom_id": "25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point:\n3. Make the captions more descriptive. It's annoying to have to search through the text for your interpretation of the figures, which is usually on a different page 4. Explain the scramble network better...\n\n"}]}}
{"custom_id": "26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point:\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n\n"}]}}
{"custom_id": "27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here. Finally, I'm hesitating to call it \"knowledge\". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.\n\n"}]}}
{"custom_id": "28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n- vulnerability discovery methodology is also questionable. The authors consider a single vulnerability at a time, and while they acknowledge and address the data imbalance issue, I am not sure about the ecological validity of such a study. Previous work has considered multiple CVEs or CWEs at a time, and report whether or not the code contains any such vulnerability. Are the authors arguing that identifying one vulnerability at a time is an intended use case? In any case, the results are difficult to interpret (or are marginal improvements at best).\n\n"}]}}
{"custom_id": "29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point:\n- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n\n"}]}}
{"custom_id": "30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n- [Originality] The winner-take-all property has been widely used in previous works such as NN-based clustering algorithms [1] and it\u2019s unclear how this paper contributes novelly to the understanding of this behavior with its extremely simplified settings, especially since most of the findings have been reported in previous works (Sec 5).\n\n"}]}}
{"custom_id": "31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point:\n1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\n\n"}]}}
{"custom_id": "32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point:\n2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n\n"}]}}
{"custom_id": "33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n- This pipeline style method including two models does not give better average results for both XVNLI and MaRVL. Baseline models in the experiments are not well introduced.\n\n"}]}}
{"custom_id": "34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n- When trained and evaluated with the same time-step, Figure 5 shows similar performance between the baseline model and the time-aware model. This makes the effectiveness of the proposed methods questionable when the goal is just to achieve good performance. Maybe under some scenarios where the training time-step and evaluation time-step are different, the proposed method might make more sense.\n\n"}]}}
{"custom_id": "35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point:\n4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n\n"}]}}
{"custom_id": "36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n\n"}]}}
{"custom_id": "37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive.\n\n"}]}}
{"custom_id": "38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n- The experiments are limited to toy data. There is a range of problems with real data where barycenters can be used and it would be interesting to show performance of the method in those settings too.\n\n"}]}}
{"custom_id": "39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point:\n2. The weak recovery problem studied here is primarily of theoretical interest, and it is not clear if the AMP algorithm is useful for non-Gaussian problems. So practical impact may be limited.\n\n"}]}}
{"custom_id": "40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point:\n- It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate.\n\n"}]}}
{"custom_id": "41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\n\n"}]}}
{"custom_id": "42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point:\n681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 \"latent in verbs\": why don't you mention objects here?\n\n"}]}}
{"custom_id": "43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point:\n- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.\n\n"}]}}
{"custom_id": "44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point:\n8.L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case? Minor Points:\n\n"}]}}
{"custom_id": "45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point:\n- Disentanglement. It is not clear how disentanglement is guaranteed. Although \"Broader Impacts and Limitations\" stated that \"Obtaining fully disentangled latent vectors ... a limitation\", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.\n\n"}]}}
{"custom_id": "46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?\n\n"}]}}
{"custom_id": "47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n3. The analysis of experimental results is insufficient. For instance, the authors only mention that the scope prompting method shows poor performance on GPT-3.5-turbo, but they do not provide any analysis of the underlying reasons behind this outcome.\n\n"}]}}
{"custom_id": "48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point:\n1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions).\n\n"}]}}
{"custom_id": "49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point:\n1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date?\n\n"}]}}
{"custom_id": "50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n\n"}]}}
{"custom_id": "51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].\n\n"}]}}
{"custom_id": "52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point:\n1. The motivation of this work should be further justified. In few-shot learning, we usually consider how to leverage a few instances to learn a generalizable model. This paper defines and creates a few-shot situation for graph link prediction, but the proposed method does not consider how to effectively use \u201cfew-shot\u201d and how to guarantee the trained model can be generalized well to new tasks with 0/few training steps.\n\n"}]}}
{"custom_id": "53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Sch\u00fctez, etc) from ones that don't.\n\n"}]}}
{"custom_id": "54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point:\n3. The ICL-HAR, while improving consistency and verifiability, has greatly impedes the accuracy scores (dropping from 70.4 to 55.6 on TRIP). This should be discussed or at least acknowledged in the main text in more detail.\n\n"}]}}
{"custom_id": "55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point:\n4. Does the claim \"It can be seen from the table that our proposed modules improve in both accuracy and completeness\" really hold? Why not use another dataset for the ablation study, e.g., the training set of Tanks & Temples or ETH3D?\n\n"}]}}
{"custom_id": "56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance \"I support it 100/% #\u092e\u0941\u0938\u094d\u0932\u093f\u092e\u094b_\u0915\u093e_\u0938\u0902\u092a\u0942\u0930\u094d\u0923_\u092c\u0939\u093f\u0937\u094d\u0915\u093e\u0930\" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification?\n\n"}]}}
{"custom_id": "57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n- The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs.\n\n"}]}}
{"custom_id": "58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n* The paper lacks an ablation study explaining why they chose the prompt in this specific way, e.g., few-shot examples for CoT might improve performance.\n\n"}]}}
{"custom_id": "59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point:\n- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.\n\n"}]}}
{"custom_id": "60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point:\n3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.\n\n"}]}}
{"custom_id": "61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n- Fig. 1 can also be drawn better to show the processing pipeline (prompt generation and manual check, demonstration selection with ground truth scores, and automatic scoring alongwith showing where model training is being used to optimize the selection modules.\n\n"}]}}
{"custom_id": "62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point:\n- The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.\n\n"}]}}
{"custom_id": "63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n2. \u00c2 Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \\sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \\lambda changes the performance, but it would be nice to see how \\eta and \\tau in equation (7) affect performance. Minor comments:\n\n"}]}}
{"custom_id": "64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\n\n"}]}}
{"custom_id": "65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point:\n5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n\n"}]}}
{"custom_id": "66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point:\n2) It seems that performance and sample efficiency are sensitive to \u03bb parameters. (Page 9, lines 310-313) I don't understand how the process of calculating the \u03bb is done. How is \u03bb computed from step here? (Page 8 lines 281-285) The authors explain why ELLA does not increase sample efficiency in a COMBO environment, but I don't quite understand what it means. [1] Yuri Burda et al, Exploration by Random Network Distillation, ICLR 2019 [2] Deepak Pathak et al, Curiosity-driven Exploration by Self-supervised Prediction, ICML 2017 [3] Roberta Raileanu et al, RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments, ICLR 2020\n\n"}]}}
{"custom_id": "67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point:\n8: s/expensive approaches2) allows/expensive approaches,2) allows/ p.8: s/estimates3) is/estimates, and3) is/ In the references: Various words in many of the references need capitalization, such as \"ai\" in Amodei et al. (2016), \"bayesian\" in many of the papers, and \"Advances in neural information processing systems\" in several of the papers. Dusenberry et al. (2020) was published in ICML 2020 Osawa et al. (2019) was published in NeurIPS 2019 Swiatkowski et al. (2020) was published in ICML 2020 p. 13, supplement, Fig.\n\n"}]}}
{"custom_id": "68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:\n\n"}]}}
{"custom_id": "69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point:\n- Some abbreviations are not defined, e.g., \u201cNE\u201d on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read. [1] S. Zhang et al, \u201cEfficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control\u201d, NeurIPS 2019. [2] Z. Ding et al, \u201cLearning Individually Inferred Communication for Multi-Agent Cooperation\u201d, NeurIPS 2020. [3] T. Wang et al, \u201cLearning Nearly Decomposable Value Functions Via Communication Minimization\u201d, ICLR 2020.\n\n"}]}}
{"custom_id": "70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - A number of claims from this paper would benefit from more in-depth analysis.\nrationale: To evaluate the review comment provided, we need to apply the definitions of the different levels of helpfulness. We'll follow a step-by-step approach to determine how helpful this comment is for the authors.\n\n1. **Identify Specificity:** The comment suggests that \"a number of claims... would benefit from more in-depth analysis.\" However, it does not specify which claims are referred to. This lack of specificity means the authors cannot pinpoint what exactly needs attention.\n\n2. **Provide Justification:** The comment fails to explain why these unspecified claims require more analysis. Without justification, the authors lack understanding of the rationale behind the suggestion.\n\n3. **Suggest Improvements:** For a comment to be helpful, it should ideally guide the authors towards potential improvements. This comment does not offer any guidance on how to conduct the analysis or what aspects to focus on.\n\n4. **Assess Actionability:** Because the comment lacks specificity, justification, and instructions for improvement, it provides no actionable steps the authors can take to improve their paper.\n\nGiven this evaluation, the comment fits the definition of \"not helpful at all\" because it fails to identify any specific weaknesses or suggest actionable improvements. It leaves the authors without clear direction or understanding of what areas to address or how to enhance their work.\n\n**Conclusion:** The comment is not helpful at all. It is too vague and lacks the necessary detail to be actionable or beneficial to the authors.\nscore: 1\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity**: The comment queries how different types of inputs (e.g., bio-medical signals or speech) should be addressed in the paper. It specifies that the authors should discuss and present solutions to this issue. Additionally, it notes that the citation appears disordered.\n\n2. **Provide Justification**: The comment implies that discussing how to handle various input types would add value to the paper. While it does not elaborate on why this is important, the suggestion itself points to a potential gap in the manuscript.\n\n3. **Suggest Improvements**: The comment suggests explicitly discussing and presenting solutions for dealing with different input types. It also hints at the need to organize the citation better, though no specific guidance is provided on how to achieve this.\n\n4. **Assess Actionability**: The comment provides actionable feedback by urging the authors to address the question raised and to tidy up the citations. However, the advice lacks depth and specific strategies, leaving the authors to figure out how best to implement these changes.\n\n### Conclusion:\n\nThe comment leaves room for improvement in terms of depth and actionable detail. It highlights important areas for improvement, such as addressing the role of different input types and organizing citations more effectively. However, the feedback does not fully guide the authors on how to accomplish these tasks.\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point:\n- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\n\n"}]}}
{"custom_id": "71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\nrationale: To properly evaluate the review comment using the given definitions, let's apply the criteria of Specificity, Provide Justification, Suggest Improvements, and Assess Actionability:\n\n1. **Identify Specificity:**\n   - The comment mentions \"fleshing out of the assumptions\" and concerns about dedicating \"a whole section of the paper plus experimental results.\" This provides a specific aspect of the paper being critiqued.\n\n2. **Provide Justification:**\n   - The comment does not provide any justification for why dedicating a lot of space to the assumptions section and experimental results might be problematic. This lack of explanation hinders understanding the underlying reasoning behind the comment.\n\n3. **Suggest Improvements:**\n   - The comment fails to suggest any improvements or alternatives. It does not guide the authors on how they could address the concern about the space allocation.\n\n4. **Assess Actionability:**\n   - Without a rationale or suggestion for improvement, it is difficult for the authors to take specific actions to address the concern. The feedback\u2019s actionability is therefore minimal.\n\n**Conclusion:**\nThe comment identifies an aspect of the paper (space allocation for the assumptions and experimental results) but does not explain why this is an issue or provide suggestions on how to improve it. The lack of clarity and guidance results in feedback that only slightly benefits the authors in understanding what they should change. Overall, the comment fails to provide actionable feedback that the authors can effectively use, making it barely helpful.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\nrationale: To evaluate the given review comment, we'll assess it through several criteria based on the provided definitions: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability. Let's explore each criterion step by step:\n\n1. **Identify Specificity**: The comment precisely identifies a specific section of the paper (lines 22-30) and mentions a particular aspect\u2014distributional RL\u2014and its discussion in terms of available methodologies (moment matching vs. quantile regression).\n\n2. **Provide Justification**: The reviewer explains that the paper lacks relevant literature, specifically highlighting that there is missing work on using moment matching for Distributional Reinforcement Learning (DRL).\n\n3. **Suggest Improvements**: The comment suggests the inclusion of a specific literature reference that should be discussed in the context of the paper when addressing various approaches to DRL\u2014the work by Nguyen-Tang et al (AAAI\u201921).\n\n4. **Assess Actionability**: The authors can take concrete action based on this feedback. They can investigate the suggested literature and incorporate relevant discussions or comparisons to enhance their paper\u2019s depth and comprehensiveness regarding the discussed topic.\n\nIn conclusion, this review comment is highly helpful to the authors. It provides detailed feedback, is specific in identifying weaknesses, justifies the need for inclusion, suggests precise improvements, and is actionable, fulfilling all criteria effectively. This enables the authors to significantly enhance their draft by adding the missing discussion.\nscore: 5\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point:\n- 241: It would also be good to state the maximum number of tasks done by any annotator.\n\n"}]}}
{"custom_id": "72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons.\nrationale: To evaluate the review comment according to the given definitions, let's analyze and break down the criteria:\n\n### Step by Step Evaluation:\n\n1. **Identify Specificity**:  \n   The comment notes that \"integrated gradients\" have been studied in existing papers and acknowledges the paper's proposal of post-processing steps to filter out \"false-positive\" neurons. However, it does not specify which papers have also conducted similar studies, nor does it delve into the specifics of how the paper's method compares or contrasts with existing studies.\n\n2. **Provide Justification**:  \n   It lacks a rationale or explanation for why mentioning previous studies impacts the current work. There are no detailed insights or justifications provided regarding the effectiveness, novelty, or limitations of the methods used in the paper.\n\n3. **Suggest Improvements**:  \n   The comment does not suggest any improvements or offer any constructive criticism. It merely provides a statement of fact without indicating how the authors could refine or advance their work, leaving the authors with no clear direction or areas for improvement.\n\n4. **Assess Actionability**:  \n   The feedback provides no actionable steps or recommendations for the authors to pursue. As such, the authors are left without tangible actions that could potentially improve their work. This significantly limits the value of the comment as it doesn't assist in enhancing the draft.\n\n### Conclusion:\n\nBased on the analysis, the comment lacks specificity, justification, suggested improvements, and actionability, rendering it not helpful at all. It neither identifies weaknesses nor aids in improving the draft comprehensively. Given these shortcomings and based on the criteria, it rightly matches the label \"1. The comment is not helpful at all.\"\nscore: 1\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\nrationale: To evaluate the review comment and determine its level of helpfulness, we need to assess it using the criteria provided: Identify Specificity, Provide Justification, Suggest Improvements, and Assess Actionability.\n\n1. **Identify Specificity**: The comment specifically highlights an issue with the evaluation of the word analogy task. This demonstrates a focus on a particular aspect of the paper.\n\n2. **Provide Justification**: The comment justifies the claim by stating that the word analogy task seems unfair because the semantic relations are explicitly encoded by the sememes, as acknowledged by the authors. This provides a reasoning behind the observation, enhancing the comment's credibility.\n\n3. **Suggest Improvements**: The comment does not explicitly provide suggestions for improvements. While it flags a potential issue, it doesn't guide the authors on how to address the concern or propose alternatives. This limits the comment's usefulness in terms of actionable advice.\n\n4. **Assess Actionability**: The comment indirectly suggests that the authors should reconsider the fairness of their evaluation method, but without specific guidance on how to achieve this. While it points the authors to a potential area of improvement, it does not provide detailed actions that can be taken.\n\n**Conclusion**: Based on this step-by-step analysis, the comment can be labeled as \"3. The comment is somewhat helpful.\" It identifies and justifies a specific issue, which is beneficial for the authors. However, the lack of direct suggestions or comprehensive guidance reduces its overall helpfulness, thus aligning with the definition for a \"somewhat helpful\" comment.\nscore: 3\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point:\n--- W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.\n\n"}]}}
{"custom_id": "73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn\u2019t provide insight into this but assumed morphologic segmentation will be invariant.\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: \n   - The comment specifically addresses the use of morphologic segmentation in cross-domain contexts and questions whether it should be conducted differently across domains or remain invariant. This specificity helps the authors pinpoint exactly what aspect of their paper needs attention.\n\n2. **Provide Justification**:\n   - The comment justifies the need for additional explanation by highlighting the importance of these questions in the context of the task domain adaptation. This adds clarity to why the authors should focus on this particular aspect.\n\n3. **Suggest Improvements**:\n   - The comment suggests that the authors should provide more information on how morphologic segmentation applies to various domains and whether it should be different or invariant. While it does not specify how to make these improvements, it does outline what content is missing and needs to be addressed.\n\n4. **Assess Actionability**:\n   - The actionability of the comment is significant because it instructs the authors to delve deeper into the topic of morphologic segmentation and domain adaptation. However, it could benefit from more detailed guidance on how to integrate these elements into the paper.\n\n**Conclusion**:\nOverall, the review comment provides clear feedback on specific weaknesses related to the explanation of morphologic segmentation in cross-domain scenarios. It prompts the authors to think about these crucial aspects, offering a specific direction for improvement, albeit without providing a comprehensive strategy. This aligns with the definition of \"4. The comment is mostly helpful,\" where the feedback is clear and actionable but could still be expanded for greater comprehensiveness.\nscore: 4\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point:\n2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps? [1] https://github.com/watchernyu/REDQ\n\n"}]}}
{"custom_id": "74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\nrationale: To evaluate the review comment according to the given definitions, let's break down the criteria and assess each aspect:\n\n1. **Identify Specificity**: The comment specifically identifies the part of the paper that is unclear\u2014namely, the description of HIERENC. It explains the issue by detailing that the input (h_i) is derived from averaging the representations of all possible context instantiations filled by entities in the vocabulary.\n\n2. **Provide Justification**: The comment justifies why this approach may be problematic by suggesting that it might introduce noise because only one instantiation is presumably correct.\n\n3. **Suggest Improvements**: While the comment does not directly offer a specific solution, it clearly highlights a conceptual flaw. By pointing out that averaging all possible instantiations could introduce noise, it implicitly suggests that a more selective method of choosing the correct instantiation might be an improvement.\n\n4. **Assess Actionability**: The feedback is actionable because it identifies a specific problem in the methodology and hints at why it might fail, which allows the authors to reconsider their approach and potentially refine how the input (h_i) is derived.\n\n**Conclusion**: Given the breakdown above, the comment is thorough in identifying a specific issue, provides a solid justification regarding why the current method might be flawed, and is actionable in terms of prompting the authors to rethink their approach to improve the clarity and functionality of their model. While it does not offer direct suggestions for improvement, the clarity and depth of the review enable significant refinement of the work, aligning well with the label of \"highly helpful\" as defined in step 5.\nscore: 5\n\n\nReview Point: - It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.\nrationale: ### Evaluation of the Review Comment:\n\n1. **Identify Specificity:**  \n   The comment identifies a specific area, namely the potential presence of societal biases in the knowledge bases being used. This is a crucial aspect to address, as it can impact the validity and applicability of the findings. However, the comment does not elaborate on what specific biases might be present or how these biases could affect the study. The level of specificity here is low because it lacks detail and clarity.\n\n2. **Provide Justification:**  \n   The comment does not provide a justification for its concern about potential societal biases. It does not explain why this aspect is important or how it could impact the study's outcomes. Providing a rationale would help the authors understand the significance of addressing this issue.\n\n3. **Suggest Improvements:**  \n   The comment fails to offer concrete suggestions or steps the authors could take to investigate or mitigate potential biases. It does not guide the authors on how to resolve the concern, which limits its usefulness.\n\n4. **Assess Actionability:**  \n   The actionability of the feedback is limited. While the comment highlights an issue that the authors could look into, it does not direct them toward specific actions or corrections. The feedback leaves the authors with the task of deciphering the problem and finding a way to address it themselves.\n\n### Conclusion:\n\nBased on the above criteria, the comment lacks specificity, justification, and actionable suggestions; hence, it is deemed barely helpful. It alerts the authors to a potential issue, but without providing detailed guidance or rationale, it offers minimal value in improving the draft.\nscore: 2\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\nrationale: ### Step-by-Step Evaluation:\n\n1. **Identify Specificity:**\n   - The comment specifically mentions the \"convergence of the proposed joint learning process\" for RNN and CopyRNN. It identifies a distinct aspect of the paper that may require more in-depth discussion, thus ensuring specificity in the feedback.\n\n2. **Provide Justification:**\n   - The justification provided in the comment lies in the potential difficulty for readers in understanding the mechanism by which stable points in the probabilistic metric space are obtained. The importance of this explanation is linked to the reproducibility of the results.\n\n3. **Suggest Improvements:**\n   - The comment suggests discussing the convergence process more thoroughly, which implies a need for adding or expanding a section in the paper. However, it does not offer explicit suggestions or strategies for how this discussion should be framed or what it should include.\n\n4. **Assess Actionability:**\n   - The actionability of this comment is moderately clear. The authors are directed to expand their discussion on convergence, but they are left to determine the precise manner and content of this expansion without additional guidance.\n\n### Conclusion:\n\nThe comment is indeed mostly helpful as it specifically identifies a potentially crucial weakness concerning the clarity and reproducibility of the paper's results. It offers a clear direction for improvement\u2014discussing the convergence process more\u2014but lacks detailed suggestions on how this discussion should be structured. Given the criteria of specificity, justification, and suggestive nature (though not fully comprehensive in specific improvement strategies), the comment fits best with the label \"4. The comment is mostly helpful.\"\nscore: 4\n\n\nReview Point:\n1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses).\n\n"}]}}
{"custom_id": "75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThese aspects are aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\n\nPlease evaluate the review point based on the aspect description provided above. generate the output as a json object with the following format:\n    \"rationale\": RATIONALE\n    \"score\": SCORE_VALUE,\n    \nhelpfulness:\nAssign a subjective score to reflect the value of the review comment to the authors.\n\nHelpfulness is rated on a scale from 1-5, and we will now provide a definition for each.\n\n1. The comment is not helpful at all\nDefinition: The comment fails to identify any meaningful weaknesses or suggest improvements, leaving the authors with no actionable feedback.\nExamples\nThe core idea of this paper is very simple and straightforward. Though the authors justify that they are the first to do it, I am unsure whether this work might count as a novel enough contribution to the NeurIPS community.\nIt might be good to add more comments.\nIn the experiments, the transfer tasks are too artificial.\n2. The comment is barely helpful\nDefinition: The comment identifies a weakness or improvement area but is vague, lacks clarity, or provides minimal guidance, making it only slightly beneficial for the authors.\nExamples\n I wonder why learn the noisy data and clean data respectively in Algorithm 1, sample mini-batch d~D, \\hat{d} ~ \\hat{D}. Whether they can be fused for learning.\n For many of the datasets tested the improvement over other approaches or even the general adversarial approach is marginal.\n Section 5: It is unclear why the superspreader model is more realistic or more challenging than the uniform corruption.\n3. The comment is somewhat helpful\nDefinition: The comment identifies weaknesses or areas for improvement but is incomplete or lacks depth. While the authors can gain some insights, the feedback does not fully address their needs for improving the draft.\nExamples\nCRUCIAL: The evaluation is unclear. Were agents evaluated on held-out environments from the same task? Or on the N_env training environments? Either way seems fine, but it should be specified!\nWhat are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\nThere is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric.\n4. The comment is mostly helpful\nDefinition: The comment provides clear and actionable feedback on weaknesses and areas for improvement, though it could be expanded or refined to be fully comprehensive and impactful.\nExamples\n It is hard to find the formal definition of the proposed CRS model. It seems to be the equation after line 175, but the authors did not say it explicitly.\nThe reviewer would appreciate some discussion on the possibility of accelerating the proposed algorithm and whether it's optimal rate.\nThe relationship between this work and the previous methods are not exposed. Since the idea of data augmentation in feature space is not new, I expect several papers closely related to this work. However, I cannot see which cited papers are closely related to this work and how it differs from them.\n5. The comment is highly helpful\nDefinition: The comment thoroughly identifies weaknesses and offers detailed, actionable, and constructive suggestions that empower the authors to significantly improve their draft.\nExamples\nI think there\u2019s a problem with EQN 2. I believe you should multiply by 5 instead of 2.\nThe abstract should act like a compact summary of your draft. The way it is not, it needs extra extra summarization. Don\u2019t include a lot of details about your proposed algorithm there.\nThe paper also overstates some claims which should be removed. For example on line 108 the paper says that \"these algorithms often diverge, likely due to the failure of this assumption\". Divergence in fitted Q-iteration could also be due to (for example) compounding errors of poor optimization of neural networks and their uncontrolled extrapolations.\n\n\nFirst output the rationale for the score you have given and then the score value.\n\n\nReview Point: 2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\nrationale: To evaluate the review comment for its helpfulness to the authors according to the criteria provided, we'll break down the comment using the defined aspects:\n\n1. **Identify Specificity**: The comment specifically inquires about the potential impact of feature engineering on performance improvement. It mentions a particular system, Uto et al. (2020), that achieved notable results (QWK of 0.801) using hand-crafted features. This specificity allows the authors to clearly understand the context and focus on a particular aspect of their study.\n\n2. **Provide Justification**: The comment provides justification for its suggestion by citing Uto et al. (2020)\u2019s success with feature engineering. This reference serves to validate the suggestion and encourages the authors to consider feature engineering as a proven method, potentially applicable to their own work.\n\n3. **Suggest Improvements**: It suggests an improvement explicitly by recommending that the authors consider using the same set of hand-crafted features identified in Uto et al. (2020). This gives the authors a tangible course of action to explore in order to enhance their results.\n\n4. **Assess Actionability**: The question posed is highly actionable because it not only encourages exploration of feature engineering but also points towards a specific example that the authors could replicate or adapt. This makes the feedback very direct and practical for the authors to act upon.\n\n**Conclusion**: The comment effectively identifies a precise area of potential improvement, backs it with evidence and context, suggests a concrete method for improvement, and offers a clear, actionable direction for the authors. The feedback is therefore highly helpful, as it meets all the criteria for thoroughness, specificity, and actionability, empowering the authors to significantly enhance their work by employing feature engineering inspired by a successful precedent.\nscore: 5\n\n\nReview Point: - I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.\nrationale: ### Step-by-Step Evaluation\n\n1. **Identify Specificity**: \n   - The comment identifies an issue with the \"probabilistic connection\" in the paper, suggesting it's not drawn well or formally enough. However, specifics about what part of the connection is unclear or lacking formalism are not provided, making the comment vague.\n\n2. **Provide Justification**: \n   - The reviewer states the connection is only motivational and not formal but does not provide examples, evidence, or explicit reasoning to support why it should be more formal, reducing the justification for the comment.\n\n3. **Suggest Improvements**:\n   - The comment suggests two potential improvements: \"cement this connection more formally\" or \"adjust the language to clarify.\" However, these suggestions are broad and lack clear guidance or specific actions, making them less useful.\n\n4. **Assess Actionability**:\n   - While the comment suggests possible actions, the lack of specificity in what constitutes a formal connection or how to adjust language limits the authors' ability to take direct action based on this feedback.\n\n### Conclusion\n\nGiven the analysis above, the comment does somewhat identify a potential weakness in the paper's method of defining the probabilistic connection but fails to offer clear guidance or depth to make the feedback more helpful. The suggestions are vague and lack detail, offering minimal actionable steps for improvement. Therefore, the feedback deserves a label of \"2. The comment is barely helpful,\" as it falls short of providing clear, specific, and actionable improvements.\nscore: 2\n\n\nReview Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\nrationale: To evaluate the given review comment according to the provided definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**:  \n   - The comment points out a \"lack of novelty\" and mentions that adversarial attacks have been previously done on NLP and image-text models. However, it merely restates a point that is already summarized in the related work section of the paper, which indicates a low level of specificity.\n  \n2. **Provide Justification**:  \n   - The justification provided is that previous work exists in this area, but the comment does not delve into why this particular application to video-text models lacks significance or novelty beyond stating that it's only a similar idea.\n\n3. **Suggest Improvements**:  \n   - The comment fails to offer any suggestions or potential areas for enhancement. It states a fact but does not guide the authors on how to address the perceived lack of novelty, such as suggesting innovative angles, comparisons with more diverse datasets, or proposing novel applications or evaluations.\n\n4. **Assess Actionability**:  \n   - The authors cannot take actionable steps based on this comment since it does not provide clear, concrete suggestions or specify what aspects could be strengthened or explored to make the paper stand out.\n\n**Conclusion**:\nBased on the criteria above, the comment lacks specificity, does not provide substantial justification beyond stating a well-known fact, fails to suggest any improvements, and offers no actionable advice. Therefore, it cannot help the authors improve their draft, aligning with the label: 1. The comment is not helpful at all.\nscore: 1\n\n\nReview Point: 11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.\nrationale: 1. **Identify Specificity:**  \n   - The comment explicitly identifies Fig. 5(a) and mentions it \"seems strange,\" indicating that there is something unclear or incorrect about it. This is a specific reference that helps the authors identify where the issue lies. Additionally, it raises specific questions about bit operations and DVS input handling, which are precise areas of inquiry.\n\n2. **Provide Justification:**  \n   - There is a justification implied in the phrases like \"Fig. 5 a seems strange\" and \"how about other bit operations?\" The former implies that there is a perceived issue with Fig. 5(a), and the latter suggests that addressing more bit operations could enrich the paper. However, the justification is implicit rather than fully elaborated.\n\n3. **Suggest Improvements:**  \n   - The comment directly suggests that the authors \"give more explanations\" for Fig. 5(a) and analyze \"the energy consumption as reference[15] did.\u201d These are clear suggestions for improvement, offering the authors concrete directions for enhancing the paper.\n\n4. **Assess Actionability:**  \n   - The comment is actionable because it provides direct questions and requests (\"please give more explanations,\" \"how did you deal with DVS input?\" and suggests analyzing energy consumption). These prompts are clear tasks the authors can undertake to improve their manuscript.\n\n**Conclusion:**  \nThe comment identifies specific issues in the paper and provides actionable feedback. However, it could benefit from greater depth in justification and explanation to be fully comprehensive and impactful. Therefore, the label of \"4. The comment is mostly helpful\" is appropriate as it offers clear direction for revision but is not exhaustive.\nscore: 4\n\n\nReview Point: - What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\nrationale: To evaluate the review comment according to the given definitions and criteria, let's break it down step by step:\n\n1. **Identify Specificity**: The comment mentions the \"Challenge Set\" (CS) and queries its use, specifically whether it is used to augment training material or just for evaluation, and asks about the data split. While it is quite specific in its questioning, it lacks detailed analysis or identification of a particular weakness in the draft.\n\n2. **Provide Justification**: The comment explains a bit about why the topic is unclear to the reviewer, mentioning a potential misunderstanding of the purpose and usage of the Challenge Set. However, it does not delve deeply into the implications or provide a strong justification as to why this information is critical or how it affects the overall quality of the draft.\n\n3. **Suggest Improvements**: The comment indirectly suggests clarifying the use and data split of the Challenge Set within the document. However, it does not provide specific proposals or detailed guidance on how to improve the explanation or structure about the CS, making it less actionable.\n\n4. **Assess Actionability**: While the comment hints at areas that lack clarity and encourages further clarification, the actionability is limited. The authors are left to determine how best to address the query, as the feedback does not outline precise steps for improvement.\n\n**Conclusion**: Given these points, the comment does meet some criteria for helpfulness by identifying a specific area that needs clarification and these queries could potentially lead to improvements in the draft. However, due to the lack of depth, detailed suggestions, or comprehensive justification, it aligns well with the label of \"somewhat helpful.\"\nscore: 3\n\n\nReview Point:\n2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n\n"}]}}

{"custom_id": "1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\nReview Point:\n2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n"}]}}
{"custom_id": "2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\n"}]}}
{"custom_id": "3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n"}]}}
{"custom_id": "4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n"}]}}
{"custom_id": "5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n"}]}}
{"custom_id": "6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\nReview Point:\n3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\n"}]}}
{"custom_id": "8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\nReview Point:\n- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\n"}]}}
{"custom_id": "11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\nReview Point:\n4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n"}]}}
{"custom_id": "15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:\n"}]}}
{"custom_id": "16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n"}]}}
{"custom_id": "17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n"}]}}
{"custom_id": "18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\nReview Point:\n3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n"}]}}
{"custom_id": "19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\nReview Point:\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n"}]}}
{"custom_id": "20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n"}]}}
{"custom_id": "21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n"}]}}
{"custom_id": "22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\nReview Point:\n3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\n"}]}}
{"custom_id": "23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- In figure 5, the y-axis label may use \"Exact Match ratio\" directly.\n"}]}}
{"custom_id": "24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?\n"}]}}
{"custom_id": "25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\nReview Point:\n- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.\n"}]}}
{"custom_id": "26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\nReview Point:\n- In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.) - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure.\n"}]}}
{"custom_id": "27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\nReview Point:\n- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n"}]}}
{"custom_id": "28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\nReview Point:\n1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\n"}]}}
{"custom_id": "29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?\n"}]}}
{"custom_id": "31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\n"}]}}
{"custom_id": "32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n- A number of claims from this paper would benefit from more in-depth analysis.\n"}]}}
{"custom_id": "33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\nReview Point:\n- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.\n"}]}}
{"custom_id": "34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\nReview Point:\n1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.\n"}]}}
{"custom_id": "35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\nReview Point:\n2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n"}]}}
{"custom_id": "37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\nReview Point:\n- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?\n"}]}}
{"custom_id": "38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\nReview Point:\n- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n"}]}}
{"custom_id": "39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\nReview Point:\n1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n"}]}}
{"custom_id": "40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n"}]}}
{"custom_id": "41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2\u00d7) and ResNet50 (4\u00d7). It would be interesting to see the results of proposed InvP with these wider backbones.\n"}]}}
{"custom_id": "42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\nReview Point:\n3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.\n"}]}}
{"custom_id": "43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\nReview Point:\n2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that \"Both CNNs and ViTs seem to benefit similarly from increased model capacity\". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.\n"}]}}
{"custom_id": "44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).\n"}]}}
{"custom_id": "45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\nReview Point:\n- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).\n"}]}}
{"custom_id": "46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.\n"}]}}
{"custom_id": "47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\nReview Point:\n* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.\n"}]}}
{"custom_id": "48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.\n"}]}}
{"custom_id": "49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n1. The novelty is limited. The proposed method is too similar to other attentional modules proposed in previous works [1, 2, 3]. The group attention design seems to be related to ResNeSt [4] but it is not discussed in the paper. Although these works did not evaluate their performance on object detection and instance segmentation, the overall structures between these modules and the one that this paper proposed are pretty similar.\n"}]}}
{"custom_id": "51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\nReview Point:\n- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.\n"}]}}
{"custom_id": "54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\nReview Point:\n- the required implicit call to the Witness oracle is confusing.\n"}]}}
{"custom_id": "55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\nReview Point:\n1) The proposed method cannot handle the headpose. While this paper defers this problem to a future work, a previous work (e.g., Gafni et al. ICCV 2021) is already able to control both facial expression and headpose. Why is it not possible to condition the headpose parameters in the NeRF beyond the facial expression similar to [Gafni et al. ICCV 2021]?\n"}]}}
{"custom_id": "56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n"}]}}
{"custom_id": "57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\nReview Point:\n- The authors have reproduced a well-known result in the literature--left political bias in ChatGPT and in LLMs in general--using the \"coarse\" (their description) methodology of passing a binary stance classifier over ChatGPT's output. The observation that language models reproduce the biases of the corpora on which they're trained has been made at each step of the evolution of these models, from word2vec to BERT to ChatGPT, and so it's unclear why this observation needs to once again be made using the authors \"coarse\" methodology.\n"}]}}
{"custom_id": "58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\nReview Point:\n- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.\n"}]}}
{"custom_id": "59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\nReview Point:\n1. The authors should make clear the distinction of when the proposed method is trained using only weak supervision and when it is semi-supervised trained. For instance, in Table 1, I think the proposed framework row refers to the semi-supervised version of the method, thus the authors should rename the column to \u2018Fully supervised\u2019 from \u2018Supervised\u2019. Maybe a better idea is to specify the data used to train ALL the parts of each model and have two big columns \u2018Mixture training data\u2019 and \u2018Single source data\u2019 which will make it much more prevalent of what is which.\n"}]}}
{"custom_id": "60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\nReview Point:\n- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice.\n"}]}}
{"custom_id": "61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n- \"semantic\" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper.\n"}]}}
{"custom_id": "63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\nReview Point:\n1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses).\n"}]}}
{"custom_id": "65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\nReview Point:\n2. The AAAI15 paper titled \"Spectral Clustering Using Multilinear SVD: Analysis, Approximations and Applications\" by Ghoshdastidar and Dukkipati seems to be a related work missed by the authors. This AAAI15 paper deals with hypergraph data with tensors as well so it should be discussed and compared against to provide a better understanding of the state-of-the-art.\n"}]}}
{"custom_id": "66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\nReview Point:\n1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n"}]}}
{"custom_id": "67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \\gamma^1 and \\gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).\n"}]}}
{"custom_id": "68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.\n"}]}}
{"custom_id": "69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.\n"}]}}
{"custom_id": "70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: \u201csingle target domain with \\textbf{sparse} labels\u201d, \u201c\u2026target distribution p_T(x, y) with label observation\u2026\u201d is mentioned, but the notation for target domain \\tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).\n"}]}}
{"custom_id": "71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n\u2022 I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.\n"}]}}
{"custom_id": "72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\nReview Point:\n1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].\n"}]}}
{"custom_id": "73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\nReview Point:\n2. The authors need to show a graph showing the plot of T vs number of images, and Expectation(T) over the imagenet test set. It is important to understand whether the performance improvement stems solely from the network design to exploit spatial redundancies, or whether the redudancies stem from the nature of ImageNet, ie., large fraction of images can be done with Glance and hence any algorithm with lower resolution will have an unfair advantage. Note, algorithms skipping layers or channels do not enjoy this luxury.\n"}]}}
{"custom_id": "74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand. Fig.\n"}]}}
{"custom_id": "75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\nReview Point:\n2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n"}]}}
{"custom_id": "76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.\n"}]}}
{"custom_id": "77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\nReview Point:\n4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately.\n"}]}}
{"custom_id": "78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n"}]}}
{"custom_id": "80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\nReview Point:\n- The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n"}]}}
{"custom_id": "82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n1. The proposed algorithm DMLCBO is based on double momentum technique. In previous works, e.g., SUSTAIN[1] and MRBO[2], double momentum technique improves the convergence rate to $\\mathcal{\\widetilde O}(\\epsilon^{-3})$ while proposed algorithm only achieves the $\\mathcal{\\widetilde O}(\\epsilon^{-4})$. The authors are encouraged to discuss the reason why DMLCBO does not achieve it and the theoretical technique difference between DMLCBO and above mentioned works.\n"}]}}
{"custom_id": "83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\nReview Point:\n- The modulator is heuristically designed. It is hard to justify if there is a scalability issue that might need tedious hyperparameter tuning for diverse training data.\n"}]}}
{"custom_id": "84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\nReview Point:\n* Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods.\n"}]}}
{"custom_id": "85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly. Here are some examples: (1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is \"trivial\" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)? (2) In line 96, I do not understand the sentence \"our lower hierarchical layers zoom in time\" and the sentence following that.\n"}]}}
{"custom_id": "86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n1) The first concern is the goal of the paper. Indeed, DAS earthquake detectors exists (one of them was cited by the autors, PhaseNet-Das, Zhu et al. 2023, there might be others), and no comparison was made, nor a justification on the benefit of your method against theirs. If the claim is to say that this is a foundation model, and the test on this task is only as a proof of concept, it should be clearer, and then show or justify a future useful application.\n"}]}}
{"custom_id": "87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.\n"}]}}
{"custom_id": "88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n2. Page 2, second paragraph in Related Work, the Walkman algorithm (Mao et al., 2020) is solved by ADMM, with two versions, one is to solve a local optimization problem, the other is to solve a gradient approximation. Therefore, it is not accurate to say that \u201cHowever, these works are all based on the simple SGD for decentralized optimization.\u201d 3. Section 3, first paragraph, in \u201cIt can often have faster convergence and better generalization than the SGD-based Algorithm 1, as will be demonstrated empirically in Section 4.1.\u201d The \u201cit\u201d does not have a clear reference.\n"}]}}
{"custom_id": "89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\nReview Point:\n5.In experimental section, authors only compared with two baselines, there\u2019re several works also focus on the same questions, for example [1,2,3], so it\u2019s suggested to add more experimental to show the effectiveness of proposed method.\n"}]}}
{"custom_id": "90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.\n"}]}}
{"custom_id": "91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\nReview Point:\n- The Related Work section is lacking details. The paragraph on long-context language models should provide a more comprehensive overview of existing methods and their limitations, positioning SSMs appropriately. This includes discussing sparse-attention mechanisms [1, 2], segmentation-based approaches [3, 4, 5], memory-enhanced segmentation strategies [6], and recursive methods [7] for handling very long documents.\n"}]}}
{"custom_id": "92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\nReview Point:\n- The writing should be improved. Some points in the paper is unclear to me.\n"}]}}
{"custom_id": "94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\nReview Point:\n2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems.\n"}]}}
{"custom_id": "95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\nReview Point:\n6) Adding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution.\n"}]}}
{"custom_id": "96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n34 \"to force the neural network to memorize them\" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact \"critical point\" as such in TopoNet [24]. Minor: I find the method section to be a bit wordy, it could be compressed on the essential definitions. There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l.\n"}]}}
{"custom_id": "97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\nReview Point:\n3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works. Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance?\n"}]}}
{"custom_id": "98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\nReview Point:\n4) The analysis from line 128 to 149 is not convincing enough. From the histogram as shown in Fig 3, the GS-P-50 model has smaller class selectivity score, which means GS-P-50 shares more features and ResNet-50 learns more class specific features. And authors hypothesize that additional context may allow the network to reduce its dependency. What is the reason such an observation can indicate GS-P-50 learns better representation? Reference: [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, CVPR, 2018. [2] W. Luo et al., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS, 2016.\n"}]}}
{"custom_id": "99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n"}]}}
{"custom_id": "100", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n1) Some observations and subsequent design decisions might be hardware and software dependent;\n"}]}}
{"custom_id": "101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n1. Limited novelty. The paper seems like a straightforward application of existing literature, specifically the DeCorr [1] that focuses on general deep graph neural networks, in a specific application domain. The contribution of this study is mainly the transposition of DeCorr's insights into graph collaborative filtering, with different datasets and backbones. Although modifications like different penalty coefficients for users and items are also proposed, the whole paper still lack enough insights about what are unique challenges of overcorrelation in recommender systems.\n"}]}}
{"custom_id": "102", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n"}]}}
{"custom_id": "103", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n1. From the methodology aspect, the novelty of paper appears to be rather limited. The ENCODE part is already proposed in [10] and the incremental contribution lies in the decomposition part which just factorizes the M_v into factor D and slices Phi_v.\n"}]}}
{"custom_id": "104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\nReview Point:\n--- W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.\n"}]}}
{"custom_id": "105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\nReview Point:\n2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty.\n"}]}}
{"custom_id": "106", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking.\n"}]}}
{"custom_id": "107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n- line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n"}]}}
{"custom_id": "108", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\nReview Point:\n- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.\n"}]}}
{"custom_id": "109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\nReview Point:\n- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2\u0080\u009cattribute\u00e2\u0080\u009d. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2\u0080\u009cLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2\u0080\u009d. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n"}]}}
{"custom_id": "110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n- I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\n"}]}}
{"custom_id": "111", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\nReview Point:\n1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n"}]}}
{"custom_id": "112", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\nReview Point:\n- The novelty of using PCA to reduce interaction count seems incremental and the significance of the paper results is unclear to me. Using PCA to reduce the interaction count seems intuitive, as PCA aims to retain the maximum information in the data with the reduced dimensionality chosen, assuming certain assumptions are met. How well are the assumptions met? [1] Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert M\u00fcller, and Pan Kessel. \"Towards robust explanations for deep neural networks.\" Pattern Recognition 121 (2022): 108194.\n"}]}}
{"custom_id": "113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\nReview Point:\n3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.\n"}]}}
{"custom_id": "114", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\nReview Point:\n3. I think figure 6C is a bit awkward, it implies negative rates, which is not the case, I would suggest using a second y-axis or another visualization which is more physically accurate.\n"}]}}
{"custom_id": "115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\nReview Point:\n1) It seems that there is still room to improve the complexity of Algorithm 2;\n"}]}}
{"custom_id": "116", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- I find this statement in the supplemental section D.4 questionable: \"Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting\". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states?\n"}]}}
{"custom_id": "117", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\nReview Point:\n1.The main part can be more concise (especially for the introduction part)and including empirical results.\n"}]}}
{"custom_id": "118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n2. The technical contribution is unclear. Most of the analysis are quite standard.\n"}]}}
{"custom_id": "119", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes:\n"}]}}
{"custom_id": "120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n2. It will be good to see some failure cases and related discussion.\n"}]}}
{"custom_id": "121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n4. In Section 4.1, \\epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \\epsilon when (11) is discussed.\n"}]}}
{"custom_id": "122", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\nReview Point:\n- In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor:\n"}]}}
{"custom_id": "123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n3. This author should add more description about the contribution of this paper.\n"}]}}
{"custom_id": "124", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: The proposed algorithm's efficiency appears promising at first glance, but without deeper performance benchmarks across diverse datasets, its generalizability is debatable.\n    rationale: The claim about the algorithm\u2019s generalizability is questioned, citing a lack of performance benchmarks. The reasoning provided is insufficient, as no specific datasets or comparisons are mentioned, making it barely verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\nReview Point:\n- it would seem to me that in section 4, \"X\" should be a multiset (and [\\cal X]**n the set of multisets of size n) instead of a set, since in order the histogram to honestly represent a graph that has repeated vertex or edge labels, you need to include the multiplicities of the labels in the graph as well.\n"}]}}
{"custom_id": "125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\nReview Point:\n2. The authors' derivation falls into classical learning theory-based bounds, which, to the best of my knowledge, does not yield realistic bounds, unless Bayesian considerations are taken into account (e.g. Bayesian-PAC based bounds).\n"}]}}
{"custom_id": "126", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\nReview Point:\n- The authors discuss how certain methods are significantly different from others, yet no significance testing is done to support these claims. For example, in line 486 the authors write \"The conversational ability of ChatGPT and GPT-4 significantly boosts translation quality and discourse awareness\" -- the difference between zh->en ChatGPT (17.4 d-BLEU; 2.8/2.9 humeval) and GPT-4 (18.8 d-BLEU; 2.6/2.8 humeval) and the scores for FeedME-2 (16.1 d-BLEU; 2.2/2.3 humeval) and PPO (17.2 d-BLEU; 2.6/2.7 humeval) is minimal and it's hard to say whether it is significant without proper testing (including checking the distribution and accounting for multiple comparisons).\n"}]}}
{"custom_id": "127", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\nReview Point:\n1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.\n"}]}}
{"custom_id": "128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\nReview Point:\n5. In my opinion, the contribution of this paper appears somewhat limited, and the proposed model seems incremental in its approach.\n"}]}}
{"custom_id": "129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n    rationale: The review point makes a claim that the bounded noise assumption is somewhat restrictive in the context of stochastic optimization literature. This claim is well-supported by references to specific works that have attempted to extend these noise conditions, namely the papers by Khaled and Richt\u00e1rik (TMLR 2023) and Gower et al. (AISTATS 2021). These references provide robust evidence for the claim, making it fully verifiable. The reviewer effectively uses external references to substantiate the claim, aligning with the criteria for a score of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\nReview Point:\n\u2022 Supervised pretraining based on the prediction of homo-lumo gap may lead to negative transfer. For example, on QM9 in downstream experiments, Transformer-M performs poorly on most tasks other than homo, lumo, and gap. This may be contradictory to the description \"general-purpose neural network model\" claimed in this paper.\n"}]}}
{"custom_id": "130", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\nReview Point:\n2. Experimental Evaluation 2.1. Ablations 2.1.1. The paper claims that \u00e2\u0080\u009cAnother distinction of our approach is the \u00e2\u0080\u009cpicking\u00e2\u0080\u009d step \u00e2\u0080\u009c. However, this aspect is not ablated. 2.2. Experiments on CIFAR. The comparison on CIFAR is not convincing 2.2.1. The continual learning literature has extensive experiments on this dataset and the paper only compares to one approach (DEN). 2.2.2. It is unclear if DEN is correctly used/evaluated. It would have been more convincing if the authors used the same setup as in the DEN paper to make sure the comparison is fair/correct.\n"}]}}
{"custom_id": "131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\nReview Point:\n- The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph?\n"}]}}
{"custom_id": "132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment asserts that \"these baselines seem weak,\" and it is fully verifiable through the provision of specific comparative examples. The reviewer effectively substantiates the claim by contrasting the current work's results with those of a stronger model, demonstrating that the baselines in question underperform. This explicit evidence, illustrated with exact numerical data and a relevant external reference, makes the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point:    - The experimental setup lacks adequate control studies, which might undermine result reliability. Although some scenarios are covered, the absence of controls for environmental variables is not deeply explored. Referencing standards from similar studies could have helped solidify the experimental design.\n    rationale:      This review comments on the absence of control studies, indicating potential weaknesses in the reliability of results. It's mostly verifiable because it logically references the need for controls in experimental design. However, it lacks specific standards or studies as examples, which would strengthen the point. Addressing this would fully support the claim, aligning with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment states that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it does not provide any supporting reasoning or examples to justify this opinion. As a result, the comment is unverifiable, since the lack of evidence and explanation makes it difficult for the authors to understand or address the concern. Therefore, it falls into Category 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\nReview Point:\n2. I have some doubts about the definations in Table1. What's the different between anchor-based regression and the regression in RepPoints? in RetinaNet, there is also only a one-shot regression. And in ATSS, this literature has proved that the regression methods do not influence a lot. The method that directly regresses [w, h] to the center point is good enough. While RepPoints regresses distance to the location of feature maps. I think there is no obvious difference between the two methods. I hope the authors can clarify this problem. If not, the motivations here is not solid enough.\n"}]}}
{"custom_id": "133", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\nReview Point:\n- When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex.\n"}]}}
{"custom_id": "135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers,\" but it does not give any examples of these papers, making the comment not verifiable at all.\n    score: 1\n    \n\nReview Point:\n3. It could be convincing to discuss case studies and error studies to highlight the effectiveness of each proposed component. For example, this paper mentions that the Element-level Graph Pre-training abandons the strategy of capturing the complex structure but focuses directly on the core elements. However, without case study, it is less convincing to figure it out. An example of case study can be found in \u201cGraph pre-training for AMR parsing and generation\u201d.\n"}]}}
{"custom_id": "139", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n    rationale: The comment suggests an alternative approach to taking the embedding of the first subword token by mentioning that it is common to average over subword representations, as done by Hewitt and Manning (2019). This claim is supported by a specific reference to an external work, providing a clear and precise reasoning for the suggestion. The inclusion of a relevant reference makes the claim fully verifiable. Therefore, the comment is classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" concerning the adversarial attacks by perturbing text, noting that similar approaches have been widely applied to NLP and image-text models, which is also summarized in the related work of this paper. This indicates mostly verifiable reasoning as the reviewer provides a logical basis for their opinion by referencing aspects covered in the paper's related work section. However, the justification could be further enhanced with more explicit examples or references, allowing for a deeper understanding of the claim. Hence, the label 4: Mostly Verifiable is appropriate.\n    score: 4\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\nReview Point:\n- As I said above, I found the writing / presentation a bit jumbled at times.\n"}]}}
{"custom_id": "141", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - While the introduction mentions the novelty of the approach, it fails to adequately explain how it differs from previous methods in terms of computational efficiency, which is crucial for real-time applications. Comparative statistics or case studies would help clarify this point.\n    rationale: The comment claims that the introduction fails to explain differences in computational efficiency with previous methods, suggesting that comparative statistics or case studies would help. This provides an initial rationale but lacks specific references or detailed comparisons, making it somewhat verifiable. The authors need further details to fully grasp and address the comment.\n    score: 3\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The review comment claims that the paper \"only compares with some weak baselines\" and provides specific comparative data from Voita et al. (2019) to support this assertion. The reviewer presents numerical results for different linguistic evaluations, such as deixis and lexical cohesion, achieved by the CADec model, contrasting them with the results of the work under review. This detailed comparison provides a robust foundation for the claim, making it fully verifiable. The use of specific data and references to external work strengthens the justification, allowing the authors to clearly understand the basis of the critique.\n    score: 5\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: The research's claim of innovation in its theoretical framework hinges on untested assumptions. Additional testing of these assumptions could strengthen the argument.\n    rationale:  The claim of innovation is questioned on the grounds of untested assumptions, yet no specific assumptions are detailed, nor is there any evidence provided. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: *  The discussion in Section 5 lacks depth, particularly regarding the impact of hyperparameter choices on model performance. Adding examples where changes in hyperparameter settings drastically alter results could improve this section.\n    rationale: The claim is that \"the discussion in Section 5 lacks depth\" due to insufficient exploration of hyperparameter impacts. Although it suggests providing examples where hyperparameter changes affect results, the comment does not specify which hyperparameters or provide instances showing this impact. Hence, it's somewhat verifiable as the authors would need to further develop the rationale themselves.\n    score: 3\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment makes a claim that \"Lines 102-106 are misleading\" due to the use of \"'such distribution'.\" While the reviewer begins to justify this by mentioning that it cannot refer to the prior discussion, the explanation is not sufficient for a full verification of the claim. More detailed reasoning or examples are needed to help the authors grasp the issue clearly, landing the comment at a \"3: Somewhat Verifiable.\"\n    score: 3\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\nReview Point:\n- The authors write \u201cIn the base IOI circuit, the Induction, Duplicate Token, and Previous Token heads primarily attend to the S2 token\u201d this is incorrect according to Section 3 of Wang et al., 2023. These heads are _active_ at the S2 token, but do not primarily attend to it.\n"}]}}
{"custom_id": "142", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point:    - The paper's section on user impact could be enhanced by considering diverse demographics. While there\u2019s mention of disparities, integrating specific demographic study results (e.g., D and E) could provide a more rounded discussion on societal effects.\n    rationale:      The comment suggests that consideration of diverse demographics is insufficiently thorough, proposing specific demographic studies for deeper insight. This claim is mostly verifiable since it\u2019s based on logical reasoning, yet lacks explicit examples from these suggested studies. Including such examples would further substantiate the point, meriting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment makes a claim by stating that \"the novelty is limited,\" suggesting that the use of a linear model for interpreting deep neural networks is not new. However, this justification is vague and lacks specificity or references to prior work, leaving the claim only borderline verifiable. \n    score: 2\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n    rationale: The comment claims that some state-of-the-art references are missing in the face recognition experiment, specifically mentioning Baidu's work. It supports this claim by providing a specific reference and comparing the results achieved by Baidu's work with those in the paper, noting that Baidu's VRF achieves better performance on LFW than the results in Table 3. This detailed comparison and reference to an external work provide a robust justification for the claim, making it fully verifiable. Therefore, the comment is rated as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n    rationale: The review comment challenges the claim made in the paper that \"over-parametrization invariably overfits the data and results in worse performance.\" The reviewer provides a counter-argument by stating that over-parameterization is often beneficial in supervised learning of deep neural networks and supports this with a reference to theoretical work that demonstrates these benefits. This provides a clear and precise reasoning, along with a reference, making the claim fully verifiable. Therefore, the comment is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: 2. The introduction of the paper provides a broad overview of linguistic theory but lacks concrete examples of its application in real-world scenarios. As a result, the section lacks depth. Including case studies could enhance the readers\u2019 understanding and application.\n    rationale: The comment claims that the introduction could be made more impactful by including case studies, asserting that the current content is too theoretical. The justification leans on the observation that a broader overview is given without any real-world applications, which are necessary to provide depth. However, the specific examples of the case studies are not mentioned, thus making it somewhat verifiable. \n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n    rationale: The review point contains a claim that \"the paper was extremely hard to follow,\" specifically regarding the experimental procedures and evaluations. However, the comment lacks any supporting evidence or detailed explanation to substantiate this claim. The reviewer does not provide specific examples or reasons why the paper is difficult to follow, making it challenging for the authors to understand and address the issue. Therefore, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: - the required implicit call to the Witness oracle is confusing.\n    rationale: The review comment makes a claim that \"the required implicit call to the Witness oracle is confusing.\" However, it does not provide any supporting evidence, reasoning, or examples to justify why it is confusing. Without additional context or explanation, the authors may find it challenging to understand and address the issue. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The choice to exclude variable Y from the regression model is questionable. Past studies often highlight its importance.\n    rationale: The comment questions the exclusion of variable Y but supports this claim only by mentioning past studies without citing specific ones. This makes the assertion barely verifiable, as it relies on a general statement without concrete references or detailed justification.\n    score: 2\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n- L15: Again, too vague, certain RNNs work well for certain natural language reasoning tasks. See for instance the literature on natural language inference and the leaderboard at https://nlp.stanford.edu/projects/snli/ - L16-18: The reinforcement learning / agent analogy seems a bit out-of-place here. I think you generally point to generalization capabilities which I believe are better illustrated by the examples you give later in the paper (from lines 229 to 253).\n"}]}}
{"custom_id": "143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n    rationale: The comment claims that \"some conclusions are not convincing,\" specifically addressing the paper's contention about continuous learning with unlabeled data. The reviewer provides a rationale by suggesting that the results might stem from limited exploration of combination methods and supports this with references to feature-replay methods in rehearsal-free continual learning, citing specific works ([R1], [R2], and [R3]) that demonstrate the potential of these methods. This provides a solid basis for the claim, with explicit references and examples that substantiate the reviewer's point. Therefore, the comment is categorized as 5: Fully Verifiable, as it offers thorough evidence and reasoning to support the claim.\n    score: 5\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment suggests that \"the main part can be more concise,\" particularly in the introduction, and implies that empirical results should be included. While it identifies areas for potential improvement, it lacks a rationale or specific examples to justify why conciseness or the inclusion of empirical results is necessary. This makes the claims barely verifiable as there is insufficient support, leaving the authors struggling to understand the underlying reasons for the suggestions. Consequently, the feedback is deemed 2: Borderline Verifiable.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point:    - The experimental setup presented lacks appropriate control studies that could enhance the reliability of the results. While the authors do provide comprehensive data across various scenarios, the absence of controls for environmental variables, as suggested by standards featuring in the works of A and B, slightly undermines confidence in the findings. Including such controls could have bolstered the strength of the conclusions drawn.\n    rationale:      The comment critiques the experimental setup for not having adequate control studies, which affects the reliability of results. The reviewer emphasizes the importance of control studies by referencing standards mentioned in works A and B, thereby making it a mostly verifiable claim. The logical reasoning centers around the need for controls to validate findings, a valid critique supported by common experimental design standards. However, the claim\u2019s verifiability could be improved with more detailed examples or references to specific standards or studies that detail how controls were effectively utilized in similar scenarios. This would provide an even clearer pathway for strengthening the experimental design. Consequently, this leads the comment to be rated as 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point:    - The proposed model's efficiency claim appears somewhat overstated. While the authors argue that their approach reduces computational costs, specific performance metrics or comparative analysis with existing models like X and Y are absent. It might be beneficial to conduct such comparisons to better substantiate their claim.\n    rationale:      The review comment suggests that the efficiency claim is exaggerated, due to the lack of specific performance metrics or comparisons with existing models X and Y. This forms a mostly verifiable claim by questioning the statement based on logical reasoning but leaves room for improvement as it lacks direct benchmarks or concrete data. Providing explicit comparisons would solidify the claim further, hence it aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The review point makes a claim that the paper \"does not dig into the theory profs and show the convergence properties of the proposed algorithm.\" However, it does not provide any supporting evidence, reasoning, or references to justify this claim. The comment lacks any detailed explanation or examples that would help the authors understand why this is an issue or how it could be addressed. As a result, the claim is unverifiable.\n    score: 1\n    \n\n    Review Point:    - The argument concerning the model's robustness to noise could be more comprehensive. While the authors describe some noise-resistant capabilities, previous models described in the literature (such as paper C) have demonstrated greater resilience. Explicitly benchmarking the model against these would validate the robustness claim with more clarity.\n    rationale:      The review calls for a more comprehensive argument regarding robustness to noise, indicating that existing literature (paper C) demonstrates greater resilience. This is a mostly verifiable claim, as it critiques the presented noise-resistance capabilities and urges a comparison against established models for clarity. The logical reasoning is sound as it encourages the use of benchmarks to substantiate claims, a recognized practice. However, the claim\u2019s verifiability is somewhat limited as it stops short of presenting specific data or detailed examples from the mentioned literature. By including such precise examples or results, the clarity and strength of the claim could be markedly improved. Thus, this aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance, which is meant to contradict the authors' claims. However, the explanation provided is minimal, stating only that the performance is \"clearly isn't uniform.\" While this suggests an issue, the reasoning lacks sufficient detail and examples, making it hard for the authors to fully understand the justification and address it effectively. As a result, the claim is only borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment includes a claim that the paper's strength could be improved by incorporating more baselines from related work. The justification for this claim is based on the observation that the current comparison, particularly with the MST baseline, lacks sufficient detail and focuses mainly on word sense disambiguation rather than on sememe-based models. However, the comment lacks specific examples or references to potential baselines from related work, resulting in it being somewhat verifiable. This incomplete articulation requires significant effort from the authors to follow and address the suggestion, thus justifying the label 3: Somewhat Verifiable.\n    score: 3\n    \n\nReview Point:\n104 it is said that every kernel can be described by a feature space parameterized by a neural network, but this is trivially not true. For instance, for RBF kernels, the RKHS is famously infinite-dimensional, such that one would need an NN with infinite width to represent it. So at most, NNs can represent finite-dimensional RKHSs in practice. This limitation should be made more clear. l.\n"}]}}
{"custom_id": "144", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_verification:\n\nObjective\nThe primary goal of this evaluation is to maximize the utility of review comments for authors. The purpose of a review is to help and guide authors in improving their drafts. Keep this in mind while assessing review points. When encountering a borderline case, ask: \u201cWill this review point help authors improve their draft?\u201d There is no correlation between the aspect score and the length of the review point.\n\nEvaluation Aspect: Verifiability\n\nDefinition: This aspect measures how well a claim in the text is verified. Assess how well the reviewer justifies or proves the claim by providing logical reasoning, using common sense, or referencing external sources. Justifications or validations can appear before or after the claim. Claims may be explicitly stated or inferred.\n\nVerification Methods:\n\nLogical reasoning supports the claim.\n\nCommon sense knowledge in the field verifies the claim (e.g., referencing established practices or standards).\n\nExternal references substantiate the claim.\n\nScoring Criteria (1-5 Scale)\n\n1 - Unverifiable\n\nDefinition: The comment contains a claim without any supporting evidence or justification.\nExamples:\n\n\"The results fall behind previous work, and the reasons for this should be investigated.\"\n\n\"For many datasets tested, the improvement over other approaches or even the general adversarial approach is marginal.\"\n\n\"While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.\"\n\n2 - Borderline Verifiable\n\nDefinition: The comment provides some support for its claim, but the justification is vague, insufficient, or not fully articulated. Authors may struggle to follow the reasoning.\nExamples:\n\n\"This method shouldn\u2019t achieve good results. If I remember correctly, I have read a paper that tried to do the same thing, but it didn\u2019t work for them.\"\n\n\"It is also unclear whether this momentum term could be a confounding factor in the comparison between PAL and SLS, as the vanilla version of SLS is just a stochastic line search applied to SGD without momentum.\"\n\n\"In the experiments, the transfer tasks are too artificial. At the pretraining stage, we train the models with examples from two classes ('bird' vs. 'frog') for CIFAR-10 and four classes (0, 1, 2, and 3) for MNIST.\"\n\n3 - Somewhat Verifiable\n\nDefinition: The comment provides support for its claim, but key elements are missing, such as specific examples, detailed explanations, or supporting references. Authors must make a significant effort to follow the justification.\nExamples:\n\n\"The evaluative framework appears somewhat limited in scope, with considerations restricted to merely three Question-Answering tasks and two language models.\"\n\n\"The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data.\"\n\n\"The approximation error is defined as the gap between the objective values, which is ambiguous unless one has seen the values in the table.\"\n\n4 - Mostly Verifiable\n\nDefinition: The comment\u2019s claim is sufficiently supported but has minor gaps. The reviewer could provide a more detailed explanation or reference.\nExamples:\n\n\"The statistical analysis appears incorrect because the p-values reported for the t-tests do not align with standard thresholds for significance.\"\n\n\"The two used datasets are very related, where the input sequence is cocktail party speech, with one outputting the audio of each stream and the other producing the ASR output of each stream.\"\n\n\"As the paper states in the intro, double Q-learning was developed to address the overestimation problem of Q-learning. However, this cannot really be seen directly from the results in the paper. The explanation given suggests that double Q-learning resolves the overestimation problem by achieving a fast convergence rate.\"\n\n5 - Fully Verifiable\n\nDefinition: The claim is thoroughly supported by explicit, sufficient, and robust evidence. This can be achieved through:\n\nClear and precise reasoning or explanation.\n\nSpecific and relevant references to external works or data.\n\nLogical and unassailable common-sense arguments.\nExamples:\n\n\"The landscape results in parameter space look very surprising because they have no assumptions on the generator and discriminator architecture except for sufficient representation. This is surprising because such global optimization results for neural networks usually require strong assumptions on the architecture.\"\n\n\"The first weakness of this work is that the wish list presented in the Introduction is broader than the actual techniques proposed. The key difference of this work lies in the dynamic prior, while previous work such as references [21] and [27] had already addressed the three properties mentioned.\"\n\n\"The paper\u2019s main idea of mixing transfer-based and query-based attacks is not novel. Several papers [9, 19] have already explored this concept. This work simply combines the best transfer-based attack (TIMI) and one of the best L2 query-based attacks (SimBA) to create SimBA++, which is the main gain over previous approaches.\"\n\nInstructions:\n\nUse the scoring scale to evaluate the verifiability of each review point.\n\nFocus on how well-supported the claims are rather than the length of the comment.\n\nConsider whether the review point meaningfully helps the authors improve their draft.\n\nGenerate a rationale and use it to output the score. \n{examples}\n\nReview Point:\n{review_point}\n\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs,\" but it does not include any verification for this claim, making the comment not verifiable at all.\n    score: 1\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment raises a claim that \"the evaluation on the word analogy task seems a bit unfair,\" supported by the observation that \"the semantic relations are explicitly encoded by the sememes.\" This provides an initial rationale but lacks comprehensive detail or evidence, such as specific examples or external references, that would fully explain why this encoding makes the evaluation unfair. Therefore, the comment is categorized as somewhat verifiable, as additional explanation is needed to thoroughly substantiate the claim.\n    score: 3\n    \n\n    Review Point:    - The chosen metric for evaluating model performance might not be optimal. It is based on a common baseline, yet literature (e.g., F and G) offers alternative metrics that could provide finer granularity. More detailed analysis using these alternatives would bolster the justification.\n    rationale:      This critique addresses the selection of an evaluation metric, suggesting alternatives as proposed in literature F and G. It forms a mostly verifiable claim with logical reasoning but lacks a detailed analysis using these alternatives. Such detailed comparisons would add strength to the argument, justifying a 4: Mostly Verifiable label.\n    score: 4\n    \n\n    Review Point: The study claims significant advancements in predictive accuracy, yet without a detailed comparison to established baselines, it's challenging to gauge the true impact of these improvements.\n    rationale: The comment suggests that the claim of improved predictive accuracy is inadequately supported due to the absence of comparisons to existing baselines. The explanation provided is limited, lacking specific examples or comparisons.\n    score: 2\n    \n\n    Review Point: 1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n    rationale: The comment claims that the spurious features discussed in Sections 3.1 and 3.2 are similar to backdoor triggers, providing specific examples from existing literature to support this claim. The reviewer references Chen et al. (2017) and Gu et al. (2019), who use random noise patterns and simple patterns as triggers, respectively. This comparison is well-supported by external references and logical reasoning, making the claim thoroughly verifiable. The explicit evidence and relevant references provided make the comment fully verifiable, justifying the label of 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n    rationale: The review point makes a claim that the work is a \"fairly straightforward extension of existing retrofitting work\" and suggests including additional baselines, such as character embeddings. However, the comment does not provide any specific examples, references, or detailed reasoning to support the claim that the work is merely an extension of existing work. The suggestion for additional baselines is also not justified with reasoning or evidence. As a result, the claim is not verifiable due to the lack of supporting information, making it difficult for the authors to understand the basis of the suggestion. Therefore, the feedback is categorized as 1: Unverifiable.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The review comment makes a claim that \"better metadata embedding options are available\" for improving performance in zero-shot learning on the CUB dataset. This claim is fully verified as the reviewer reinforces it by referencing Table 1 of the paper \"Learning Deep Representations of Fine-Grained Visual Descriptions\" by Reed et al., presented at CVPR 2016. This specific reference provides a robust foundation for the claim, rendering the comment fully verifiable with respect to the standards of evidence and verification.\n    score: 5\n    \n\n    Review Point: - Your paper does not sufficiently discuss the algorithm\u2019s limitations regarding scalability. While this is briefly touched upon, a more thorough analysis with references to existing challenges would better highlight the limitations in managing extensive datasets.\n    rationale: The comment critiques the paper for not adequately discussing scalability limitations, hinting at the need for reference to existing challenges. This is a mostly verifiable claim, as the reasoning is based on logical expectations for detailed analysis in scalability, but lacks explicit examples or references. Including such references or examples would enhance the justification, warranting a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The review point suggests that \"a number of claims from this paper would benefit from more in-depth analysis.\" This is a subjective opinion indicating that the paper's claims are insufficiently analyzed. However, the comment does not provide any specific examples or reasoning to support this claim, making it difficult for the authors to understand which claims need more analysis or why. As a result, the comment lacks verifiability and is rated as \"1: Unverifiable.\"\n    score: 1\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature.\" This claim is fully verified by providing specific details about the discrepancy between the section description and the equation, particularly pointing out the absence of the term for r^q, which is the question representation. This detailed explanation attends to the comment's verifiability, thus making it fully verifiable. Therefore, the label assigned to this feedback is appropriately classified as 5: Fully Verifiable.\n    score: 5\n    \n\n    Review Point: 4. The experimental setup described from lines 150 to 180 could benefit from a more thorough discussion of potential confounding variables. While it is noted that some control measures are in place, the lack of detail regarding these measures and their effectiveness leaves room for improvement. Consider referencing similar studies where confounding variables were successfully managed to strengthen the argument.\n    rationale: The claim suggests that the experimental setup lacks a detailed discussion of confounding variables, which could impact the results' validity. Although it mentions the existence of control measures, the comment doesn't provide specific details or examples, making it somewhat verifiable. Without concrete references or comparisons to similar well-controlled studies, the authors might struggle to fully understand and address the issue, hence the label 3: Somewhat Verifiable.\n    score: 3\n    \n\n    Review Point: The experimental setup is positioned as novel, but given the scant explanation of how it differs fundamentally from prior work, the extent of its novelty is questionable.\n    rationale:  The reviewer challenges the claim of novelty without detailing how prior work is similar or different. This lack of explicit differentiation makes it difficult for authors to address the critique or demonstrate the uniqueness of their setup effectively. This lack of elaboration renders the claim borderline verifiable.\n    score: 2\n    \n\n    Review Point: - The writing style in the discussion section could hinder readability, potentially obscuring key insights. Consider simplifying complex sentences.\n    rationale: The claim that the writing style affects readability suggests simplification. The justification is vague, not specifying which parts or how readability is compromised. This makes the suggestion borderline verifiable, offering little concrete evidence or examples.\n    score: 2\n    \n\n    Review Point: - The finding that method X achieves better performance might be skewed by the choice of baselines, which do not reflect the state-of-the-art methods. Including a wider range of contemporary baselines could provide a more accurate comparison.\n    rationale: The claim concerns potential skewing of results due to outdated baselines, suggesting the inclusion of state-of-the-art methods. This provides partial verification by highlighting the issue but lacks specific examples of baselines considered state-of-the-art. Further detail would help substantiate the claim fully, classifying it as somewhat verifiable.\n    score: 3\n    \n\n    Review Point: The conclusion drawn from the data seems overly optimistic, particularly given the limitations of the dataset. Some additional context or data variability might be worth considering to reinforce the findings.\n    rationale: The comment posits that the optimism of the conclusion may not be warranted due to dataset limitations. However, the justification is vague and lacks specific examples or detailed reasoning, making it borderline verifiable.\n    score: 2\n    \n\n    Review Point: 5. The results presented in Table 3 appear optimistic because the chosen metrics favor the proposed algorithm but do not adequately account for variance in more complex datasets. Including metrics from a broader range of datasets would enhance the validity of these results.\n    rationale: The claim is that the results seem \"optimistic\" due to the favoring of certain metrics. This is partially justified by stating that these metrics may not account for variance in complex datasets. However, specific examples of such datasets or metrics that could be included are missing. This makes the claim somewhat verifiable, as it would require more detailed evidence or examples to fully validate the argument.\n    score: 3\n    \n\n    Review Point: 5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n    rationale: The comment expresses a subjective opinion that the paper's empirical results and analyses are difficult to synthesize into a coherent understanding of the research question and hypothesis. This is a claim that requires verification. However, the comment does not provide specific examples or detailed reasoning to support this claim, making it challenging for the authors to understand the basis of the reviewer's difficulty. The lack of detailed justification or examples results in the comment being classified as \"1: Unverifiable,\" as it does not provide sufficient evidence or reasoning to support the claim.\n    score: 1\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The review comment makes a verifiable claim that the \"dataset itself might have a lot of noise,\" citing the example that the 'pristine' set of tweets may include misinformation and out-of-context images. This reasoning provides a logical basis for the claim, making it mostly verifiable. However, while the argument is logically sound, it lacks detailed evidence or references, leaving minor gaps that could be addressed. Therefore, the evaluation is consistent with the label: 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and suggests that the correct approach involves using a bidirectional encoder to create a set of vectors. The comment is mostly verifiable as it provides some justification by referencing Figure 2, indicating an inconsistency with the statement in question. However, while the comment describes the perceived issue and proposes a correction, the verification could be strengthened with additional explanation or external references to fully substantiate the claim. Therefore, the comment aligns with a label of 4: Mostly Verifiable.\n    score: 4\n    \n\n    Review Point: 1. The paragraph suggesting that parameter tuning enhances performance is underdeveloped without specifying which parameters make significant impacts. An expanded analysis with examples of impactful parameters would strengthen this argument.\n    rationale: The claim about parameter tuning's efficacy is mentioned but not in detail. While it hints at the need for specific impactful parameters, the lack of example parameters or analysis leads to a somewhat verifiable comment. Additional information is necessary for the authors to fully address this critique.\n    score: 3\n    \n\n    Review Point: 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n    rationale: The comment contains a claim that dedicating a whole section of the paper plus experimental results to fleshing out the assumptions is excessive. However, the reviewer does not provide specific reasoning or evidence to support why this allocation of space is considered excessive or how it impacts the paper's overall quality. The lack of detailed justification or examples makes it difficult for the authors to understand the basis of the claim and address it effectively. Therefore, the comment is considered unverifiable.\n    score: 1\n    \n\nReview Point:\n* The proposed method is not well-positioned in literature. It's worth pointing out that the key idea of representing the marginal score as the expectation of scores of distributions conditioned on inputs is actually quite well-known. It has been used, for example, to develop the original denoising score matching objective [1]. It is also used in the literature as \"score-interpolation\" [2]. I just named a few but I would recommend the authors to do a thorough literature review as I believe this property is used in many more works.\n"}]}}

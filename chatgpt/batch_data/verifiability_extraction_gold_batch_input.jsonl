{"custom_id": "1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\nReview Point:\n2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n"}]}}
{"custom_id": "2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).\n"}]}}
{"custom_id": "3", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\nReview Point:\n- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n"}]}}
{"custom_id": "4", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n"}]}}
{"custom_id": "5", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n"}]}}
{"custom_id": "6", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\nReview Point:\n3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\n"}]}}
{"custom_id": "7", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n"}]}}
{"custom_id": "8", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\nReview Point:\n- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)\u2019s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\n"}]}}
{"custom_id": "9", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n"}]}}
{"custom_id": "10", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers? I thank the authors for their response.\n"}]}}
{"custom_id": "11", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n"}]}}
{"custom_id": "12", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\nReview Point:\n2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.\n"}]}}
{"custom_id": "13", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\nReview Point:\n1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\n"}]}}
{"custom_id": "14", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n781 \"both tasks\": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.\n"}]}}
{"custom_id": "15", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\nReview Point:\n- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:\n"}]}}
{"custom_id": "16", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.\n"}]}}
{"custom_id": "17", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n"}]}}
{"custom_id": "18", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\nReview Point:\n3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n"}]}}
{"custom_id": "19", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\nReview Point:\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.\n"}]}}
{"custom_id": "20", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\nReview Point:\n- The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n"}]}}
{"custom_id": "21", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -\n"}]}}
{"custom_id": "22", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\nReview Point:\n3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: \u201cThe may be attributed\u2026\u201d -> \u201cThis may be attributed\u2026\n"}]}}
{"custom_id": "23", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n- In figure 5, the y-axis label may use \"Exact Match ratio\" directly.\n"}]}}
{"custom_id": "24", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?\n"}]}}
{"custom_id": "25", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.\n"}]}}
{"custom_id": "26", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n- In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.) - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure.\n"}]}}
{"custom_id": "27", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n"}]}}
{"custom_id": "28", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.\n"}]}}
{"custom_id": "29", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?\n"}]}}
{"custom_id": "30", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021)\n"}]}}
{"custom_id": "31", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\nReview Point:\n3. It will be nice to see some examples of the system on actual texts (vs. other components & models).\n"}]}}
{"custom_id": "32", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\nReview Point:\n- A number of claims from this paper would benefit from more in-depth analysis.\n"}]}}
{"custom_id": "33", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.\n"}]}}
{"custom_id": "34", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.\n"}]}}
{"custom_id": "35", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n"}]}}
{"custom_id": "36", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?\n"}]}}
{"custom_id": "37", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?\n"}]}}
{"custom_id": "38", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n"}]}}
{"custom_id": "39", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\nReview Point:\n1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n"}]}}
{"custom_id": "40", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?\n"}]}}
{"custom_id": "41", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2\u00d7) and ResNet50 (4\u00d7). It would be interesting to see the results of proposed InvP with these wider backbones.\n"}]}}
{"custom_id": "42", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.\n"}]}}
{"custom_id": "43", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\nReview Point:\n2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that \"Both CNNs and ViTs seem to benefit similarly from increased model capacity\". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.\n"}]}}
{"custom_id": "44", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).\n"}]}}
{"custom_id": "45", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).\n"}]}}
{"custom_id": "46", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\nReview Point:\n3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.\n"}]}}
{"custom_id": "47", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.\n"}]}}
{"custom_id": "48", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.\n"}]}}
{"custom_id": "49", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. The novelty is limited. The proposed method is too similar to other attentional modules proposed in previous works [1, 2, 3]. The group attention design seems to be related to ResNeSt [4] but it is not discussed in the paper. Although these works did not evaluate their performance on object detection and instance segmentation, the overall structures between these modules and the one that this paper proposed are pretty similar.\n"}]}}
{"custom_id": "50", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. Fig. 3 e. Since the preactivation values of two networks are the same membrane potentials, their output cosine similarity will be very high. Why not directly illustrate the results of the latter loss term of Eqn 13?\n"}]}}
{"custom_id": "51", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.\n"}]}}
{"custom_id": "52", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n3. I didn't find all parameter values. What are the model parameters for task 1? What lambda was chosen for the Boltzmann policy. But more importantly: How were the parameters chosen? Maximum likelihood estimates?\n"}]}}
{"custom_id": "53", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n4. Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?\n"}]}}
{"custom_id": "54", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n- the required implicit call to the Witness oracle is confusing.\n"}]}}
{"custom_id": "55", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\nReview Point:\n1) The proposed method cannot handle the headpose. While this paper defers this problem to a future work, a previous work (e.g., Gafni et al. ICCV 2021) is already able to control both facial expression and headpose. Why is it not possible to condition the headpose parameters in the NeRF beyond the facial expression similar to [Gafni et al. ICCV 2021]?\n"}]}}
{"custom_id": "56", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.\n"}]}}
{"custom_id": "57", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The authors have reproduced a well-known result in the literature--left political bias in ChatGPT and in LLMs in general--using the \"coarse\" (their description) methodology of passing a binary stance classifier over ChatGPT's output. The observation that language models reproduce the biases of the corpora on which they're trained has been made at each step of the evolution of these models, from word2vec to BERT to ChatGPT, and so it's unclear why this observation needs to once again be made using the authors \"coarse\" methodology.\n"}]}}
{"custom_id": "58", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\nReview Point:\n- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.\n"}]}}
{"custom_id": "59", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\nReview Point:\n1. The authors should make clear the distinction of when the proposed method is trained using only weak supervision and when it is semi-supervised trained. For instance, in Table 1, I think the proposed framework row refers to the semi-supervised version of the method, thus the authors should rename the column to \u2018Fully supervised\u2019 from \u2018Supervised\u2019. Maybe a better idea is to specify the data used to train ALL the parts of each model and have two big columns \u2018Mixture training data\u2019 and \u2018Single source data\u2019 which will make it much more prevalent of what is which.\n"}]}}
{"custom_id": "60", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice.\n"}]}}
{"custom_id": "61", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n- \"semantic\" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper.\n"}]}}
{"custom_id": "62", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.\n"}]}}
{"custom_id": "63", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses).\n"}]}}
{"custom_id": "64", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\nReview Point:\n2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I\u2019m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?\n"}]}}
{"custom_id": "65", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n2. The AAAI15 paper titled \"Spectral Clustering Using Multilinear SVD: Analysis, Approximations and Applications\" by Ghoshdastidar and Dukkipati seems to be a related work missed by the authors. This AAAI15 paper deals with hypergraph data with tensors as well so it should be discussed and compared against to provide a better understanding of the state-of-the-art.\n"}]}}
{"custom_id": "66", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.\n"}]}}
{"custom_id": "67", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\nReview Point:\n- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \\gamma^1 and \\gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).\n"}]}}
{"custom_id": "68", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.\n"}]}}
{"custom_id": "69", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.\n"}]}}
{"custom_id": "70", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\nReview Point:\n- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: \u201csingle target domain with \\textbf{sparse} labels\u201d, \u201c\u2026target distribution p_T(x, y) with label observation\u2026\u201d is mentioned, but the notation for target domain \\tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).\n"}]}}
{"custom_id": "71", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n\u2022 I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.\n"}]}}
{"custom_id": "72", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].\n"}]}}
{"custom_id": "73", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n2. The authors need to show a graph showing the plot of T vs number of images, and Expectation(T) over the imagenet test set. It is important to understand whether the performance improvement stems solely from the network design to exploit spatial redundancies, or whether the redudancies stem from the nature of ImageNet, ie., large fraction of images can be done with Glance and hence any algorithm with lower resolution will have an unfair advantage. Note, algorithms skipping layers or channels do not enjoy this luxury.\n"}]}}
{"custom_id": "74", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand. Fig.\n"}]}}
{"custom_id": "75", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work \"Targeting Ultimate Accuracy: Face Recognition via Deep Embedding\", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.\n"}]}}
{"custom_id": "76", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.\n"}]}}
{"custom_id": "77", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately.\n"}]}}
{"custom_id": "78", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n- L.490: \u201cwe take the embedding of the first subword token as the verb embedding.\u201d It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).\n"}]}}
{"custom_id": "79", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation.\n"}]}}
{"custom_id": "80", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt\u00b4arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.\n"}]}}
{"custom_id": "81", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n- Search models comparison 5.1: what does 100 steps here mean? Is it 100 sampled strategies?\n"}]}}
{"custom_id": "82", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\nReview Point:\n1. The proposed algorithm DMLCBO is based on double momentum technique. In previous works, e.g., SUSTAIN[1] and MRBO[2], double momentum technique improves the convergence rate to $\\mathcal{\\widetilde O}(\\epsilon^{-3})$ while proposed algorithm only achieves the $\\mathcal{\\widetilde O}(\\epsilon^{-4})$. The authors are encouraged to discuss the reason why DMLCBO does not achieve it and the theoretical technique difference between DMLCBO and above mentioned works.\n"}]}}
{"custom_id": "83", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The modulator is heuristically designed. It is hard to justify if there is a scalability issue that might need tedious hyperparameter tuning for diverse training data.\n"}]}}
{"custom_id": "84", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\nReview Point:\n* Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods.\n"}]}}
{"custom_id": "85", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly. Here are some examples: (1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is \"trivial\" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)? (2) In line 96, I do not understand the sentence \"our lower hierarchical layers zoom in time\" and the sentence following that.\n"}]}}
{"custom_id": "86", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1) The first concern is the goal of the paper. Indeed, DAS earthquake detectors exists (one of them was cited by the autors, PhaseNet-Das, Zhu et al. 2023, there might be others), and no comparison was made, nor a justification on the benefit of your method against theirs. If the claim is to say that this is a foundation model, and the test on this task is only as a proof of concept, it should be clearer, and then show or justify a future useful application.\n"}]}}
{"custom_id": "87", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.\n"}]}}
{"custom_id": "88", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2. Page 2, second paragraph in Related Work, the Walkman algorithm (Mao et al., 2020) is solved by ADMM, with two versions, one is to solve a local optimization problem, the other is to solve a gradient approximation. Therefore, it is not accurate to say that \u201cHowever, these works are all based on the simple SGD for decentralized optimization.\u201d 3. Section 3, first paragraph, in \u201cIt can often have faster convergence and better generalization than the SGD-based Algorithm 1, as will be demonstrated empirically in Section 4.1.\u201d The \u201cit\u201d does not have a clear reference.\n"}]}}
{"custom_id": "89", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n5.In experimental section, authors only compared with two baselines, there\u2019re several works also focus on the same questions, for example [1,2,3], so it\u2019s suggested to add more experimental to show the effectiveness of proposed method.\n"}]}}
{"custom_id": "90", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\nReview Point:\n* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.\n"}]}}
{"custom_id": "91", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n- The Related Work section is lacking details. The paragraph on long-context language models should provide a more comprehensive overview of existing methods and their limitations, positioning SSMs appropriately. This includes discussing sparse-attention mechanisms [1, 2], segmentation-based approaches [3, 4, 5], memory-enhanced segmentation strategies [6], and recursive methods [7] for handling very long documents.\n"}]}}
{"custom_id": "92", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\nReview Point:\n- The writing should be improved. Some points in the paper is unclear to me.\n"}]}}
{"custom_id": "93", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\nReview Point:\n2. The synthetic experiment in a non-separable case seems to be a problem. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model?\n"}]}}
{"custom_id": "94", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\nReview Point:\n2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems.\n"}]}}
{"custom_id": "95", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n6) Adding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution.\n"}]}}
{"custom_id": "96", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\nReview Point:\n34 \"to force the neural network to memorize them\" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact \"critical point\" as such in TopoNet [24]. Minor: I find the method section to be a bit wordy, it could be compressed on the essential definitions. There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l.\n"}]}}
{"custom_id": "97", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works. Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance?\n"}]}}
{"custom_id": "98", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n4) The analysis from line 128 to 149 is not convincing enough. From the histogram as shown in Fig 3, the GS-P-50 model has smaller class selectivity score, which means GS-P-50 shares more features and ResNet-50 learns more class specific features. And authors hypothesize that additional context may allow the network to reduce its dependency. What is the reason such an observation can indicate GS-P-50 learns better representation? Reference: [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, CVPR, 2018. [2] W. Luo et al., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS, 2016.\n"}]}}
{"custom_id": "99", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.\n"}]}}
{"custom_id": "100", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1) Some observations and subsequent design decisions might be hardware and software dependent;\n"}]}}
{"custom_id": "101", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. Limited novelty. The paper seems like a straightforward application of existing literature, specifically the DeCorr [1] that focuses on general deep graph neural networks, in a specific application domain. The contribution of this study is mainly the transposition of DeCorr's insights into graph collaborative filtering, with different datasets and backbones. Although modifications like different penalty coefficients for users and items are also proposed, the whole paper still lack enough insights about what are unique challenges of overcorrelation in recommender systems.\n"}]}}
{"custom_id": "102", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n"}]}}
{"custom_id": "103", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n1. From the methodology aspect, the novelty of paper appears to be rather limited. The ENCODE part is already proposed in [10] and the incremental contribution lies in the decomposition part which just factorizes the M_v into factor D and slices Phi_v.\n"}]}}
{"custom_id": "104", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n--- W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.\n"}]}}
{"custom_id": "105", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty.\n"}]}}
{"custom_id": "106", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking.\n"}]}}
{"custom_id": "107", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- line 47 - 48 \"over-parametrization invariably overfits the data and results in worse performance\": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].\n"}]}}
{"custom_id": "108", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.\n"}]}}
{"custom_id": "109", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2\u0080\u009cattribute\u00e2\u0080\u009d. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2\u0080\u009cLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2\u0080\u009d. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n"}]}}
{"custom_id": "110", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\nReview Point:\n- I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI\u201921, \u201cDistributional Reinforcement Learning via Moment Matching\u201d). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.\n"}]}}
{"custom_id": "111", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\nReview Point:\n1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n"}]}}
{"custom_id": "112", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\nReview Point:\n- The novelty of using PCA to reduce interaction count seems incremental and the significance of the paper results is unclear to me. Using PCA to reduce the interaction count seems intuitive, as PCA aims to retain the maximum information in the data with the reduced dimensionality chosen, assuming certain assumptions are met. How well are the assumptions met? [1] Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert M\u00fcller, and Pan Kessel. \"Towards robust explanations for deep neural networks.\" Pattern Recognition 121 (2022): 108194.\n"}]}}
{"custom_id": "113", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.\n"}]}}
{"custom_id": "114", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\nReview Point:\n3. I think figure 6C is a bit awkward, it implies negative rates, which is not the case, I would suggest using a second y-axis or another visualization which is more physically accurate.\n"}]}}
{"custom_id": "115", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\nReview Point:\n1) It seems that there is still room to improve the complexity of Algorithm 2;\n"}]}}
{"custom_id": "116", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- I find this statement in the supplemental section D.4 questionable: \"Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting\". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states?\n"}]}}
{"custom_id": "117", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1.The main part can be more concise (especially for the introduction part)and including empirical results.\n"}]}}
{"custom_id": "118", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\nReview Point:\n2. The technical contribution is unclear. Most of the analysis are quite standard.\n"}]}}
{"custom_id": "119", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\nReview Point:\n2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes:\n"}]}}
{"custom_id": "120", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\nReview Point:\n2. It will be good to see some failure cases and related discussion.\n"}]}}
{"custom_id": "121", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\nReview Point:\n4. In Section 4.1, \\epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \\epsilon when (11) is discussed.\n"}]}}
{"custom_id": "122", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\nReview Point:\n- In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor:\n"}]}}
{"custom_id": "123", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n3. This author should add more description about the contribution of this paper.\n"}]}}
{"custom_id": "124", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- it would seem to me that in section 4, \"X\" should be a multiset (and [\\cal X]**n the set of multisets of size n) instead of a set, since in order the histogram to honestly represent a graph that has repeated vertex or edge labels, you need to include the multiplicities of the labels in the graph as well.\n"}]}}
{"custom_id": "125", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\nReview Point:\n2. The authors' derivation falls into classical learning theory-based bounds, which, to the best of my knowledge, does not yield realistic bounds, unless Bayesian considerations are taken into account (e.g. Bayesian-PAC based bounds).\n"}]}}
{"custom_id": "126", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\nReview Point:\n- The authors discuss how certain methods are significantly different from others, yet no significance testing is done to support these claims. For example, in line 486 the authors write \"The conversational ability of ChatGPT and GPT-4 significantly boosts translation quality and discourse awareness\" -- the difference between zh->en ChatGPT (17.4 d-BLEU; 2.8/2.9 humeval) and GPT-4 (18.8 d-BLEU; 2.6/2.8 humeval) and the scores for FeedME-2 (16.1 d-BLEU; 2.2/2.3 humeval) and PPO (17.2 d-BLEU; 2.6/2.7 humeval) is minimal and it's hard to say whether it is significant without proper testing (including checking the distribution and accounting for multiple comparisons).\n"}]}}
{"custom_id": "127", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\nReview Point:\n1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.\n"}]}}
{"custom_id": "128", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\nReview Point:\n5. In my opinion, the contribution of this paper appears somewhat limited, and the proposed model seems incremental in its approach.\n"}]}}
{"custom_id": "129", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\nReview Point:\n\u2022 Supervised pretraining based on the prediction of homo-lumo gap may lead to negative transfer. For example, on QM9 in downstream experiments, Transformer-M performs poorly on most tasks other than homo, lumo, and gap. This may be contradictory to the description \"general-purpose neural network model\" claimed in this paper.\n"}]}}
{"custom_id": "130", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\nReview Point:\n2. Experimental Evaluation 2.1. Ablations 2.1.1. The paper claims that \u00e2\u0080\u009cAnother distinction of our approach is the \u00e2\u0080\u009cpicking\u00e2\u0080\u009d step \u00e2\u0080\u009c. However, this aspect is not ablated. 2.2. Experiments on CIFAR. The comparison on CIFAR is not convincing 2.2.1. The continual learning literature has extensive experiments on this dataset and the paper only compares to one approach (DEN). 2.2.2. It is unclear if DEN is correctly used/evaluated. It would have been more convincing if the authors used the same setup as in the DEN paper to make sure the comparison is fair/correct.\n"}]}}
{"custom_id": "131", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph?\n"}]}}
{"custom_id": "132", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2. I have some doubts about the definations in Table1. What's the different between anchor-based regression and the regression in RepPoints? in RetinaNet, there is also only a one-shot regression. And in ATSS, this literature has proved that the regression methods do not influence a lot. The method that directly regresses [w, h] to the center point is good enough. While RepPoints regresses distance to the location of feature maps. I think there is no obvious difference between the two methods. I hope the authors can clarify this problem. If not, the motivations here is not solid enough.\n"}]}}
{"custom_id": "133", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex.\n"}]}}
{"custom_id": "134", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: - The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.\n    rationale: The comment presents a claim that the clustering approach is not generalizable to all parts of speech due to non-uniform performance. However, the explanation is minimal, making the claim borderline verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n2. On line 205, it should be Fig. 1 instead of Fig. 5.1. In latex, please put '\\label' after the '\\caption', and the bug will be solved.\n"}]}}
{"custom_id": "135", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\nReview Point:\n3. It could be convincing to discuss case studies and error studies to highlight the effectiveness of each proposed component. For example, this paper mentions that the Element-level Graph Pre-training abandons the strategy of capturing the complex structure but focuses directly on the core elements. However, without case study, it is less convincing to figure it out. An example of case study can be found in \u201cGraph pre-training for AMR parsing and generation\u201d.\n"}]}}
{"custom_id": "136", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\nReview Point:\n7) in the same section, the notation {\\cal P} with a subscript is used several times without being defined.\n"}]}}
{"custom_id": "137", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: It is unclear how the reported results were averaged across runs. Could you specify whether they were computed over five independent trials?\n    rationale: The comment asks for clarification regarding the methodology but does not contain an opinion, judgment, or suggestion that requires verification. It is purely factual, so \"No\" is the appropriate label.\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- l148: known, instead of know - l156: please define \\gamma_0^{***} - Figure 1: Please specify the meaning of the colors in the caption as well as the text.\n"}]}}
{"custom_id": "138", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: If the training data is imbalanced, the model might exhibit biased predictions.\n    rationale: This is a logical statement based on common knowledge in machine learning. It does not present an opinion or a claim requiring verification, so it fits the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: If the dataset contains noisy labels, it is likely that the model's generalization will be negatively impacted.\n    rationale: The comment makes a logical deduction based on widely known principles in machine learning. It does not express an opinion or a claim requiring validation, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?\n"}]}}
{"custom_id": "139", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: - For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are \u00e2_x0080__x009c_attribute\u00e2_x0080__x009d_. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in \u00e2_x0080__x009c_Learning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016\u00e2_x0080__x009d_. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.\n    rationale: The comment claims that \"better metadata embedding options are available\" for zero-shot learning and supports this claim with a reference to Reed et al. (CVPR 2016). Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: The runtime of the algorithm increases as the input size grows due to its O(N\u00b2) complexity.\n    rationale: This statement is a factual, logical inference based on computational complexity theory. Since it does not introduce a subjective claim, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- As I said above, I found the writing / presentation a bit jumbled at times.\n"}]}}
{"custom_id": "140", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The main part can be more concise (especially for the introduction part)and including empirical results.\n    rationale: The comment claims that \"the main part can be more concise\" and suggests including empirical results. However, it lacks specific justification, making the claims barely verifiable. As this is a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn\u2019t clear to me why the ones you chose were better options.\n    rationale: The comment primarily consists of questions seeking clarification and does not make any subjective opinions, judgments, or suggestions that would require validation or support. Therefore, the classification \"No\" is appropriate.\n    score: No\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.\n    rationale: The comment claims a \"lack of novelty\" regarding adversarial attacks, referencing the related work section. While it provides a logical basis, additional references would strengthen it, making the claim mostly verifiable. As it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- I wonder what is the total computational complexity compared to other methods (e.g., emerging convolutions). If I imagine the Woodbury flow working on a mobile device, the number of operations could cause a significant power demand.\n"}]}}
{"custom_id": "141", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Larger models generally require more computational resources for training and inference.\n    rationale: The statement is a widely accepted fact in deep learning and does not introduce an opinion, judgment, or suggestion. It is purely descriptive, so it is labeled \"No\".\n    score: No\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\\mathcal{R}^{r \\times r}$, $\\mathcal{R}^{r \\times o}$, $\\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\n    rationale: The comment consists of a methodological suggestion and an observational analysis regarding the comparison of parsing F-1 and perplexity. The sentence \"Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.\" logically assesses the consequences of the proposed setup. These statements are factual and do not embody subjective claims needing verification. Therefore, the comment contains no claims, affirming the label \"No\" as appropriate.\n    score: No\n    \n\nReview Point:\n- The authors write \u201cIn the base IOI circuit, the Induction, Duplicate Token, and Previous Token heads primarily attend to the S2 token\u201d this is incorrect according to Section 3 of Wang et al., 2023. These heads are _active_ at the S2 token, but do not primarily attend to it.\n"}]}}
{"custom_id": "142", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: Why did you choose a batch size of 64? Would a larger batch size improve performance?\n    rationale: The comment consists of a question and does not present an opinion or judgment requiring verification. It is purely factual, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - A number of claims from this paper would benefit from more in-depth analysis.\n    rationale: The comment claims that \"a number of claims from this paper would benefit from more in-depth analysis.\" However, it lacks supporting reasoning or examples, making it unverifiable. Since this is still a claim, it should be labeled as \"Yes\".\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.\n    rationale: The comment claims that the paper\u2019s strength could be improved by incorporating more baselines, particularly noting an issue with the MST baseline. However, it lacks specific references, making it somewhat verifiable. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Given that Method A optimizes for accuracy, it is expected that it may sacrifice some interpretability.\n    rationale: This is a logical inference based on common trade-offs in machine learning. Since it does not present a subjective claim needing justification, it is classified as \"No\".\n    score: No\n    \n\n    Review Point: 1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.\n    rationale: The comment claims that \"the paper does not dig into the theory proofs.\" Since this is a claim rather than a neutral statement, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n- L15: Again, too vague, certain RNNs work well for certain natural language reasoning tasks. See for instance the literature on natural language inference and the leaderboard at https://nlp.stanford.edu/projects/snli/ - L16-18: The reinforcement learning / agent analogy seems a bit out-of-place here. I think you generally point to generalization capabilities which I believe are better illustrated by the examples you give later in the paper (from lines 229 to 253).\n"}]}}
{"custom_id": "143", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: When the number of training samples is small, overfitting is more likely to occur.\n    rationale: This is a factual and logical statement that follows from basic principles of statistical learning. It does not introduce a claim that requires verification.\n    score: No\n    \n\n    Review Point: If the loss function is not well-calibrated, the model\u2019s predictions may not be reliable in probabilistic settings.\n    rationale: The comment presents a logical statement based on common machine learning knowledge. It does not express a subjective claim requiring verification, making \"No\" the correct label.\n    score: No\n    \n\n    Review Point: - The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n    rationale: The comment claims that the sentence in line 212 is \"not strictly correct\" and references Figure 2 for support. While mostly verifiable, additional explanation could strengthen it. Since this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.\n    rationale: The comment claims that \"these baselines seem weak\" and fully verifies this by providing comparative examples and numerical data. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: - Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.\n    rationale: The comment claims that the \"dataset itself might have a lot of noise,\" citing an example of misinformation in tweets. While the reasoning is sound, it lacks detailed evidence, making the claim mostly verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\nReview Point:\n104 it is said that every kernel can be described by a feature space parameterized by a neural network, but this is trivially not true. For instance, for RBF kernels, the RKHS is famously infinite-dimensional, such that one would need an NN with infinite width to represent it. So at most, NNs can represent finite-dimensional RKHSs in practice. This limitation should be made more clear. l.\n"}]}}
{"custom_id": "144", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o", "temperature": 0.1, "messages": [{"role": "system", "content": "\nYou are an expert in evaluating peer review comments with respect to different aspects.\n"}, {"role": "user", "content": "\nThis aspect is aimed to maximize the utilization of the review comments for the authors. The primary purpose of the review is to help/guide authors in improving their drafts. Keep this in mind while evaluating the review point. Whenever you encounter a borderline case, think: \u201cWill this review point help authors improve their draft?\u201d. There is no correlation between the aspect score and the length of the review point.\nEvaluate the review point based on the aspect description provided next.\n\nverifiability_extraction:\n\n\nClaim Extraction\n\nObjective:\nDetermine whether the given text contains claims or merely consists of factual statements.\n\nOpinion & Claims\n\nSubjective statements, including opinions or disagreements with experimental choices.\n\nSuggestions or requests for changes (e.g., indicating something should be removed, added, or discussed).\n\nJudgments about sections of the paper (e.g., stating that something is unclear, lacks detail, or is well-written).\n\nDeductions or inferred observations beyond stating mere facts.\n\nAny statement where evidence or justification is required to support the claim.\n\nNormal Statements\n\nDefinition: A statement that does not contain an opinion, claim, or suggestion but consists solely of factual, descriptive content that requires no justification.\n\nIndicating existence or absence of something without suggesting changes.\n\nGeneral statements about the paper that do not express an opinion.\n\nObjective, factual statements that do not require verification.\n\nRequests for clarification or general questions.\n\nLogical statements or directly inferable information.\n\nPositive claims (e.g., \"The paper is well written\") are considered neutral as they do not help authors improve their work.\n\nScoring Criteria:\n\nYes: If the text contains claims, opinions, or suggestions.\n\nNo: If the text consists solely of normal statements.\n\n{examples}\nReview Point:\n{review_point}\n\n\nGenerate a rationale and use it to output the score. \n\n    Review Point: 2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?\n    rationale: The comment primarily consists of inquiries and an observation regarding the lack of an explanation in the paper. It does not include any claims, opinions, or suggestions that require verification. Therefore, there are no claims present in this comment. It is factual and descriptive in nature, fitting the classification of \"No\"\n    score: No\n    \n\n    Review Point: 5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \\sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \\alpha_v computation seems to do) might be ill conditioned and numerically unstable.\n    rationale: The comment claims that \"it is not clear how the question is being used to learn attention on the image feature\" and fully verifies this claim by pointing out a missing term in the equation. Since this is a claim and fully verifiable, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: Regularization techniques such as L2 regularization can help reduce overfitting by penalizing large weights.\n    rationale: This is a well-known fact in machine learning and does not constitute a subjective claim or an opinion that requires verification. It is classified as \"No\".\n    score: No\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: Could you clarify whether the baseline results were obtained from the original paper or reproduced in your experiments?\n    rationale: The comment is a question requesting clarification and does not make any subjective claims or judgments. It is purely factual, fitting the \"No\" label.\n    score: No\n    \n\n    Review Point: 2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes.\n    rationale: The comment claims that \"the evaluation on the word analogy task seems a bit unfair,\" providing an initial rationale but lacking comprehensive evidence. This makes it somewhat verifiable. Since it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.\n    rationale: The comment claims that \"the novelty is limited,\" suggesting that using a linear model for interpreting deep neural networks is not new. However, the justification is vague, making the claim borderline verifiable. Because it is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: 1) Lines 102-106 is misleading. While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n    rationale: The comment claims that \"Lines 102-106 are misleading\" due to the phrase \"'such distribution'.\" The justification is partially provided but lacks depth, making the claim somewhat verifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\n    Review Point: You mention that dataset X is used frequently in this domain, but there is no reference supporting this statement. Could you add one?\n    rationale: The comment points out the lack of a reference and suggests adding one but does not assert a subjective claim requiring verification. Since it is a factual observation with a request, it is labeled as \"No\".\n    score: No\n    \n\n    Review Point: - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the \u201cfalse-positive\u201d neurons, however, the paper doesn\u2019t show how important these post-processing steps are. I think an ablation study may be needed.\n    rationale: The comment claims that \"integrated gradients to measure the attribution has been studied in existing papers\" but does not provide references, making it unverifiable. Because this is a claim, it should be labeled as \"Yes.\"\n    score: Yes\n    \n\nReview Point:\n* The proposed method is not well-positioned in literature. It's worth pointing out that the key idea of representing the marginal score as the expectation of scores of distributions conditioned on inputs is actually quite well-known. It has been used, for example, to develop the original denoising score matching objective [1]. It is also used in the literature as \"score-interpolation\" [2]. I just named a few but I would recommend the authors to do a thorough literature review as I believe this property is used in many more works.\n"}]}}
